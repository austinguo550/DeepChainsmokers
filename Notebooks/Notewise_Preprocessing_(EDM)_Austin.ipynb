{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Notewise Preprocessing (EDM) - Austin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RWyZf8Ojbc4i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# INSTRUCTIONS\n",
        "\n",
        "Workflow: Run each cell in order. Put or extract data into relevant folders as defined in section 2. Make remaining folders in drive.  **Cells denoted with * may require extra action.**\n",
        "\n",
        "\n",
        "```\n",
        "Folder Structure Suggested for Section 2:\n",
        "drive/\n",
        "    train_data/ <- input midis go here\n",
        "    train_output/\n",
        "        graphs/ <- train/val accuracy plots go here\n",
        "        intermed/ <- intermediate weights, preprocessing go here\n",
        "        stats/ (currently not being used)\n",
        "        midi/ <- output midis go here\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3pEE99zUNRlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 Imports"
      ]
    },
    {
      "metadata": {
        "id": "lowbKt1LMnng",
        "colab_type": "code",
        "outputId": "5b781813-55dc-4b16-c95b-d976c07f5efb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "# Import Data Manip, Debug\n",
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Music21\n",
        "!pip install music21\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "\n",
        "# Import Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import CuDNNLSTM\n",
        "from keras.layers import CuDNNGRU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import Flatten\n",
        "from keras.regularizers import L1L2\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: music21 in /usr/local/lib/python3.6/dist-packages (5.5.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DSQDTCyePubS",
        "colab_type": "code",
        "outputId": "ef41b4da-1279-4cee-a3ca-071dbb5d74f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "cell_type": "code",
      "source": [
        "# stuff needed for colaboratory to connect with drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxRAEDfZNkIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 Mount and Set Directories*"
      ]
    },
    {
      "metadata": {
        "id": "-1Mmh8PPNeDj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D8-DMyMBNxG2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_FOLDER = 'drive/Project/train_data/edm_all'\n",
        "GRAPHS_FOLDER = 'drive/Project/train_output/graphs'\n",
        "MIDI_OUTPUT_FOLDER = 'drive/Project/train_output/midi'\n",
        "INTERMED_FOLDER = 'drive/Project/train_output/intermed'\n",
        "STATS_FOLDER = 'drive/Project/train_output/stats'\n",
        "\n",
        "# data specific\n",
        "EDM_CORPUS = '/edm_'\n",
        "\n",
        "# SET THIS TO ONE OF ABOVE (ACTION)\n",
        "CORPUS = EDM_CORPUS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v3TI003prxXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Notewise Root and Rest Learning"
      ]
    },
    {
      "metadata": {
        "id": "UZy7RtMqQRNC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Set Training Parameters*"
      ]
    },
    {
      "metadata": {
        "id": "MBD3DkdvQU6z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SET PARAMETERS (ACTION)\n",
        "RESTS = True\n",
        "ROOT_EXTRACTION = True\n",
        "DURATION_BATCH_SIZE = 256\n",
        "NOTE_BATCH_SIZE = 128\n",
        "# SPECIFY PARAMETERS TO TEST AS LIST\n",
        "DROPOUTS = 0\n",
        "MODEL_SIZES = 256\n",
        "EPOCHS = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "If7uhXkTQB4a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 Preprocess Data (MIDI Data Into Notes Corpus and Duration Corpus)"
      ]
    },
    {
      "metadata": {
        "id": "9PxPLld-PyNb",
        "colab_type": "code",
        "outputId": "8a0d900e-7503-4f61-9c15-d2741b931ec1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1044
        }
      },
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "notes_corpus = []\n",
        "durations_corpus = []\n",
        "\n",
        "for file in glob.glob(DATA_FOLDER + \"/*.mid\"):\n",
        "    try:\n",
        "      print(\"Extracting MIDI File: \", file)\n",
        "      midi_stream = converter.parse(file)\n",
        "\n",
        "      notes = None\n",
        "\n",
        "      partition = instrument.partitionByInstrument(midi_stream)\n",
        "\n",
        "      if not RESTS:\n",
        "        # No rests\n",
        "        if partition: \n",
        "            notes = partition.parts[0].recurse()\n",
        "        else: \n",
        "            notes = midi_stream.flat.notes\n",
        "      else:\n",
        "        # With rests\n",
        "        if partition: \n",
        "            notes = partition.parts[0].recurse()\n",
        "        else: \n",
        "            notes = midi_stream.notesAndRests\n",
        "          \n",
        "      in_song_notes = []\n",
        "      in_song_durations = []\n",
        "      for element in notes:\n",
        "          in_song_durations.append(element.duration.quarterLength)\n",
        "          if isinstance(element, note.Note):\n",
        "              in_song_notes.append(str(element.pitch))\n",
        "          elif RESTS and isinstance(element, note.Rest):\n",
        "              in_song_notes.append(\"R\")\n",
        "          elif isinstance(element, chord.Chord):\n",
        "              if ROOT_EXTRACTION:\n",
        "                  in_song_notes.append(element.root().nameWithOctave)\n",
        "              else:\n",
        "                  in_song.append('.'.join(str(n) for n in element.normalOrder))\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    notes_corpus.append(in_song_notes)\n",
        "    durations_corpus.append(in_song_durations)\n",
        "            \n",
        "# Write\n",
        "with open(INTERMED_FOLDER + CORPUS + 'notes_corpus', 'wb+') as filepath:\n",
        "    pickle.dump(notes_corpus, filepath)\n",
        "    \n",
        "with open(INTERMED_FOLDER + CORPUS + 'durations_corpus', 'wb+') as filepath:\n",
        "    pickle.dump(durations_corpus, filepath)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Afrojack _ David Guetta - Another Life  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Lonely Together.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Axwell _ Ingrosso - More Than You Know  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Sing Me To Sleep  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Without You.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alesso ft. Matthew Koma - Years (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Tired  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Alone  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Faded (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Dear Boy.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Axwell _ Shapov - Belong (Axwell and Years Remode) (Midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 2 - C Maj.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 3 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Bruno Mars - That_s What I Like (Alan Walker Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Calvin Harris - Blame  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Calvin Harris - My Way  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 5 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 6 - F Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 1 - C Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 4 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 7 - F Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 14 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 8 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 16 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 9 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 15 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 10 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 13 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 12 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 11 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Galantis - Love On Me  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kill Paris ft. Royal - Operate (Illenium Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Headhunterz _ KSHMR - Dharma  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Ritual (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Galantis - No Money  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Summer  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kygo _ Selena Gomez - It Ain_t Me  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Alone (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kygo _ Ellie Goulding - First Time  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Madeon - Icarus  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix _ Bebe Rexha - In The Name Of Love  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix - There For You  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix ft. The Federal Empire - Hold On _ Believe (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Porter Robinson _ Madeon - Shelter (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix _ Dua Lipa - Scared To Be Lonely  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Porter Robinson - Sad Machine  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Nero - The Thrill (Porter Robinson Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Seven Lions x Echos - Cold Skin  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Sebastian Ingrosso _ Alesso feat. Ryan Tedder - Calling (Lose My Mind).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/R3hab _ KSHMR - Strong  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers _ Coldplay - Something Just Like This.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers ft. Halsey - Closer.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers ft. Phoebe Ryan - All We Know.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers Ft. Daya - Don_t Let Me Down.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers - Paris.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Tiesto _ KSHMR feat. Vassy - Secrets  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Zedd _ Hailee Steinfeld Grey - Starving  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers feat. XYLO - Setting Fires.mid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HDF17yhLSWLE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5 Preprocess Corpus Into Train Data"
      ]
    },
    {
      "metadata": {
        "id": "0kDTz_c9VxNk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Notes Corpus"
      ]
    },
    {
      "metadata": {
        "id": "CvOF4e3mSKDy",
        "colab_type": "code",
        "outputId": "c58b9f49-3c4b-48c4-9217-27f69bfd73c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# load notes corpus\n",
        "with open(INTERMED_FOLDER + CORPUS + 'notes_corpus', 'rb') as filepath:\n",
        "    notes_corpus = pickle.load(filepath)\n",
        "    \n",
        "# If doing learning for one song at a time only\n",
        "flattened_notes_corpus = []\n",
        "for song_notes in notes_corpus:\n",
        "    flattened_notes_corpus += song_notes\n",
        "\n",
        "vocab_size = len(set(flattened_notes_corpus))\n",
        "print(vocab_size)\n",
        "\n",
        "# Produce input output sequences\n",
        "window_size = 60\n",
        "note_sequence_input = []\n",
        "next_note_output = []\n",
        "\n",
        "notes = sorted(set(flattened_notes_corpus))\n",
        "note2int = dict((note, num) for num, note in enumerate(notes))\n",
        "\n",
        "for i in range(0, len(notes_corpus)):\n",
        "    for j in range(0, len(notes_corpus[i]) - window_size):\n",
        "        current_sequence = [note2int[note] for note in notes_corpus[i][j:window_size+j]]\n",
        "        next_note = note2int[notes_corpus[i][window_size+j]]\n",
        "        note_sequence_input.append(current_sequence)\n",
        "        next_note_output.append(next_note)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YD3BiXehS8Xq",
        "colab_type": "code",
        "outputId": "3806ff3f-bd25-4548-ffb2-3b419cde14fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# check train and label shapes\n",
        "training_data = np.reshape(note_sequence_input, (len(note_sequence_input), window_size , 1))\n",
        "training_data = training_data / float(vocab_size)\n",
        "print('Train shape: ' + str(training_data.shape))\n",
        "training_label = np_utils.to_categorical(next_note_output)\n",
        "print('Label shape: ' + str(training_label.shape))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (11392, 60, 1)\n",
            "Label shape: (11392, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BwxdDozxcAUD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.2 Durations Corpus"
      ]
    },
    {
      "metadata": {
        "id": "js6CAvf7V2R5",
        "colab_type": "code",
        "outputId": "3a8432f3-dc71-42e1-b0d2-cc992b8073f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# load notes corpus\n",
        "with open(INTERMED_FOLDER + CORPUS + 'durations_corpus', 'rb') as filepath:\n",
        "    duration_corpus = pickle.load(filepath)\n",
        "\n",
        "# Learn on one song at a time\n",
        "flattened_duration_corpus = []\n",
        "for song_durations in duration_corpus:\n",
        "    flattened_duration_corpus += song_durations\n",
        "    \n",
        "import collections\n",
        "counter = collections.Counter(flattened_duration_corpus)\n",
        "print(counter)\n",
        "\n",
        "\n",
        "duration_vocab_size = len(set(flattened_duration_corpus))\n",
        "print(duration_vocab_size)\n",
        "\n",
        "# Produce input output sequences\n",
        "duration_window_size = 30\n",
        "duration_sequence_input = []\n",
        "next_duration_output = []\n",
        "\n",
        "durations = sorted(set(flattened_duration_corpus))\n",
        "duration2int = dict((duration, num) for num, duration in enumerate(durations))\n",
        "\n",
        "print(duration2int)\n",
        "# Write\n",
        "with open(INTERMED_FOLDER + \"/edm_duration_counter\", 'wb+') as filepath:\n",
        "    pickle.dump(counter, filepath)\n",
        "\n",
        "for i in range(0, len(duration_corpus)):\n",
        "    for j in range(0, len(duration_corpus[i]) - duration_window_size):\n",
        "        current_duration_sequence = [duration2int[note] for note in duration_corpus[i][j:duration_window_size+j]]\n",
        "        next_duration = duration2int[duration_corpus[i][duration_window_size+j]]\n",
        "        duration_sequence_input.append(current_duration_sequence)\n",
        "        next_duration_output.append(next_duration)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0.25: 4993, 0.5: 4383, 0.0: 966, Fraction(1, 3): 935, 1.0: 735, Fraction(1, 6): 729, 0.75: 461, 4.0: 165, 2.0: 122, Fraction(2, 3): 93, Fraction(1, 12): 86, Fraction(5, 12): 72, 1.5: 55, 3.0: 41, Fraction(7, 6): 32, 1.75: 30, 1.25: 29, 2.25: 25, 4.5: 22, 3.5: 13, 6.5: 13, 8.0: 11, 4.25: 9, 2.5: 9, Fraction(25, 6): 7, 5.0: 6, 7.0: 5, 8.5: 4, 32.0: 4, 6.0: 4, Fraction(5, 3): 3, 48.0: 2, 68.0: 2, 176.0: 2, 80.0: 2, 16.0: 2, Fraction(23, 3): 1, 47.5: 1, 3.25: 1, 5.75: 1, 200.5: 1, 36.25: 1, 64.25: 1, 84.25: 1, 72.5: 1, 72.25: 1, 14.5: 1, 9.0: 1, 27.5: 1, 61.0: 1, 161.0: 1, 5.25: 1, 4.75: 1, 94.5: 1, 128.25: 1, 18.0: 1, 39.5: 1, Fraction(553, 6): 1, Fraction(4, 3): 1, 32.5: 1, 36.0: 1})\n",
            "61\n",
            "{0.0: 0, Fraction(1, 12): 1, Fraction(1, 6): 2, 0.25: 3, Fraction(1, 3): 4, Fraction(5, 12): 5, 0.5: 6, Fraction(2, 3): 7, 0.75: 8, 1.0: 9, Fraction(7, 6): 10, 1.25: 11, Fraction(4, 3): 12, 1.5: 13, Fraction(5, 3): 14, 1.75: 15, 2.0: 16, 2.25: 17, 2.5: 18, 3.0: 19, 3.25: 20, 3.5: 21, 4.0: 22, Fraction(25, 6): 23, 4.25: 24, 4.5: 25, 4.75: 26, 5.0: 27, 5.25: 28, 5.75: 29, 6.0: 30, 6.5: 31, 7.0: 32, Fraction(23, 3): 33, 8.0: 34, 8.5: 35, 9.0: 36, 14.5: 37, 16.0: 38, 18.0: 39, 27.5: 40, 32.0: 41, 32.5: 42, 36.0: 43, 36.25: 44, 39.5: 45, 47.5: 46, 48.0: 47, 61.0: 48, 64.25: 49, 68.0: 50, 72.25: 51, 72.5: 52, 80.0: 53, 84.25: 54, Fraction(553, 6): 55, 94.5: 56, 128.25: 57, 161.0: 58, 176.0: 59, 200.5: 60}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5u_Zi1BIWSqP",
        "colab_type": "code",
        "outputId": "e1710382-fc8d-49ae-95a2-0a57b8121917",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# check train and label shapes\n",
        "duration_training_data = np.reshape(duration_sequence_input, (len(duration_sequence_input), duration_window_size , 1))\n",
        "duration_training_data = duration_training_data / float(duration_vocab_size)\n",
        "print('Train shape: ' + str(duration_training_data.shape))\n",
        "duration_training_label = np_utils.to_categorical(next_duration_output)\n",
        "print('Label shape: ' + str(duration_training_label.shape))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (12837, 30, 1)\n",
            "Label shape: (12837, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ijVUScfKaopU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 6 Train"
      ]
    },
    {
      "metadata": {
        "id": "Ra6hL91vasVT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1 Helpers to Create Model"
      ]
    },
    {
      "metadata": {
        "id": "tzWIhY5sZ7-h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(network_input, n_vocab, model_size, dropout):\n",
        "  model = Sequential()\n",
        "  reg = L1L2(0, 0)\n",
        "  model.add(LSTM(\n",
        "      model_size,\n",
        "      input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "      return_sequences=True,\n",
        "      dropout=dropout, recurrent_dropout=0.3\n",
        "  ))\n",
        "  model.add(CuDNNLSTM(model_size, return_sequences=True, kernel_regularizer=reg))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(CuDNNLSTM(model_size, kernel_regularizer=reg))\n",
        "  model.add(Dense(128))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_callback_list(model_size, dropout, model_type):\n",
        "  filepath = INTERMED_FOLDER + '/%skarpathy-model-weights-%s-%s-%s.hdf5' % (CORPUS, model_type, model_size, dropout)\n",
        "  model_checkpoint = ModelCheckpoint(\n",
        "    filepath,\n",
        "    monitor='loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    mode='min'\n",
        "  )\n",
        "  return [model_checkpoint], filepath\n",
        "\n",
        "# acc history\n",
        "def setup_plot(dropout, size):\n",
        "  plt.title('Model Accuracy vs. Epoc with Dropout=%s Size=%s' % (dropout, size))\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  \n",
        "def plot_history(history, model_type, dropout, size):\n",
        "  plt.plot(history.history['acc'], label=\"%s train accuracy\" % model_type)\n",
        "  plt.plot(history.history['val_acc'], label=\"%s val accuracy\" % model_type)\n",
        "\n",
        "def save_plot(file_path):\n",
        "  plt.legend()\n",
        "  plt.savefig(file_path)\n",
        "  plt.clf()\n",
        "  \n",
        "def predict_duration(model, WEIGHT_PATH):\n",
        "  # Prediction\n",
        "  model.load_weights(WEIGHT_PATH)\n",
        "  starting_sequence = np.random.randint(219, size=duration_window_size)\n",
        "  pattern_sequence = starting_sequence.tolist()\n",
        "  prediction_output = []\n",
        "\n",
        "  duration2note = dict((num, note) for num, note in enumerate(durations))\n",
        "  print (duration2note)\n",
        "  \n",
        "  for i in range(400):\n",
        "      prediction_input = np.reshape(pattern_sequence, (1, len(pattern_sequence), 1))\n",
        "      prediction_input = prediction_input / float(duration_vocab_size)\n",
        "\n",
        "      if i%3 == 0:   \n",
        "          print('\\r', 'Predicting.  Duration: ', i, end='')  \n",
        "      if i%3 == 1:   \n",
        "          print('\\r', 'Predicting.. Duration: ', i, end='')\n",
        "      if i%3 == 2:   \n",
        "          print('\\r', 'Predicting...Duration: ', i, end='')\n",
        "      prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "      prediction_values = np.arange(len(prediction[0]))\n",
        "      prediction_prob = np.asarray(list(prediction[0])) / float(sum(prediction[0]))\n",
        "\n",
        "      # Most probable note prediction\n",
        "#       index = np.argmax(prediction)\n",
        "#       note_instance = duration2note[index]\n",
        "#       prediction_output.append(note_instance)\n",
        "      index = np.random.choice(prediction_values, 1, p=prediction_prob)\n",
        "      note_instance = duration2note[int(index[0])]\n",
        "      prediction_output.append(note_instance)\n",
        "\n",
        "      pattern_sequence.append(index)\n",
        "      pattern_sequence = pattern_sequence[1:len(pattern_sequence)]    \n",
        "\n",
        "  prediction_output = prediction_output[300:len(prediction_output)]\n",
        "\n",
        "  # Write\n",
        "  with open(INTERMED_FOLDER + (\"%sduration_prediction_output\" % CORPUS), 'wb+') as filepath:\n",
        "      pickle.dump(prediction_output, filepath)\n",
        "  \n",
        "  return prediction_output\n",
        "\n",
        "def predict_note(model, WEIGHT_PATH):\n",
        "  model.load_weights(WEIGHT_PATH)\n",
        "  # Prediction\n",
        "  starting_sequence = np.random.randint(219, size=window_size)\n",
        "  pattern_sequence = starting_sequence.tolist()\n",
        "  prediction_output = []\n",
        "\n",
        "  int2note = dict((num, note) for num, note in enumerate(notes))\n",
        "  print (int2note)\n",
        "\n",
        "  for i in range(400):\n",
        "      prediction_input = np.reshape(pattern_sequence, (1, len(pattern_sequence), 1))\n",
        "      prediction_input = prediction_input / float(vocab_size)\n",
        "\n",
        "      if i%3 == 0:   \n",
        "          print('\\r', 'Predicting.  Note: ', i, end='')  \n",
        "      if i%3 == 1:   \n",
        "          print('\\r', 'Predicting.. Note: ', i, end='')\n",
        "      if i%3 == 2:   \n",
        "          print('\\r', 'Predicting...Note: ', i, end='')\n",
        "      prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "#       prediction_values = np.arange(len(prediction[0]))\n",
        "#       prediction_prob = np.asarray(list(prediction[0])) / float(sum(prediction[0]))\n",
        "#       print(prediction_prob)\n",
        "\n",
        "      # Most probable note prediction\n",
        "      index = np.argmax(prediction)\n",
        "      note_instance = int2note[index]\n",
        "      prediction_output.append(note_instance)\n",
        "\n",
        "      # Predict based on prob dist\n",
        "#       index = np.random.choice(prediction_values, 1, p=prediction_prob)\n",
        "  #     print(index[0])\n",
        "  #     print(type(index[0]))\n",
        "#       note_instance = int2note[int(index[0])]\n",
        "#       prediction_output.append(note_instance)\n",
        "\n",
        "      pattern_sequence.append(index)\n",
        "      pattern_sequence = pattern_sequence[1:len(pattern_sequence)]    \n",
        "\n",
        "  prediction_output = prediction_output[300:len(prediction_output)]\n",
        "  # Write\n",
        "  with open(INTERMED_FOLDER + (\"%snotes_prediction_output\" % CORPUS), 'wb+') as filepath:\n",
        "      pickle.dump(prediction_output, filepath)\n",
        "  return prediction_output\n",
        "\n",
        "def output_midi(prediction_output, duration_prediction_output, dropout, model_size):\n",
        "  offset = 0\n",
        "  output_notes = []\n",
        "  for pattern, duration in zip(prediction_output, duration_prediction_output):\n",
        "\n",
        "      if ('.' in pattern) or pattern.isdigit():\n",
        "          chord_array = pattern.split('.')\n",
        "          chord_notes = []\n",
        "          for note_instance in chord_array:\n",
        "              note_object = note.Note(int(note_instance))\n",
        "              note_object.duration.quarterLength = duration\n",
        "              note_object.storedInstrument = instrument.Piano()\n",
        "              chord_notes.append(note_object)\n",
        "          chord_object = chord.Chord(chord_notes)\n",
        "          chord_object.offset = offset\n",
        "          output_notes.append(chord_object)\n",
        "      elif 'R' == pattern:\n",
        "          note_object = note.Rest()\n",
        "          note_object.duration.quarterLength = duration\n",
        "          note_object.offset = offset\n",
        "          output_notes.append(note_object)\n",
        "      else:\n",
        "          note_object = note.Note(pattern)\n",
        "          note_object.duration.quarterLength = duration\n",
        "          note_object.offset = offset\n",
        "          note_object.storedInstrument = instrument.Piano()\n",
        "          output_notes.append(note_object)\n",
        "\n",
        "      offset += 0.5\n",
        "\n",
        "  midi_stream = stream.Stream(output_notes)\n",
        "  midi_stream.write('midi', fp=MIDI_OUTPUT_FOLDER + CORPUS + '%s_%s.mid' % (dropout, model_size))\n",
        "  print('\\nWrote midi...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-kd2Y8v6bLeT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2 Preprocessing Optimization"
      ]
    },
    {
      "metadata": {
        "id": "XV7lYNDJakkh",
        "colab_type": "code",
        "outputId": "b55299bd-4011-4d3b-c3ed-a85a26b26a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14278
        }
      },
      "cell_type": "code",
      "source": [
        "notes_histories = {}\n",
        "duration_histories = {}\n",
        "\n",
        "dropout, model_size = DROPOUTS, MODEL_SIZES\n",
        "setup_plot(dropout, model_size)\n",
        "print('Running duration training on notewise with rests:%s and root extraction:%s' % (RESTS, ROOT_EXTRACTION))\n",
        "duration_callbacks, duration_weight_path = create_callback_list('duration', dropout, model_size)\n",
        "duration_model = create_model(duration_training_data, duration_vocab_size, model_size, dropout)\n",
        "duration_histories[(dropout, model_size)] = duration_model.fit(duration_training_data, duration_training_label, epochs=EPOCHS, batch_size=DURATION_BATCH_SIZE, callbacks=duration_callbacks, validation_split=0.2)\n",
        "plot_history(duration_histories[(dropout, model_size)], 'Durations', dropout, model_size)\n",
        "# output intermed duration\n",
        "duration_prediction = predict_duration(duration_model, duration_weight_path)\n",
        "\n",
        "print('\\n\\nRunning notes training on d:%s s:%s' % (dropout, model_size))\n",
        "notes_callbacks, note_weight_path = create_callback_list('notes', dropout, model_size)\n",
        "notes_model = create_model(training_data, vocab_size, model_size, dropout)\n",
        "notes_histories[(dropout, model_size)] = notes_model.fit(training_data, training_label, epochs=EPOCHS, batch_size=NOTE_BATCH_SIZE, callbacks=notes_callbacks, validation_split=0.2)\n",
        "plot_history(notes_histories[(dropout, model_size)], 'Notes', dropout, model_size)\n",
        "save_plot(GRAPHS_FOLDER + EDM_CORPUS + 'dropout=%s_size=%s' % (dropout, model_size))\n",
        "# output intermed notes\n",
        "note_prediction = predict_note(notes_model, note_weight_path)\n",
        "\n",
        "# output final midi\n",
        "output_midi(note_prediction, duration_prediction, dropout, model_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running duration training on notewise with rests:True and root extraction:True\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 10269 samples, validate on 2568 samples\n",
            "Epoch 1/200\n",
            "10269/10269 [==============================] - 10s 980us/step - loss: 2.1845 - acc: 0.3636 - val_loss: 1.9608 - val_acc: 0.2551\n",
            "Epoch 2/200\n",
            "10269/10269 [==============================] - 5s 467us/step - loss: 1.7054 - acc: 0.4735 - val_loss: 1.8558 - val_acc: 0.4225\n",
            "Epoch 3/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.5694 - acc: 0.4890 - val_loss: 1.8807 - val_acc: 0.4381\n",
            "Epoch 4/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.5420 - acc: 0.4953 - val_loss: 1.9999 - val_acc: 0.2543\n",
            "Epoch 5/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 1.5273 - acc: 0.4921 - val_loss: 1.9159 - val_acc: 0.4311\n",
            "Epoch 6/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.5156 - acc: 0.4970 - val_loss: 1.8575 - val_acc: 0.3914\n",
            "Epoch 7/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 1.5060 - acc: 0.4965 - val_loss: 1.8967 - val_acc: 0.3991\n",
            "Epoch 8/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 1.4943 - acc: 0.4887 - val_loss: 1.9072 - val_acc: 0.4393\n",
            "Epoch 9/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 1.4935 - acc: 0.4930 - val_loss: 1.9013 - val_acc: 0.4081\n",
            "Epoch 10/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 1.4871 - acc: 0.4919 - val_loss: 1.8846 - val_acc: 0.4085\n",
            "Epoch 11/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.4915 - acc: 0.4961 - val_loss: 1.8360 - val_acc: 0.4381\n",
            "Epoch 12/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 1.4652 - acc: 0.4950 - val_loss: 1.8981 - val_acc: 0.4280\n",
            "Epoch 13/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.5124 - acc: 0.4883 - val_loss: 1.8655 - val_acc: 0.4221\n",
            "Epoch 14/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 1.4583 - acc: 0.4958 - val_loss: 1.9506 - val_acc: 0.4221\n",
            "Epoch 15/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.4809 - acc: 0.4978 - val_loss: 1.9282 - val_acc: 0.3933\n",
            "Epoch 16/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 1.4471 - acc: 0.5051 - val_loss: 1.9676 - val_acc: 0.3863\n",
            "Epoch 17/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 1.4223 - acc: 0.5169 - val_loss: 1.9538 - val_acc: 0.3945\n",
            "Epoch 18/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 1.3812 - acc: 0.5224 - val_loss: 2.0467 - val_acc: 0.3859\n",
            "Epoch 19/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.3764 - acc: 0.5285 - val_loss: 2.0895 - val_acc: 0.3941\n",
            "Epoch 20/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.3731 - acc: 0.5318 - val_loss: 2.1296 - val_acc: 0.3727\n",
            "Epoch 21/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.3591 - acc: 0.5330 - val_loss: 2.0828 - val_acc: 0.3929\n",
            "Epoch 22/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.3043 - acc: 0.5440 - val_loss: 2.0363 - val_acc: 0.3762\n",
            "Epoch 23/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 1.3014 - acc: 0.5407 - val_loss: 2.1186 - val_acc: 0.3509\n",
            "Epoch 24/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 1.2708 - acc: 0.5472 - val_loss: 2.2183 - val_acc: 0.3840\n",
            "Epoch 25/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.2382 - acc: 0.5577 - val_loss: 2.2720 - val_acc: 0.3575\n",
            "Epoch 26/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 1.2300 - acc: 0.5566 - val_loss: 2.1959 - val_acc: 0.3921\n",
            "Epoch 27/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 1.2062 - acc: 0.5607 - val_loss: 2.2383 - val_acc: 0.3832\n",
            "Epoch 28/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 1.1757 - acc: 0.5723 - val_loss: 2.2565 - val_acc: 0.4011\n",
            "Epoch 29/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.1213 - acc: 0.5899 - val_loss: 2.4696 - val_acc: 0.3680\n",
            "Epoch 30/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 1.0878 - acc: 0.6067 - val_loss: 2.4483 - val_acc: 0.3555\n",
            "Epoch 31/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.1494 - acc: 0.5757 - val_loss: 2.3546 - val_acc: 0.3820\n",
            "Epoch 32/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.0297 - acc: 0.6124 - val_loss: 2.5336 - val_acc: 0.3610\n",
            "Epoch 33/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 1.0253 - acc: 0.6246 - val_loss: 2.5597 - val_acc: 0.3290\n",
            "Epoch 34/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.9713 - acc: 0.6387 - val_loss: 2.6289 - val_acc: 0.3715\n",
            "Epoch 35/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 1.0214 - acc: 0.6203 - val_loss: 2.5818 - val_acc: 0.3446\n",
            "Epoch 36/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.9226 - acc: 0.6612 - val_loss: 2.6575 - val_acc: 0.3516\n",
            "Epoch 37/200\n",
            "10269/10269 [==============================] - 5s 450us/step - loss: 0.8447 - acc: 0.6858 - val_loss: 2.6885 - val_acc: 0.3614\n",
            "Epoch 38/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.7580 - acc: 0.7236 - val_loss: 2.9932 - val_acc: 0.3575\n",
            "Epoch 39/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.7263 - acc: 0.7273 - val_loss: 3.0329 - val_acc: 0.3637\n",
            "Epoch 40/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.6629 - acc: 0.7554 - val_loss: 3.2302 - val_acc: 0.3392\n",
            "Epoch 41/200\n",
            "10269/10269 [==============================] - 5s 466us/step - loss: 1.0992 - acc: 0.6445 - val_loss: 2.1526 - val_acc: 0.3991\n",
            "Epoch 42/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 1.4877 - acc: 0.5042 - val_loss: 1.8896 - val_acc: 0.3824\n",
            "Epoch 43/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 1.2984 - acc: 0.5496 - val_loss: 1.9561 - val_acc: 0.3995\n",
            "Epoch 44/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 1.1770 - acc: 0.5772 - val_loss: 2.1628 - val_acc: 0.3769\n",
            "Epoch 45/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 1.1279 - acc: 0.6046 - val_loss: 2.1278 - val_acc: 0.3917\n",
            "Epoch 46/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 1.0401 - acc: 0.6223 - val_loss: 2.3433 - val_acc: 0.3333\n",
            "Epoch 47/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.9413 - acc: 0.6602 - val_loss: 2.4545 - val_acc: 0.3688\n",
            "Epoch 48/200\n",
            "10269/10269 [==============================] - 5s 450us/step - loss: 0.8886 - acc: 0.6780 - val_loss: 2.5414 - val_acc: 0.3505\n",
            "Epoch 49/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.8550 - acc: 0.6895 - val_loss: 2.7255 - val_acc: 0.3606\n",
            "Epoch 50/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.7939 - acc: 0.7080 - val_loss: 2.8104 - val_acc: 0.3590\n",
            "Epoch 51/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.6893 - acc: 0.7554 - val_loss: 2.7227 - val_acc: 0.3711\n",
            "Epoch 52/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.6224 - acc: 0.7792 - val_loss: 2.9585 - val_acc: 0.3524\n",
            "Epoch 53/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.5552 - acc: 0.8028 - val_loss: 2.9933 - val_acc: 0.3470\n",
            "Epoch 54/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.5168 - acc: 0.8196 - val_loss: 3.1772 - val_acc: 0.3781\n",
            "Epoch 55/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.4777 - acc: 0.8326 - val_loss: 3.3604 - val_acc: 0.3458\n",
            "Epoch 56/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.4135 - acc: 0.8521 - val_loss: 3.4165 - val_acc: 0.3536\n",
            "Epoch 57/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 0.3809 - acc: 0.8642 - val_loss: 3.4064 - val_acc: 0.3684\n",
            "Epoch 58/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.3718 - acc: 0.8703 - val_loss: 3.7151 - val_acc: 0.3583\n",
            "Epoch 59/200\n",
            "10269/10269 [==============================] - 5s 466us/step - loss: 0.3707 - acc: 0.8675 - val_loss: 3.9913 - val_acc: 0.3551\n",
            "Epoch 60/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.3116 - acc: 0.8907 - val_loss: 3.8229 - val_acc: 0.3551\n",
            "Epoch 61/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 0.3358 - acc: 0.8837 - val_loss: 3.9205 - val_acc: 0.3797\n",
            "Epoch 62/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.2916 - acc: 0.8991 - val_loss: 4.0246 - val_acc: 0.3937\n",
            "Epoch 63/200\n",
            "10269/10269 [==============================] - 5s 461us/step - loss: 0.2428 - acc: 0.9204 - val_loss: 4.1879 - val_acc: 0.3711\n",
            "Epoch 64/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.2096 - acc: 0.9287 - val_loss: 4.4578 - val_acc: 0.3680\n",
            "Epoch 65/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.2009 - acc: 0.9322 - val_loss: 4.4381 - val_acc: 0.3606\n",
            "Epoch 66/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.1864 - acc: 0.9393 - val_loss: 4.3804 - val_acc: 0.3633\n",
            "Epoch 67/200\n",
            "10269/10269 [==============================] - 5s 466us/step - loss: 0.2063 - acc: 0.9308 - val_loss: 4.5059 - val_acc: 0.3571\n",
            "Epoch 68/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.1807 - acc: 0.9399 - val_loss: 4.5589 - val_acc: 0.3637\n",
            "Epoch 69/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.1672 - acc: 0.9435 - val_loss: 4.5936 - val_acc: 0.3489\n",
            "Epoch 70/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.1593 - acc: 0.9461 - val_loss: 4.6973 - val_acc: 0.3551\n",
            "Epoch 71/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.1816 - acc: 0.9385 - val_loss: 4.6092 - val_acc: 0.3641\n",
            "Epoch 72/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.1609 - acc: 0.9459 - val_loss: 4.7082 - val_acc: 0.3493\n",
            "Epoch 73/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.1621 - acc: 0.9461 - val_loss: 4.8755 - val_acc: 0.3454\n",
            "Epoch 74/200\n",
            "10269/10269 [==============================] - 5s 461us/step - loss: 0.1287 - acc: 0.9588 - val_loss: 4.8215 - val_acc: 0.3602\n",
            "Epoch 75/200\n",
            "10269/10269 [==============================] - 5s 472us/step - loss: 0.1098 - acc: 0.9634 - val_loss: 4.7381 - val_acc: 0.3672\n",
            "Epoch 76/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.1215 - acc: 0.9606 - val_loss: 4.9357 - val_acc: 0.3828\n",
            "Epoch 77/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0979 - acc: 0.9680 - val_loss: 5.0343 - val_acc: 0.3602\n",
            "Epoch 78/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.0970 - acc: 0.9682 - val_loss: 5.0931 - val_acc: 0.3742\n",
            "Epoch 79/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.1058 - acc: 0.9654 - val_loss: 5.1928 - val_acc: 0.3618\n",
            "Epoch 80/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.1322 - acc: 0.9561 - val_loss: 5.0131 - val_acc: 0.3621\n",
            "Epoch 81/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.1155 - acc: 0.9622 - val_loss: 5.0211 - val_acc: 0.3832\n",
            "Epoch 82/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.1053 - acc: 0.9675 - val_loss: 4.9638 - val_acc: 0.3711\n",
            "Epoch 83/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.1063 - acc: 0.9658 - val_loss: 5.0968 - val_acc: 0.3773\n",
            "Epoch 84/200\n",
            "10269/10269 [==============================] - 5s 461us/step - loss: 0.1666 - acc: 0.9461 - val_loss: 5.0420 - val_acc: 0.3524\n",
            "Epoch 85/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.1228 - acc: 0.9605 - val_loss: 5.2570 - val_acc: 0.3649\n",
            "Epoch 86/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.1127 - acc: 0.9641 - val_loss: 4.9341 - val_acc: 0.3917\n",
            "Epoch 87/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.1006 - acc: 0.9670 - val_loss: 5.2261 - val_acc: 0.3680\n",
            "Epoch 88/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0911 - acc: 0.9699 - val_loss: 5.1107 - val_acc: 0.3727\n",
            "Epoch 89/200\n",
            "10269/10269 [==============================] - 5s 468us/step - loss: 0.0758 - acc: 0.9760 - val_loss: 5.3156 - val_acc: 0.3715\n",
            "Epoch 90/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0737 - acc: 0.9759 - val_loss: 5.3149 - val_acc: 0.3734\n",
            "Epoch 91/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.0697 - acc: 0.9775 - val_loss: 5.4919 - val_acc: 0.3641\n",
            "Epoch 92/200\n",
            "10269/10269 [==============================] - 5s 449us/step - loss: 0.0615 - acc: 0.9802 - val_loss: 5.3858 - val_acc: 0.3840\n",
            "Epoch 93/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0744 - acc: 0.9762 - val_loss: 5.3079 - val_acc: 0.3610\n",
            "Epoch 94/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0665 - acc: 0.9784 - val_loss: 5.4674 - val_acc: 0.3828\n",
            "Epoch 95/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.0638 - acc: 0.9799 - val_loss: 5.4892 - val_acc: 0.3738\n",
            "Epoch 96/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.0623 - acc: 0.9793 - val_loss: 5.3648 - val_acc: 0.3894\n",
            "Epoch 97/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0619 - acc: 0.9787 - val_loss: 5.3853 - val_acc: 0.3684\n",
            "Epoch 98/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0823 - acc: 0.9747 - val_loss: 5.4759 - val_acc: 0.3571\n",
            "Epoch 99/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0676 - acc: 0.9780 - val_loss: 5.3918 - val_acc: 0.3621\n",
            "Epoch 100/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.0647 - acc: 0.9792 - val_loss: 5.6337 - val_acc: 0.3625\n",
            "Epoch 101/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0781 - acc: 0.9758 - val_loss: 5.4201 - val_acc: 0.3805\n",
            "Epoch 102/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.1020 - acc: 0.9651 - val_loss: 5.4097 - val_acc: 0.3703\n",
            "Epoch 103/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.2641 - acc: 0.9163 - val_loss: 4.8326 - val_acc: 0.3855\n",
            "Epoch 104/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.1378 - acc: 0.9536 - val_loss: 5.1705 - val_acc: 0.3602\n",
            "Epoch 105/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.1137 - acc: 0.9617 - val_loss: 4.8971 - val_acc: 0.3637\n",
            "Epoch 106/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.0905 - acc: 0.9721 - val_loss: 4.9070 - val_acc: 0.3466\n",
            "Epoch 107/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.0819 - acc: 0.9740 - val_loss: 5.2485 - val_acc: 0.3520\n",
            "Epoch 108/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.0802 - acc: 0.9753 - val_loss: 5.2178 - val_acc: 0.3766\n",
            "Epoch 109/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.0607 - acc: 0.9796 - val_loss: 5.2289 - val_acc: 0.3633\n",
            "Epoch 110/200\n",
            "10269/10269 [==============================] - 5s 469us/step - loss: 0.0701 - acc: 0.9793 - val_loss: 5.1912 - val_acc: 0.3812\n",
            "Epoch 111/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.0608 - acc: 0.9810 - val_loss: 5.2225 - val_acc: 0.3836\n",
            "Epoch 112/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0602 - acc: 0.9799 - val_loss: 5.2856 - val_acc: 0.3824\n",
            "Epoch 113/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0541 - acc: 0.9826 - val_loss: 5.3629 - val_acc: 0.3816\n",
            "Epoch 114/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0511 - acc: 0.9828 - val_loss: 5.4029 - val_acc: 0.3894\n",
            "Epoch 115/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.0531 - acc: 0.9819 - val_loss: 5.4017 - val_acc: 0.3847\n",
            "Epoch 116/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0541 - acc: 0.9814 - val_loss: 5.1725 - val_acc: 0.3610\n",
            "Epoch 117/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.0615 - acc: 0.9811 - val_loss: 5.5021 - val_acc: 0.3910\n",
            "Epoch 118/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.0623 - acc: 0.9801 - val_loss: 5.3069 - val_acc: 0.3879\n",
            "Epoch 119/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0630 - acc: 0.9805 - val_loss: 5.3441 - val_acc: 0.3660\n",
            "Epoch 120/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.0551 - acc: 0.9827 - val_loss: 5.3308 - val_acc: 0.3793\n",
            "Epoch 121/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.0577 - acc: 0.9811 - val_loss: 5.4692 - val_acc: 0.3847\n",
            "Epoch 122/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.0559 - acc: 0.9810 - val_loss: 5.6127 - val_acc: 0.3528\n",
            "Epoch 123/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0701 - acc: 0.9781 - val_loss: 5.3999 - val_acc: 0.3512\n",
            "Epoch 124/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0666 - acc: 0.9790 - val_loss: 5.5211 - val_acc: 0.3606\n",
            "Epoch 125/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0549 - acc: 0.9817 - val_loss: 5.3652 - val_acc: 0.3859\n",
            "Epoch 126/200\n",
            "10269/10269 [==============================] - 5s 459us/step - loss: 0.0528 - acc: 0.9826 - val_loss: 5.4358 - val_acc: 0.3672\n",
            "Epoch 127/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0634 - acc: 0.9814 - val_loss: 5.4859 - val_acc: 0.3664\n",
            "Epoch 128/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0510 - acc: 0.9834 - val_loss: 5.4690 - val_acc: 0.3769\n",
            "Epoch 129/200\n",
            "10269/10269 [==============================] - 5s 466us/step - loss: 0.0466 - acc: 0.9833 - val_loss: 5.5531 - val_acc: 0.3851\n",
            "Epoch 130/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.0469 - acc: 0.9836 - val_loss: 5.4670 - val_acc: 0.3660\n",
            "Epoch 131/200\n",
            "10269/10269 [==============================] - 5s 458us/step - loss: 0.0485 - acc: 0.9835 - val_loss: 5.5445 - val_acc: 0.3758\n",
            "Epoch 132/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.0505 - acc: 0.9833 - val_loss: 5.6467 - val_acc: 0.3734\n",
            "Epoch 133/200\n",
            "10269/10269 [==============================] - 5s 461us/step - loss: 0.0806 - acc: 0.9739 - val_loss: 5.5823 - val_acc: 0.3750\n",
            "Epoch 134/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.2342 - acc: 0.9280 - val_loss: 5.0975 - val_acc: 0.3544\n",
            "Epoch 135/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.2233 - acc: 0.9291 - val_loss: 4.9513 - val_acc: 0.3699\n",
            "Epoch 136/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.0936 - acc: 0.9701 - val_loss: 5.2031 - val_acc: 0.3579\n",
            "Epoch 137/200\n",
            "10269/10269 [==============================] - 5s 460us/step - loss: 0.0674 - acc: 0.9782 - val_loss: 5.1810 - val_acc: 0.3637\n",
            "Epoch 138/200\n",
            "10269/10269 [==============================] - 5s 457us/step - loss: 0.0554 - acc: 0.9827 - val_loss: 5.2605 - val_acc: 0.3493\n",
            "Epoch 139/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0450 - acc: 0.9847 - val_loss: 5.2991 - val_acc: 0.3571\n",
            "Epoch 140/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0430 - acc: 0.9849 - val_loss: 5.4096 - val_acc: 0.3489\n",
            "Epoch 141/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0465 - acc: 0.9849 - val_loss: 5.3762 - val_acc: 0.3672\n",
            "Epoch 142/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0434 - acc: 0.9851 - val_loss: 5.3514 - val_acc: 0.3633\n",
            "Epoch 143/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0430 - acc: 0.9846 - val_loss: 5.4716 - val_acc: 0.3528\n",
            "Epoch 144/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0411 - acc: 0.9859 - val_loss: 5.4619 - val_acc: 0.3586\n",
            "Epoch 145/200\n",
            "10269/10269 [==============================] - 5s 450us/step - loss: 0.0415 - acc: 0.9852 - val_loss: 5.4220 - val_acc: 0.3637\n",
            "Epoch 146/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.0448 - acc: 0.9847 - val_loss: 5.4742 - val_acc: 0.3789\n",
            "Epoch 147/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0505 - acc: 0.9839 - val_loss: 5.4016 - val_acc: 0.3699\n",
            "Epoch 148/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 0.0551 - acc: 0.9833 - val_loss: 5.3857 - val_acc: 0.3762\n",
            "Epoch 149/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0552 - acc: 0.9846 - val_loss: 5.4358 - val_acc: 0.3664\n",
            "Epoch 150/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0524 - acc: 0.9841 - val_loss: 5.3663 - val_acc: 0.3851\n",
            "Epoch 151/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0542 - acc: 0.9840 - val_loss: 5.6223 - val_acc: 0.3738\n",
            "Epoch 152/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0536 - acc: 0.9841 - val_loss: 5.4554 - val_acc: 0.3719\n",
            "Epoch 153/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0502 - acc: 0.9853 - val_loss: 5.5297 - val_acc: 0.3949\n",
            "Epoch 154/200\n",
            "10269/10269 [==============================] - 5s 456us/step - loss: 0.0531 - acc: 0.9835 - val_loss: 5.5898 - val_acc: 0.3719\n",
            "Epoch 155/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0566 - acc: 0.9830 - val_loss: 5.4392 - val_acc: 0.3750\n",
            "Epoch 156/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.1319 - acc: 0.9605 - val_loss: 5.4337 - val_acc: 0.3637\n",
            "Epoch 157/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.1237 - acc: 0.9607 - val_loss: 5.4131 - val_acc: 0.3707\n",
            "Epoch 158/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0727 - acc: 0.9747 - val_loss: 5.4553 - val_acc: 0.3684\n",
            "Epoch 159/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0828 - acc: 0.9731 - val_loss: 5.4024 - val_acc: 0.3781\n",
            "Epoch 160/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.0624 - acc: 0.9795 - val_loss: 5.4129 - val_acc: 0.3637\n",
            "Epoch 161/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0516 - acc: 0.9832 - val_loss: 5.6924 - val_acc: 0.3731\n",
            "Epoch 162/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0751 - acc: 0.9767 - val_loss: 5.1859 - val_acc: 0.3692\n",
            "Epoch 163/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0508 - acc: 0.9831 - val_loss: 5.5066 - val_acc: 0.3879\n",
            "Epoch 164/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0482 - acc: 0.9842 - val_loss: 5.3876 - val_acc: 0.3754\n",
            "Epoch 165/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0478 - acc: 0.9839 - val_loss: 5.3576 - val_acc: 0.3715\n",
            "Epoch 166/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0424 - acc: 0.9853 - val_loss: 5.4770 - val_acc: 0.3797\n",
            "Epoch 167/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0410 - acc: 0.9853 - val_loss: 5.5375 - val_acc: 0.3773\n",
            "Epoch 168/200\n",
            "10269/10269 [==============================] - 5s 462us/step - loss: 0.0394 - acc: 0.9849 - val_loss: 5.5416 - val_acc: 0.3859\n",
            "Epoch 169/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0399 - acc: 0.9856 - val_loss: 5.5380 - val_acc: 0.3758\n",
            "Epoch 170/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 0.0420 - acc: 0.9843 - val_loss: 5.4885 - val_acc: 0.3758\n",
            "Epoch 171/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0432 - acc: 0.9859 - val_loss: 5.5456 - val_acc: 0.3695\n",
            "Epoch 172/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0426 - acc: 0.9860 - val_loss: 5.5401 - val_acc: 0.3820\n",
            "Epoch 173/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 0.0430 - acc: 0.9854 - val_loss: 5.3618 - val_acc: 0.3762\n",
            "Epoch 174/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0411 - acc: 0.9856 - val_loss: 5.4602 - val_acc: 0.3789\n",
            "Epoch 175/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.0516 - acc: 0.9824 - val_loss: 5.3515 - val_acc: 0.3734\n",
            "Epoch 176/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0679 - acc: 0.9762 - val_loss: 5.2824 - val_acc: 0.3925\n",
            "Epoch 177/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0773 - acc: 0.9760 - val_loss: 5.5097 - val_acc: 0.3544\n",
            "Epoch 178/200\n",
            "10269/10269 [==============================] - 5s 449us/step - loss: 0.0593 - acc: 0.9800 - val_loss: 5.2500 - val_acc: 0.3769\n",
            "Epoch 179/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0806 - acc: 0.9765 - val_loss: 5.6764 - val_acc: 0.3583\n",
            "Epoch 180/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0664 - acc: 0.9778 - val_loss: 5.6941 - val_acc: 0.3590\n",
            "Epoch 181/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0607 - acc: 0.9801 - val_loss: 5.6336 - val_acc: 0.3594\n",
            "Epoch 182/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.0537 - acc: 0.9809 - val_loss: 5.5128 - val_acc: 0.3812\n",
            "Epoch 183/200\n",
            "10269/10269 [==============================] - 5s 449us/step - loss: 0.0514 - acc: 0.9832 - val_loss: 5.5197 - val_acc: 0.3832\n",
            "Epoch 184/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0640 - acc: 0.9802 - val_loss: 5.4737 - val_acc: 0.3571\n",
            "Epoch 185/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0511 - acc: 0.9836 - val_loss: 5.4780 - val_acc: 0.3832\n",
            "Epoch 186/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 0.0462 - acc: 0.9857 - val_loss: 5.5753 - val_acc: 0.3820\n",
            "Epoch 187/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0568 - acc: 0.9830 - val_loss: 5.4678 - val_acc: 0.3703\n",
            "Epoch 188/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0582 - acc: 0.9816 - val_loss: 5.6597 - val_acc: 0.3680\n",
            "Epoch 189/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0628 - acc: 0.9776 - val_loss: 5.4817 - val_acc: 0.3960\n",
            "Epoch 190/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.0576 - acc: 0.9813 - val_loss: 5.5190 - val_acc: 0.3879\n",
            "Epoch 191/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0573 - acc: 0.9823 - val_loss: 5.3161 - val_acc: 0.3590\n",
            "Epoch 192/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0732 - acc: 0.9777 - val_loss: 5.3183 - val_acc: 0.3812\n",
            "Epoch 193/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0889 - acc: 0.9742 - val_loss: 5.4038 - val_acc: 0.3952\n",
            "Epoch 194/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0605 - acc: 0.9796 - val_loss: 5.3586 - val_acc: 0.3952\n",
            "Epoch 195/200\n",
            "10269/10269 [==============================] - 5s 452us/step - loss: 0.0585 - acc: 0.9811 - val_loss: 5.4526 - val_acc: 0.3719\n",
            "Epoch 196/200\n",
            "10269/10269 [==============================] - 5s 454us/step - loss: 0.0603 - acc: 0.9791 - val_loss: 5.4211 - val_acc: 0.3984\n",
            "Epoch 197/200\n",
            "10269/10269 [==============================] - 5s 453us/step - loss: 0.0530 - acc: 0.9824 - val_loss: 5.4066 - val_acc: 0.3812\n",
            "Epoch 198/200\n",
            "10269/10269 [==============================] - 5s 455us/step - loss: 0.0476 - acc: 0.9833 - val_loss: 5.3475 - val_acc: 0.3968\n",
            "Epoch 199/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 0.0412 - acc: 0.9843 - val_loss: 5.3200 - val_acc: 0.3972\n",
            "Epoch 200/200\n",
            "10269/10269 [==============================] - 5s 451us/step - loss: 0.0445 - acc: 0.9842 - val_loss: 5.3778 - val_acc: 0.3933\n",
            "{0: 0.0, 1: Fraction(1, 12), 2: Fraction(1, 6), 3: 0.25, 4: Fraction(1, 3), 5: Fraction(5, 12), 6: 0.5, 7: Fraction(2, 3), 8: 0.75, 9: 1.0, 10: Fraction(7, 6), 11: 1.25, 12: Fraction(4, 3), 13: 1.5, 14: Fraction(5, 3), 15: 1.75, 16: 2.0, 17: 2.25, 18: 2.5, 19: 3.0, 20: 3.25, 21: 3.5, 22: 4.0, 23: Fraction(25, 6), 24: 4.25, 25: 4.5, 26: 4.75, 27: 5.0, 28: 5.25, 29: 5.75, 30: 6.0, 31: 6.5, 32: 7.0, 33: Fraction(23, 3), 34: 8.0, 35: 8.5, 36: 9.0, 37: 14.5, 38: 16.0, 39: 18.0, 40: 27.5, 41: 32.0, 42: 32.5, 43: 36.0, 44: 36.25, 45: 39.5, 46: 47.5, 47: 48.0, 48: 61.0, 49: 64.25, 50: 68.0, 51: 72.25, 52: 72.5, 53: 80.0, 54: 84.25, 55: Fraction(553, 6), 56: 94.5, 57: 128.25, 58: 161.0, 59: 176.0, 60: 200.5}\n",
            " Predicting.  Duration:  399\n",
            "\n",
            "Running notes training on d:0 s:256\n",
            "Train on 9113 samples, validate on 2279 samples\n",
            "Epoch 1/200\n",
            "9113/9113 [==============================] - 14s 2ms/step - loss: 3.1077 - acc: 0.3792 - val_loss: 3.3134 - val_acc: 0.3414\n",
            "Epoch 2/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 3.0278 - acc: 0.3858 - val_loss: 3.2608 - val_acc: 0.3414\n",
            "Epoch 3/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 3.0192 - acc: 0.3858 - val_loss: 3.3333 - val_acc: 0.3414\n",
            "Epoch 4/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 3.0007 - acc: 0.3855 - val_loss: 3.3447 - val_acc: 0.3414\n",
            "Epoch 5/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.9727 - acc: 0.3856 - val_loss: 3.3634 - val_acc: 0.3361\n",
            "Epoch 6/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.9439 - acc: 0.3857 - val_loss: 3.5108 - val_acc: 0.3414\n",
            "Epoch 7/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.8756 - acc: 0.3852 - val_loss: 3.5060 - val_acc: 0.3361\n",
            "Epoch 8/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.7749 - acc: 0.3883 - val_loss: 3.6151 - val_acc: 0.3265\n",
            "Epoch 9/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.6332 - acc: 0.3912 - val_loss: 3.5912 - val_acc: 0.3168\n",
            "Epoch 10/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.3476 - acc: 0.4248 - val_loss: 3.7592 - val_acc: 0.2826\n",
            "Epoch 11/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.0531 - acc: 0.4659 - val_loss: 3.9301 - val_acc: 0.2685\n",
            "Epoch 12/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 1.7861 - acc: 0.5086 - val_loss: 4.0962 - val_acc: 0.2892\n",
            "Epoch 13/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 1.4872 - acc: 0.5645 - val_loss: 4.2864 - val_acc: 0.2580\n",
            "Epoch 14/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 1.2653 - acc: 0.6140 - val_loss: 4.4206 - val_acc: 0.2317\n",
            "Epoch 15/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 1.0570 - acc: 0.6666 - val_loss: 4.4826 - val_acc: 0.2510\n",
            "Epoch 16/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.8987 - acc: 0.7157 - val_loss: 4.8600 - val_acc: 0.2470\n",
            "Epoch 17/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.7704 - acc: 0.7631 - val_loss: 4.8903 - val_acc: 0.2470\n",
            "Epoch 18/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.6258 - acc: 0.8034 - val_loss: 5.3502 - val_acc: 0.2154\n",
            "Epoch 19/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.5052 - acc: 0.8471 - val_loss: 5.4908 - val_acc: 0.2062\n",
            "Epoch 20/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.4183 - acc: 0.8752 - val_loss: 5.8678 - val_acc: 0.1992\n",
            "Epoch 21/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.3548 - acc: 0.8972 - val_loss: 5.9913 - val_acc: 0.2229\n",
            "Epoch 22/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.2816 - acc: 0.9215 - val_loss: 6.1948 - val_acc: 0.1948\n",
            "Epoch 23/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.2409 - acc: 0.9313 - val_loss: 6.2902 - val_acc: 0.2159\n",
            "Epoch 24/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1989 - acc: 0.9445 - val_loss: 6.4169 - val_acc: 0.2290\n",
            "Epoch 25/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1688 - acc: 0.9550 - val_loss: 6.6493 - val_acc: 0.2093\n",
            "Epoch 26/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1610 - acc: 0.9562 - val_loss: 6.8448 - val_acc: 0.2027\n",
            "Epoch 27/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1253 - acc: 0.9670 - val_loss: 7.0014 - val_acc: 0.2185\n",
            "Epoch 28/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1073 - acc: 0.9725 - val_loss: 6.9748 - val_acc: 0.2115\n",
            "Epoch 29/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1130 - acc: 0.9682 - val_loss: 7.2240 - val_acc: 0.2124\n",
            "Epoch 30/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1311 - acc: 0.9652 - val_loss: 7.1642 - val_acc: 0.2216\n",
            "Epoch 31/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0944 - acc: 0.9754 - val_loss: 7.3875 - val_acc: 0.2163\n",
            "Epoch 32/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0961 - acc: 0.9743 - val_loss: 7.7255 - val_acc: 0.1900\n",
            "Epoch 33/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1199 - acc: 0.9671 - val_loss: 7.2743 - val_acc: 0.2185\n",
            "Epoch 34/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1064 - acc: 0.9710 - val_loss: 7.5086 - val_acc: 0.2176\n",
            "Epoch 35/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0613 - acc: 0.9842 - val_loss: 7.3855 - val_acc: 0.2220\n",
            "Epoch 36/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0470 - acc: 0.9883 - val_loss: 7.5595 - val_acc: 0.2326\n",
            "Epoch 37/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0449 - acc: 0.9869 - val_loss: 7.6837 - val_acc: 0.2133\n",
            "Epoch 38/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0469 - acc: 0.9866 - val_loss: 7.8708 - val_acc: 0.2168\n",
            "Epoch 39/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0418 - acc: 0.9889 - val_loss: 7.7236 - val_acc: 0.2102\n",
            "Epoch 40/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0426 - acc: 0.9884 - val_loss: 7.7878 - val_acc: 0.2045\n",
            "Epoch 41/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0497 - acc: 0.9868 - val_loss: 7.9492 - val_acc: 0.2058\n",
            "Epoch 42/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0356 - acc: 0.9899 - val_loss: 7.9328 - val_acc: 0.2128\n",
            "Epoch 43/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0330 - acc: 0.9898 - val_loss: 7.9232 - val_acc: 0.2133\n",
            "Epoch 44/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0331 - acc: 0.9899 - val_loss: 7.8222 - val_acc: 0.2137\n",
            "Epoch 45/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0367 - acc: 0.9894 - val_loss: 7.8816 - val_acc: 0.2115\n",
            "Epoch 46/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0457 - acc: 0.9867 - val_loss: 8.0027 - val_acc: 0.1922\n",
            "Epoch 47/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.2012 - acc: 0.9387 - val_loss: 7.9292 - val_acc: 0.2211\n",
            "Epoch 48/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1395 - acc: 0.9592 - val_loss: 7.9701 - val_acc: 0.2093\n",
            "Epoch 49/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0760 - acc: 0.9799 - val_loss: 7.8504 - val_acc: 0.2172\n",
            "Epoch 50/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0608 - acc: 0.9843 - val_loss: 8.0556 - val_acc: 0.2111\n",
            "Epoch 51/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0499 - acc: 0.9858 - val_loss: 8.0724 - val_acc: 0.2049\n",
            "Epoch 52/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0585 - acc: 0.9855 - val_loss: 8.0021 - val_acc: 0.2203\n",
            "Epoch 53/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0375 - acc: 0.9901 - val_loss: 8.0671 - val_acc: 0.2005\n",
            "Epoch 54/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0352 - acc: 0.9899 - val_loss: 8.1393 - val_acc: 0.2014\n",
            "Epoch 55/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0353 - acc: 0.9901 - val_loss: 8.0572 - val_acc: 0.2071\n",
            "Epoch 56/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0375 - acc: 0.9890 - val_loss: 8.0171 - val_acc: 0.1900\n",
            "Epoch 57/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0291 - acc: 0.9912 - val_loss: 8.0527 - val_acc: 0.2168\n",
            "Epoch 58/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0300 - acc: 0.9902 - val_loss: 8.0367 - val_acc: 0.2102\n",
            "Epoch 59/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0277 - acc: 0.9910 - val_loss: 8.0697 - val_acc: 0.2286\n",
            "Epoch 60/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0301 - acc: 0.9906 - val_loss: 8.1703 - val_acc: 0.2277\n",
            "Epoch 61/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0307 - acc: 0.9913 - val_loss: 8.2246 - val_acc: 0.2027\n",
            "Epoch 62/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0268 - acc: 0.9920 - val_loss: 8.2007 - val_acc: 0.2146\n",
            "Epoch 63/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0291 - acc: 0.9910 - val_loss: 8.2392 - val_acc: 0.2089\n",
            "Epoch 64/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0318 - acc: 0.9909 - val_loss: 8.2489 - val_acc: 0.2054\n",
            "Epoch 65/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0268 - acc: 0.9917 - val_loss: 8.1837 - val_acc: 0.2032\n",
            "Epoch 66/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0255 - acc: 0.9911 - val_loss: 8.2391 - val_acc: 0.2115\n",
            "Epoch 67/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0285 - acc: 0.9910 - val_loss: 8.1939 - val_acc: 0.2269\n",
            "Epoch 68/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0334 - acc: 0.9900 - val_loss: 8.2158 - val_acc: 0.2071\n",
            "Epoch 69/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0277 - acc: 0.9910 - val_loss: 8.4109 - val_acc: 0.2119\n",
            "Epoch 70/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.2516 - acc: 0.9289 - val_loss: 8.2955 - val_acc: 0.1729\n",
            "Epoch 71/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.3619 - acc: 0.8884 - val_loss: 7.7756 - val_acc: 0.2115\n",
            "Epoch 72/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0951 - acc: 0.9703 - val_loss: 7.7737 - val_acc: 0.2128\n",
            "Epoch 73/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0417 - acc: 0.9869 - val_loss: 7.8884 - val_acc: 0.2198\n",
            "Epoch 74/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0322 - acc: 0.9895 - val_loss: 7.9110 - val_acc: 0.2383\n",
            "Epoch 75/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0312 - acc: 0.9897 - val_loss: 8.0331 - val_acc: 0.2251\n",
            "Epoch 76/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0247 - acc: 0.9917 - val_loss: 7.9712 - val_acc: 0.2207\n",
            "Epoch 77/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0255 - acc: 0.9911 - val_loss: 7.9814 - val_acc: 0.2251\n",
            "Epoch 78/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0237 - acc: 0.9913 - val_loss: 7.9722 - val_acc: 0.2273\n",
            "Epoch 79/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0240 - acc: 0.9909 - val_loss: 8.0868 - val_acc: 0.2387\n",
            "Epoch 80/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0236 - acc: 0.9921 - val_loss: 8.0695 - val_acc: 0.2075\n",
            "Epoch 81/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0225 - acc: 0.9919 - val_loss: 8.0731 - val_acc: 0.2176\n",
            "Epoch 82/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0227 - acc: 0.9920 - val_loss: 8.0724 - val_acc: 0.2198\n",
            "Epoch 83/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0249 - acc: 0.9919 - val_loss: 8.0847 - val_acc: 0.2203\n",
            "Epoch 84/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0237 - acc: 0.9921 - val_loss: 8.1052 - val_acc: 0.2330\n",
            "Epoch 85/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0228 - acc: 0.9918 - val_loss: 8.0231 - val_acc: 0.2286\n",
            "Epoch 86/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0220 - acc: 0.9919 - val_loss: 8.1302 - val_acc: 0.2185\n",
            "Epoch 87/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0215 - acc: 0.9918 - val_loss: 8.1137 - val_acc: 0.2238\n",
            "Epoch 88/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0259 - acc: 0.9919 - val_loss: 8.1685 - val_acc: 0.2383\n",
            "Epoch 89/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0249 - acc: 0.9912 - val_loss: 8.1068 - val_acc: 0.2435\n",
            "Epoch 90/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0248 - acc: 0.9919 - val_loss: 8.1028 - val_acc: 0.2290\n",
            "Epoch 91/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0259 - acc: 0.9916 - val_loss: 8.1240 - val_acc: 0.2255\n",
            "Epoch 92/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0260 - acc: 0.9911 - val_loss: 8.1658 - val_acc: 0.2286\n",
            "Epoch 93/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0287 - acc: 0.9902 - val_loss: 8.2567 - val_acc: 0.2185\n",
            "Epoch 94/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0524 - acc: 0.9844 - val_loss: 8.4821 - val_acc: 0.2045\n",
            "Epoch 95/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.2189 - acc: 0.9341 - val_loss: 8.2261 - val_acc: 0.1992\n",
            "Epoch 96/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1205 - acc: 0.9652 - val_loss: 8.4266 - val_acc: 0.1966\n",
            "Epoch 97/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0978 - acc: 0.9683 - val_loss: 8.3298 - val_acc: 0.2194\n",
            "Epoch 98/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0438 - acc: 0.9866 - val_loss: 8.2731 - val_acc: 0.2238\n",
            "Epoch 99/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0421 - acc: 0.9865 - val_loss: 8.2656 - val_acc: 0.2185\n",
            "Epoch 100/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0318 - acc: 0.9901 - val_loss: 8.3536 - val_acc: 0.1970\n",
            "Epoch 101/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0279 - acc: 0.9916 - val_loss: 8.3841 - val_acc: 0.2229\n",
            "Epoch 102/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0248 - acc: 0.9910 - val_loss: 8.3319 - val_acc: 0.2089\n",
            "Epoch 103/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0244 - acc: 0.9910 - val_loss: 8.3897 - val_acc: 0.2269\n",
            "Epoch 104/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0233 - acc: 0.9923 - val_loss: 8.3431 - val_acc: 0.2049\n",
            "Epoch 105/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0234 - acc: 0.9917 - val_loss: 8.3557 - val_acc: 0.2229\n",
            "Epoch 106/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0267 - acc: 0.9913 - val_loss: 8.4048 - val_acc: 0.2176\n",
            "Epoch 107/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0214 - acc: 0.9924 - val_loss: 8.4111 - val_acc: 0.2260\n",
            "Epoch 108/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0227 - acc: 0.9916 - val_loss: 8.3958 - val_acc: 0.2216\n",
            "Epoch 109/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0219 - acc: 0.9913 - val_loss: 8.4566 - val_acc: 0.2115\n",
            "Epoch 110/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0213 - acc: 0.9919 - val_loss: 8.4109 - val_acc: 0.2071\n",
            "Epoch 111/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0261 - acc: 0.9906 - val_loss: 8.3155 - val_acc: 0.2216\n",
            "Epoch 112/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0245 - acc: 0.9922 - val_loss: 8.3194 - val_acc: 0.2290\n",
            "Epoch 113/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0235 - acc: 0.9922 - val_loss: 8.3290 - val_acc: 0.2150\n",
            "Epoch 114/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0228 - acc: 0.9922 - val_loss: 8.3169 - val_acc: 0.2304\n",
            "Epoch 115/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0232 - acc: 0.9918 - val_loss: 8.3226 - val_acc: 0.2198\n",
            "Epoch 116/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0230 - acc: 0.9921 - val_loss: 8.4289 - val_acc: 0.2290\n",
            "Epoch 117/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0263 - acc: 0.9912 - val_loss: 8.4545 - val_acc: 0.2159\n",
            "Epoch 118/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0238 - acc: 0.9922 - val_loss: 8.2814 - val_acc: 0.2194\n",
            "Epoch 119/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0922 - acc: 0.9721 - val_loss: 8.2616 - val_acc: 0.1891\n",
            "Epoch 120/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1138 - acc: 0.9694 - val_loss: 8.4617 - val_acc: 0.2273\n",
            "Epoch 121/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0834 - acc: 0.9748 - val_loss: 8.6372 - val_acc: 0.2014\n",
            "Epoch 122/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0557 - acc: 0.9828 - val_loss: 8.4718 - val_acc: 0.2295\n",
            "Epoch 123/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0349 - acc: 0.9889 - val_loss: 8.6079 - val_acc: 0.2440\n",
            "Epoch 124/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0277 - acc: 0.9912 - val_loss: 8.5378 - val_acc: 0.2247\n",
            "Epoch 125/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0260 - acc: 0.9909 - val_loss: 8.4633 - val_acc: 0.2356\n",
            "Epoch 126/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0242 - acc: 0.9920 - val_loss: 8.5368 - val_acc: 0.2308\n",
            "Epoch 127/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0232 - acc: 0.9911 - val_loss: 8.5652 - val_acc: 0.2352\n",
            "Epoch 128/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0212 - acc: 0.9921 - val_loss: 8.5228 - val_acc: 0.2352\n",
            "Epoch 129/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0206 - acc: 0.9921 - val_loss: 8.5030 - val_acc: 0.2374\n",
            "Epoch 130/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0238 - acc: 0.9912 - val_loss: 8.5983 - val_acc: 0.2312\n",
            "Epoch 131/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0248 - acc: 0.9919 - val_loss: 8.6380 - val_acc: 0.2264\n",
            "Epoch 132/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0219 - acc: 0.9918 - val_loss: 8.5567 - val_acc: 0.2273\n",
            "Epoch 133/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0202 - acc: 0.9917 - val_loss: 8.5356 - val_acc: 0.2233\n",
            "Epoch 134/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0197 - acc: 0.9924 - val_loss: 8.5389 - val_acc: 0.2466\n",
            "Epoch 135/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0211 - acc: 0.9910 - val_loss: 8.4866 - val_acc: 0.2356\n",
            "Epoch 136/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0243 - acc: 0.9921 - val_loss: 8.4866 - val_acc: 0.2466\n",
            "Epoch 137/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0238 - acc: 0.9923 - val_loss: 8.4620 - val_acc: 0.2299\n",
            "Epoch 138/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0250 - acc: 0.9916 - val_loss: 8.5320 - val_acc: 0.2391\n",
            "Epoch 139/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0258 - acc: 0.9920 - val_loss: 8.4666 - val_acc: 0.2383\n",
            "Epoch 140/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0279 - acc: 0.9909 - val_loss: 8.5688 - val_acc: 0.2361\n",
            "Epoch 141/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0312 - acc: 0.9903 - val_loss: 8.3760 - val_acc: 0.2435\n",
            "Epoch 142/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0460 - acc: 0.9872 - val_loss: 8.6288 - val_acc: 0.2334\n",
            "Epoch 143/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0519 - acc: 0.9841 - val_loss: 8.4726 - val_acc: 0.2352\n",
            "Epoch 144/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0701 - acc: 0.9784 - val_loss: 8.6675 - val_acc: 0.2168\n",
            "Epoch 145/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0644 - acc: 0.9804 - val_loss: 8.6066 - val_acc: 0.2062\n",
            "Epoch 146/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0531 - acc: 0.9838 - val_loss: 8.5679 - val_acc: 0.2308\n",
            "Epoch 147/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0587 - acc: 0.9832 - val_loss: 8.6023 - val_acc: 0.2304\n",
            "Epoch 148/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0440 - acc: 0.9857 - val_loss: 8.6226 - val_acc: 0.2211\n",
            "Epoch 149/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0306 - acc: 0.9888 - val_loss: 8.7961 - val_acc: 0.2229\n",
            "Epoch 150/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0309 - acc: 0.9902 - val_loss: 8.8469 - val_acc: 0.2207\n",
            "Epoch 151/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0256 - acc: 0.9905 - val_loss: 8.6957 - val_acc: 0.2185\n",
            "Epoch 152/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0267 - acc: 0.9911 - val_loss: 8.6891 - val_acc: 0.2352\n",
            "Epoch 153/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0285 - acc: 0.9906 - val_loss: 8.8513 - val_acc: 0.2225\n",
            "Epoch 154/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0234 - acc: 0.9920 - val_loss: 8.6617 - val_acc: 0.2229\n",
            "Epoch 155/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0205 - acc: 0.9921 - val_loss: 8.6457 - val_acc: 0.2431\n",
            "Epoch 156/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0254 - acc: 0.9907 - val_loss: 8.6892 - val_acc: 0.2247\n",
            "Epoch 157/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0232 - acc: 0.9917 - val_loss: 8.6530 - val_acc: 0.2233\n",
            "Epoch 158/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0211 - acc: 0.9920 - val_loss: 8.6195 - val_acc: 0.2203\n",
            "Epoch 159/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0189 - acc: 0.9914 - val_loss: 8.6534 - val_acc: 0.2352\n",
            "Epoch 160/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0251 - acc: 0.9913 - val_loss: 8.5856 - val_acc: 0.2269\n",
            "Epoch 161/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0207 - acc: 0.9912 - val_loss: 8.6338 - val_acc: 0.2203\n",
            "Epoch 162/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0237 - acc: 0.9922 - val_loss: 8.6217 - val_acc: 0.2312\n",
            "Epoch 163/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0209 - acc: 0.9914 - val_loss: 8.7263 - val_acc: 0.2356\n",
            "Epoch 164/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0214 - acc: 0.9921 - val_loss: 8.7478 - val_acc: 0.2198\n",
            "Epoch 165/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0189 - acc: 0.9923 - val_loss: 8.5642 - val_acc: 0.2133\n",
            "Epoch 166/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0195 - acc: 0.9922 - val_loss: 8.6807 - val_acc: 0.2440\n",
            "Epoch 167/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0193 - acc: 0.9917 - val_loss: 8.6145 - val_acc: 0.2343\n",
            "Epoch 168/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0184 - acc: 0.9921 - val_loss: 8.6142 - val_acc: 0.2391\n",
            "Epoch 169/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0179 - acc: 0.9923 - val_loss: 8.6569 - val_acc: 0.2286\n",
            "Epoch 170/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0249 - acc: 0.9909 - val_loss: 8.4697 - val_acc: 0.2049\n",
            "Epoch 171/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1538 - acc: 0.9561 - val_loss: 8.7693 - val_acc: 0.2124\n",
            "Epoch 172/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1269 - acc: 0.9610 - val_loss: 8.5433 - val_acc: 0.2356\n",
            "Epoch 173/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0602 - acc: 0.9802 - val_loss: 8.6020 - val_acc: 0.2563\n",
            "Epoch 174/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0291 - acc: 0.9905 - val_loss: 8.6228 - val_acc: 0.2475\n",
            "Epoch 175/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0220 - acc: 0.9925 - val_loss: 8.7069 - val_acc: 0.2505\n",
            "Epoch 176/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0196 - acc: 0.9926 - val_loss: 8.7339 - val_acc: 0.2462\n",
            "Epoch 177/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0202 - acc: 0.9924 - val_loss: 8.7267 - val_acc: 0.2444\n",
            "Epoch 178/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0211 - acc: 0.9924 - val_loss: 8.7657 - val_acc: 0.2444\n",
            "Epoch 179/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0206 - acc: 0.9921 - val_loss: 8.7479 - val_acc: 0.2593\n",
            "Epoch 180/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0197 - acc: 0.9924 - val_loss: 8.7478 - val_acc: 0.2435\n",
            "Epoch 181/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0184 - acc: 0.9924 - val_loss: 8.7397 - val_acc: 0.2532\n",
            "Epoch 182/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0198 - acc: 0.9929 - val_loss: 8.7428 - val_acc: 0.2492\n",
            "Epoch 183/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0182 - acc: 0.9926 - val_loss: 8.7480 - val_acc: 0.2514\n",
            "Epoch 184/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0197 - acc: 0.9919 - val_loss: 8.7672 - val_acc: 0.2576\n",
            "Epoch 185/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0195 - acc: 0.9920 - val_loss: 8.7282 - val_acc: 0.2510\n",
            "Epoch 186/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0195 - acc: 0.9928 - val_loss: 8.7396 - val_acc: 0.2527\n",
            "Epoch 187/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0191 - acc: 0.9919 - val_loss: 8.7440 - val_acc: 0.2501\n",
            "Epoch 188/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0189 - acc: 0.9922 - val_loss: 8.7051 - val_acc: 0.2488\n",
            "Epoch 189/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0172 - acc: 0.9922 - val_loss: 8.7006 - val_acc: 0.2317\n",
            "Epoch 190/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0174 - acc: 0.9932 - val_loss: 8.7112 - val_acc: 0.2488\n",
            "Epoch 191/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0171 - acc: 0.9922 - val_loss: 8.6621 - val_acc: 0.2383\n",
            "Epoch 192/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0186 - acc: 0.9924 - val_loss: 8.7363 - val_acc: 0.2536\n",
            "Epoch 193/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0175 - acc: 0.9924 - val_loss: 8.7219 - val_acc: 0.2492\n",
            "Epoch 194/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0178 - acc: 0.9924 - val_loss: 8.7237 - val_acc: 0.2387\n",
            "Epoch 195/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0170 - acc: 0.9928 - val_loss: 8.7219 - val_acc: 0.2541\n",
            "Epoch 196/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0181 - acc: 0.9924 - val_loss: 8.6444 - val_acc: 0.2519\n",
            "Epoch 197/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0166 - acc: 0.9925 - val_loss: 8.6720 - val_acc: 0.2422\n",
            "Epoch 198/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0164 - acc: 0.9925 - val_loss: 8.7993 - val_acc: 0.2470\n",
            "Epoch 199/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0171 - acc: 0.9925 - val_loss: 8.6281 - val_acc: 0.2475\n",
            "Epoch 200/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0198 - acc: 0.9920 - val_loss: 8.6771 - val_acc: 0.2527\n",
            "{0: 'A1', 1: 'A2', 2: 'A3', 3: 'A4', 4: 'A5', 5: 'B-1', 6: 'B-2', 7: 'B-3', 8: 'B-4', 9: 'B-5', 10: 'B1', 11: 'B2', 12: 'B3', 13: 'B4', 14: 'B5', 15: 'C#1', 16: 'C#2', 17: 'C#3', 18: 'C#4', 19: 'C#5', 20: 'C#6', 21: 'C2', 22: 'C3', 23: 'C4', 24: 'C5', 25: 'C6', 26: 'D1', 27: 'D2', 28: 'D3', 29: 'D4', 30: 'D5', 31: 'D6', 32: 'E-2', 33: 'E-3', 34: 'E-4', 35: 'E-5', 36: 'E-6', 37: 'E1', 38: 'E2', 39: 'E3', 40: 'E4', 41: 'E5', 42: 'E6', 43: 'F#1', 44: 'F#2', 45: 'F#3', 46: 'F#4', 47: 'F#5', 48: 'F#6', 49: 'F1', 50: 'F2', 51: 'F3', 52: 'F4', 53: 'F5', 54: 'G#1', 55: 'G#2', 56: 'G#3', 57: 'G#4', 58: 'G#5', 59: 'G1', 60: 'G2', 61: 'G3', 62: 'G4', 63: 'G5', 64: 'R'}\n",
            " Predicting.  Note:  399\n",
            "Wrote midi...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "xQCIp9r-sxwp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Notewise Root"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jdYcbpd6s3A1"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Set Training Parameters*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "88zW5Z6rs3A6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SET PARAMETERS (ACTION)\n",
        "RESTS = False\n",
        "ROOT_EXTRACTION = True\n",
        "DURATION_BATCH_SIZE = 256\n",
        "NOTE_BATCH_SIZE = 128\n",
        "# SPECIFY PARAMETERS TO TEST AS LIST\n",
        "DROPOUTS = 0\n",
        "MODEL_SIZES = 256\n",
        "EPOCHS = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "S48gjPm1s_CK"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 Preprocess Data (MIDI Data Into Notes Corpus and Duration Corpus)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "6823de53-6446-43df-ab7f-66d89149efdf",
        "id": "n6ju9CN4s_CO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1044
        }
      },
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "notes_corpus = []\n",
        "durations_corpus = []\n",
        "\n",
        "for file in glob.glob(DATA_FOLDER + \"/*.mid\"):\n",
        "    try:\n",
        "      print(\"Extracting MIDI File: \", file)\n",
        "      midi_stream = converter.parse(file)\n",
        "\n",
        "      notes = None\n",
        "\n",
        "      partition = instrument.partitionByInstrument(midi_stream)\n",
        "\n",
        "      if not RESTS:\n",
        "        # No rests\n",
        "        if partition: \n",
        "            notes = partition.parts[0].recurse()\n",
        "        else: \n",
        "            notes = midi_stream.flat.notes\n",
        "      else:\n",
        "        # With rests\n",
        "        if partition: \n",
        "            notes = partition.parts[0].recurse()\n",
        "        else: \n",
        "            notes = midi_stream.notesAndRests\n",
        "          \n",
        "      in_song_notes = []\n",
        "      in_song_durations = []\n",
        "      for element in notes:\n",
        "          in_song_durations.append(element.duration.quarterLength)\n",
        "          if isinstance(element, note.Note):\n",
        "              in_song_notes.append(str(element.pitch))\n",
        "          elif RESTS and isinstance(element, note.Rest):\n",
        "              in_song_notes.append(\"R\")\n",
        "          elif isinstance(element, chord.Chord):\n",
        "              if ROOT_EXTRACTION:\n",
        "                  in_song_notes.append(element.root().nameWithOctave)\n",
        "              else:\n",
        "                  in_song.append('.'.join(str(n) for n in element.normalOrder))\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    notes_corpus.append(in_song_notes)\n",
        "    durations_corpus.append(in_song_durations)\n",
        "            \n",
        "# Write\n",
        "with open(INTERMED_FOLDER + CORPUS + 'notes_corpus', 'wb+') as filepath:\n",
        "    pickle.dump(notes_corpus, filepath)\n",
        "    \n",
        "with open(INTERMED_FOLDER + CORPUS + 'durations_corpus', 'wb+') as filepath:\n",
        "    pickle.dump(durations_corpus, filepath)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Afrojack _ David Guetta - Another Life  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Lonely Together.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Axwell _ Ingrosso - More Than You Know  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Sing Me To Sleep  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Without You.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alesso ft. Matthew Koma - Years (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Tired  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Alone  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Faded (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Dear Boy.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Axwell _ Shapov - Belong (Axwell and Years Remode) (Midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 2 - C Maj.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 3 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Bruno Mars - That_s What I Like (Alan Walker Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Calvin Harris - Blame  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Calvin Harris - My Way  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 5 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 6 - F Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 1 - C Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 4 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 7 - F Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 14 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 8 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 16 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 9 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 15 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 10 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 13 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 12 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 11 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Galantis - Love On Me  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kill Paris ft. Royal - Operate (Illenium Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Headhunterz _ KSHMR - Dharma  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Ritual (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Galantis - No Money  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Summer  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kygo _ Selena Gomez - It Ain_t Me  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Alone (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kygo _ Ellie Goulding - First Time  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Madeon - Icarus  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix _ Bebe Rexha - In The Name Of Love  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix - There For You  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix ft. The Federal Empire - Hold On _ Believe (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Porter Robinson _ Madeon - Shelter (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix _ Dua Lipa - Scared To Be Lonely  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Porter Robinson - Sad Machine  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Nero - The Thrill (Porter Robinson Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Seven Lions x Echos - Cold Skin  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Sebastian Ingrosso _ Alesso feat. Ryan Tedder - Calling (Lose My Mind).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/R3hab _ KSHMR - Strong  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers _ Coldplay - Something Just Like This.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers ft. Halsey - Closer.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers ft. Phoebe Ryan - All We Know.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers Ft. Daya - Don_t Let Me Down.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers - Paris.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Tiesto _ KSHMR feat. Vassy - Secrets  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Zedd _ Hailee Steinfeld Grey - Starving  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers feat. XYLO - Setting Fires.mid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpCVM3rqtVBB"
      },
      "cell_type": "markdown",
      "source": [
        "# 5 Preprocess Corpus Into Train Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "atPrtzt6tVBI"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Notes Corpus"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b78b38e5-2ec0-4b68-b8ee-b45cb354c61d",
        "id": "Tw8fJEk8tVBM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# load notes corpus\n",
        "with open(INTERMED_FOLDER + CORPUS + 'notes_corpus', 'rb') as filepath:\n",
        "    notes_corpus = pickle.load(filepath)\n",
        "    \n",
        "# If doing learning for one song at a time only\n",
        "flattened_notes_corpus = []\n",
        "for song_notes in notes_corpus:\n",
        "    flattened_notes_corpus += song_notes\n",
        "\n",
        "vocab_size = len(set(flattened_notes_corpus))\n",
        "print(vocab_size)\n",
        "\n",
        "# Produce input output sequences\n",
        "window_size = 60\n",
        "note_sequence_input = []\n",
        "next_note_output = []\n",
        "\n",
        "notes = sorted(set(flattened_notes_corpus))\n",
        "note2int = dict((note, num) for num, note in enumerate(notes))\n",
        "\n",
        "for i in range(0, len(notes_corpus)):\n",
        "    for j in range(0, len(notes_corpus[i]) - window_size):\n",
        "        current_sequence = [note2int[note] for note in notes_corpus[i][j:window_size+j]]\n",
        "        next_note = note2int[notes_corpus[i][window_size+j]]\n",
        "        note_sequence_input.append(current_sequence)\n",
        "        next_note_output.append(next_note)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "4558b5a1-b3d7-4a77-c4c6-82fecbb0f360",
        "id": "8PrFbyLytVBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# check train and label shapes\n",
        "training_data = np.reshape(note_sequence_input, (len(note_sequence_input), window_size , 1))\n",
        "training_data = training_data / float(vocab_size)\n",
        "print('Train shape: ' + str(training_data.shape))\n",
        "training_label = np_utils.to_categorical(next_note_output)\n",
        "print('Label shape: ' + str(training_label.shape))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (6258, 60, 1)\n",
            "Label shape: (6258, 68)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gJuuh07TtVBi"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.2 Durations Corpus"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "94182f2f-c71b-4c9d-96a8-3ef4ed491217",
        "id": "xn3bs0FztVBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# load notes corpus\n",
        "with open(INTERMED_FOLDER + CORPUS + 'durations_corpus', 'rb') as filepath:\n",
        "    duration_corpus = pickle.load(filepath)\n",
        "\n",
        "# Learn on one song at a time\n",
        "flattened_duration_corpus = []\n",
        "for song_durations in duration_corpus:\n",
        "    flattened_duration_corpus += song_durations\n",
        "    \n",
        "import collections\n",
        "counter = collections.Counter(flattened_duration_corpus)\n",
        "print(counter)\n",
        "\n",
        "\n",
        "duration_vocab_size = len(set(flattened_duration_corpus))\n",
        "print(duration_vocab_size)\n",
        "\n",
        "# Produce input output sequences\n",
        "duration_window_size = 30\n",
        "duration_sequence_input = []\n",
        "next_duration_output = []\n",
        "\n",
        "durations = sorted(set(flattened_duration_corpus))\n",
        "duration2int = dict((duration, num) for num, duration in enumerate(durations))\n",
        "\n",
        "print(duration2int)\n",
        "# Write\n",
        "with open(INTERMED_FOLDER + \"/edm_duration_counter\", 'wb+') as filepath:\n",
        "    pickle.dump(counter, filepath)\n",
        "\n",
        "for i in range(0, len(duration_corpus)):\n",
        "    for j in range(0, len(duration_corpus[i]) - duration_window_size):\n",
        "        current_duration_sequence = [duration2int[note] for note in duration_corpus[i][j:duration_window_size+j]]\n",
        "        next_duration = duration2int[duration_corpus[i][duration_window_size+j]]\n",
        "        duration_sequence_input.append(current_duration_sequence)\n",
        "        next_duration_output.append(next_duration)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0.25: 5380, 0.5: 4464, Fraction(1, 3): 1009, 0.0: 998, 1.0: 741, Fraction(1, 6): 729, 0.75: 485, 4.0: 166, 2.0: 130, Fraction(2, 3): 98, Fraction(1, 12): 86, Fraction(5, 12): 72, 1.5: 59, 3.0: 41, Fraction(7, 6): 32, 1.25: 31, 1.75: 30, 2.25: 25, 4.5: 22, 3.5: 13, 6.5: 13, 8.0: 11, 4.25: 9, 2.5: 9, Fraction(25, 6): 7, 5.0: 6, 7.0: 5, 8.5: 4, 32.0: 4, 6.0: 4, Fraction(5, 3): 3, 48.0: 2, 68.0: 2, 176.0: 2, 80.0: 2, 16.0: 2, Fraction(23, 3): 1, 47.5: 1, 3.25: 1, 5.75: 1, 200.5: 1, 36.25: 1, 64.25: 1, 84.25: 1, 72.5: 1, 72.25: 1, 14.5: 1, 9.0: 1, 27.5: 1, 61.0: 1, 161.0: 1, 5.25: 1, 4.75: 1, 94.5: 1, 128.25: 1, 18.0: 1, 39.5: 1, Fraction(553, 6): 1, Fraction(4, 3): 1, 32.5: 1, 36.0: 1})\n",
            "61\n",
            "{0.0: 0, Fraction(1, 12): 1, Fraction(1, 6): 2, 0.25: 3, Fraction(1, 3): 4, Fraction(5, 12): 5, 0.5: 6, Fraction(2, 3): 7, 0.75: 8, 1.0: 9, Fraction(7, 6): 10, 1.25: 11, Fraction(4, 3): 12, 1.5: 13, Fraction(5, 3): 14, 1.75: 15, 2.0: 16, 2.25: 17, 2.5: 18, 3.0: 19, 3.25: 20, 3.5: 21, 4.0: 22, Fraction(25, 6): 23, 4.25: 24, 4.5: 25, 4.75: 26, 5.0: 27, 5.25: 28, 5.75: 29, 6.0: 30, 6.5: 31, 7.0: 32, Fraction(23, 3): 33, 8.0: 34, 8.5: 35, 9.0: 36, 14.5: 37, 16.0: 38, 18.0: 39, 27.5: 40, 32.0: 41, 32.5: 42, 36.0: 43, 36.25: 44, 39.5: 45, 47.5: 46, 48.0: 47, 61.0: 48, 64.25: 49, 68.0: 50, 72.25: 51, 72.5: 52, 80.0: 53, 84.25: 54, Fraction(553, 6): 55, 94.5: 56, 128.25: 57, 161.0: 58, 176.0: 59, 200.5: 60}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b3be8c9c-895b-4514-ed77-0860165318a7",
        "id": "DYIJqWNZtVBr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# check train and label shapes\n",
        "duration_training_data = np.reshape(duration_sequence_input, (len(duration_sequence_input), duration_window_size , 1))\n",
        "duration_training_data = duration_training_data / float(duration_vocab_size)\n",
        "print('Train shape: ' + str(duration_training_data.shape))\n",
        "duration_training_label = np_utils.to_categorical(next_duration_output)\n",
        "print('Label shape: ' + str(duration_training_label.shape))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (13030, 30, 1)\n",
            "Label shape: (13030, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CJ7qsp3ItjYx"
      },
      "cell_type": "markdown",
      "source": [
        "# 6 Train"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HXICQ93ntjY5"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1 Helpers to Create Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lS3ji0DgtjY8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(network_input, n_vocab, model_size, dropout):\n",
        "  model = Sequential()\n",
        "  reg = L1L2(0, 0)\n",
        "  model.add(LSTM(\n",
        "      model_size,\n",
        "      input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "      return_sequences=True,\n",
        "      dropout=dropout, recurrent_dropout=0.3\n",
        "  ))\n",
        "  model.add(CuDNNLSTM(model_size, return_sequences=True, kernel_regularizer=reg))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(CuDNNLSTM(model_size, kernel_regularizer=reg))\n",
        "  model.add(Dense(128))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_callback_list(model_size, dropout, model_type):\n",
        "  filepath = INTERMED_FOLDER + '/%skarpathy-model-weights-%s-%s-%s.hdf5' % (CORPUS, model_type, model_size, dropout)\n",
        "  model_checkpoint = ModelCheckpoint(\n",
        "    filepath,\n",
        "    monitor='loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    mode='min'\n",
        "  )\n",
        "  return [model_checkpoint], filepath\n",
        "\n",
        "# acc history\n",
        "def setup_plot(dropout, size):\n",
        "  plt.title('Model Accuracy vs. Epoc with Dropout=%s Size=%s' % (dropout, size))\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  \n",
        "def plot_history(history, model_type, dropout, size):\n",
        "  plt.plot(history.history['acc'], label=\"%s train accuracy\" % model_type)\n",
        "  plt.plot(history.history['val_acc'], label=\"%s val accuracy\" % model_type)\n",
        "\n",
        "def save_plot(file_path):\n",
        "  plt.legend()\n",
        "  plt.savefig(file_path)\n",
        "  plt.clf()\n",
        "  \n",
        "def predict_duration(model, WEIGHT_PATH):\n",
        "  # Prediction\n",
        "  model.load_weights(WEIGHT_PATH)\n",
        "  starting_sequence = np.random.randint(219, size=duration_window_size)\n",
        "  pattern_sequence = starting_sequence.tolist()\n",
        "  prediction_output = []\n",
        "\n",
        "  duration2note = dict((num, note) for num, note in enumerate(durations))\n",
        "  print (duration2note)\n",
        "  \n",
        "  for i in range(400):\n",
        "      prediction_input = np.reshape(pattern_sequence, (1, len(pattern_sequence), 1))\n",
        "      prediction_input = prediction_input / float(duration_vocab_size)\n",
        "\n",
        "      if i%3 == 0:   \n",
        "          print('\\r', 'Predicting.  Duration: ', i, end='')  \n",
        "      if i%3 == 1:   \n",
        "          print('\\r', 'Predicting.. Duration: ', i, end='')\n",
        "      if i%3 == 2:   \n",
        "          print('\\r', 'Predicting...Duration: ', i, end='')\n",
        "      prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "      prediction_values = np.arange(len(prediction[0]))\n",
        "      prediction_prob = np.asarray(list(prediction[0])) / float(sum(prediction[0]))\n",
        "\n",
        "      # Most probable note prediction\n",
        "#       index = np.argmax(prediction)\n",
        "#       note_instance = duration2note[index]\n",
        "#       prediction_output.append(note_instance)\n",
        "      index = np.random.choice(prediction_values, 1, p=prediction_prob)\n",
        "      note_instance = duration2note[int(index[0])]\n",
        "      prediction_output.append(note_instance)\n",
        "\n",
        "      pattern_sequence.append(index)\n",
        "      pattern_sequence = pattern_sequence[1:len(pattern_sequence)]    \n",
        "\n",
        "  prediction_output = prediction_output[300:len(prediction_output)]\n",
        "\n",
        "  # Write\n",
        "  with open(INTERMED_FOLDER + (\"%sduration_prediction_output\" % CORPUS), 'wb+') as filepath:\n",
        "      pickle.dump(prediction_output, filepath)\n",
        "  \n",
        "  return prediction_output\n",
        "\n",
        "def predict_note(model, WEIGHT_PATH):\n",
        "  model.load_weights(WEIGHT_PATH)\n",
        "  # Prediction\n",
        "  starting_sequence = np.random.randint(219, size=window_size)\n",
        "  pattern_sequence = starting_sequence.tolist()\n",
        "  prediction_output = []\n",
        "\n",
        "  int2note = dict((num, note) for num, note in enumerate(notes))\n",
        "  print (int2note)\n",
        "\n",
        "  for i in range(400):\n",
        "      prediction_input = np.reshape(pattern_sequence, (1, len(pattern_sequence), 1))\n",
        "      prediction_input = prediction_input / float(vocab_size)\n",
        "\n",
        "      if i%3 == 0:   \n",
        "          print('\\r', 'Predicting.  Note: ', i, end='')  \n",
        "      if i%3 == 1:   \n",
        "          print('\\r', 'Predicting.. Note: ', i, end='')\n",
        "      if i%3 == 2:   \n",
        "          print('\\r', 'Predicting...Note: ', i, end='')\n",
        "      prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "#       prediction_values = np.arange(len(prediction[0]))\n",
        "#       prediction_prob = np.asarray(list(prediction[0])) / float(sum(prediction[0]))\n",
        "#       print(prediction_prob)\n",
        "\n",
        "      # Most probable note prediction\n",
        "      index = np.argmax(prediction)\n",
        "      note_instance = int2note[index]\n",
        "      prediction_output.append(note_instance)\n",
        "\n",
        "      # Predict based on prob dist\n",
        "#       index = np.random.choice(prediction_values, 1, p=prediction_prob)\n",
        "  #     print(index[0])\n",
        "  #     print(type(index[0]))\n",
        "#       note_instance = int2note[int(index[0])]\n",
        "#       prediction_output.append(note_instance)\n",
        "\n",
        "      pattern_sequence.append(index)\n",
        "      pattern_sequence = pattern_sequence[1:len(pattern_sequence)]    \n",
        "\n",
        "  prediction_output = prediction_output[300:len(prediction_output)]\n",
        "  # Write\n",
        "  with open(INTERMED_FOLDER + (\"%snotes_prediction_output\" % CORPUS), 'wb+') as filepath:\n",
        "      pickle.dump(prediction_output, filepath)\n",
        "  return prediction_output\n",
        "\n",
        "def output_midi(prediction_output, duration_prediction_output, dropout, model_size):\n",
        "  offset = 0\n",
        "  output_notes = []\n",
        "  for pattern, duration in zip(prediction_output, duration_prediction_output):\n",
        "\n",
        "      if ('.' in pattern) or pattern.isdigit():\n",
        "          chord_array = pattern.split('.')\n",
        "          chord_notes = []\n",
        "          for note_instance in chord_array:\n",
        "              note_object = note.Note(int(note_instance))\n",
        "              note_object.duration.quarterLength = duration\n",
        "              note_object.storedInstrument = instrument.Piano()\n",
        "              chord_notes.append(note_object)\n",
        "          chord_object = chord.Chord(chord_notes)\n",
        "          chord_object.offset = offset\n",
        "          output_notes.append(chord_object)\n",
        "      elif 'R' == pattern:\n",
        "          note_object = note.Rest()\n",
        "          note_object.duration.quarterLength = duration\n",
        "          note_object.offset = offset\n",
        "          output_notes.append(note_object)\n",
        "      else:\n",
        "          note_object = note.Note(pattern)\n",
        "          note_object.duration.quarterLength = duration\n",
        "          note_object.offset = offset\n",
        "          note_object.storedInstrument = instrument.Piano()\n",
        "          output_notes.append(note_object)\n",
        "\n",
        "      offset += 0.5\n",
        "\n",
        "  midi_stream = stream.Stream(output_notes)\n",
        "  midi_stream.write('midi', fp=MIDI_OUTPUT_FOLDER + CORPUS + '%s_%s.mid' % (dropout, model_size))\n",
        "  print('\\nWrote midi...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sqVfHjjZtjZF"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2 Preprocessing Optimization"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4q9dE9VrtjZH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14121
        },
        "outputId": "2c91492d-243a-47d0-8139-99ce9f03d1a6"
      },
      "cell_type": "code",
      "source": [
        "notes_histories = {}\n",
        "duration_histories = {}\n",
        "\n",
        "dropout, model_size = DROPOUTS, MODEL_SIZES\n",
        "setup_plot(dropout, model_size)\n",
        "print('Running duration training on notewise with rests:%s and root extraction:%s' % (RESTS, ROOT_EXTRACTION))\n",
        "duration_callbacks, duration_weight_path = create_callback_list('duration', dropout, model_size)\n",
        "duration_model = create_model(duration_training_data, duration_vocab_size, model_size, dropout)\n",
        "duration_histories[(dropout, model_size)] = duration_model.fit(duration_training_data, duration_training_label, epochs=EPOCHS, batch_size=DURATION_BATCH_SIZE, callbacks=duration_callbacks, validation_split=0.2)\n",
        "plot_history(duration_histories[(dropout, model_size)], 'Durations', dropout, model_size)\n",
        "# output intermed duration\n",
        "duration_prediction = predict_duration(duration_model, duration_weight_path)\n",
        "\n",
        "print('\\n\\nRunning notes training on d:%s s:%s' % (dropout, model_size))\n",
        "notes_callbacks, note_weight_path = create_callback_list('notes', dropout, model_size)\n",
        "notes_model = create_model(training_data, vocab_size, model_size, dropout)\n",
        "notes_histories[(dropout, model_size)] = notes_model.fit(training_data, training_label, epochs=EPOCHS, batch_size=NOTE_BATCH_SIZE, callbacks=notes_callbacks, validation_split=0.2)\n",
        "plot_history(notes_histories[(dropout, model_size)], 'Notes', dropout, model_size)\n",
        "save_plot(GRAPHS_FOLDER + EDM_CORPUS + 'dropout=%s_size=%s' % (dropout, model_size))\n",
        "# output intermed notes\n",
        "note_prediction = predict_note(notes_model, note_weight_path)\n",
        "\n",
        "# output final midi\n",
        "output_midi(note_prediction, duration_prediction, dropout, model_size)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running duration training on notewise with rests:False and root extraction:True\n",
            "Train on 10424 samples, validate on 2606 samples\n",
            "Epoch 1/200\n",
            "10424/10424 [==============================] - 6s 620us/step - loss: 2.1591 - acc: 0.3850 - val_loss: 1.9878 - val_acc: 0.2552\n",
            "Epoch 2/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 1.7219 - acc: 0.4556 - val_loss: 1.8861 - val_acc: 0.3937\n",
            "Epoch 3/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.5677 - acc: 0.4976 - val_loss: 1.8923 - val_acc: 0.4359\n",
            "Epoch 4/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.5310 - acc: 0.5000 - val_loss: 1.8624 - val_acc: 0.4056\n",
            "Epoch 5/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.5082 - acc: 0.4982 - val_loss: 1.9897 - val_acc: 0.4248\n",
            "Epoch 6/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 1.5021 - acc: 0.5015 - val_loss: 1.8175 - val_acc: 0.4117\n",
            "Epoch 7/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 1.4985 - acc: 0.4948 - val_loss: 1.8850 - val_acc: 0.4106\n",
            "Epoch 8/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.4762 - acc: 0.5037 - val_loss: 1.8931 - val_acc: 0.4367\n",
            "Epoch 9/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.4790 - acc: 0.4987 - val_loss: 1.9375 - val_acc: 0.4037\n",
            "Epoch 10/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 1.4944 - acc: 0.5008 - val_loss: 1.8409 - val_acc: 0.4340\n",
            "Epoch 11/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 1.4614 - acc: 0.4987 - val_loss: 1.9349 - val_acc: 0.4363\n",
            "Epoch 12/200\n",
            "10424/10424 [==============================] - 5s 454us/step - loss: 1.4628 - acc: 0.4978 - val_loss: 1.8761 - val_acc: 0.4275\n",
            "Epoch 13/200\n",
            "10424/10424 [==============================] - 5s 449us/step - loss: 1.4527 - acc: 0.5047 - val_loss: 1.9369 - val_acc: 0.4309\n",
            "Epoch 14/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 1.4832 - acc: 0.5016 - val_loss: 1.8807 - val_acc: 0.4263\n",
            "Epoch 15/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 1.4614 - acc: 0.5024 - val_loss: 1.9198 - val_acc: 0.4229\n",
            "Epoch 16/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 1.4331 - acc: 0.5076 - val_loss: 1.9882 - val_acc: 0.3507\n",
            "Epoch 17/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 1.4972 - acc: 0.4996 - val_loss: 1.9741 - val_acc: 0.2870\n",
            "Epoch 18/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 1.4351 - acc: 0.5127 - val_loss: 2.1049 - val_acc: 0.3085\n",
            "Epoch 19/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 1.4180 - acc: 0.5204 - val_loss: 2.1335 - val_acc: 0.3830\n",
            "Epoch 20/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.3697 - acc: 0.5328 - val_loss: 2.0193 - val_acc: 0.4233\n",
            "Epoch 21/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 1.3498 - acc: 0.5388 - val_loss: 2.1864 - val_acc: 0.3277\n",
            "Epoch 22/200\n",
            "10424/10424 [==============================] - 5s 442us/step - loss: 1.3226 - acc: 0.5425 - val_loss: 2.3864 - val_acc: 0.2782\n",
            "Epoch 23/200\n",
            "10424/10424 [==============================] - 5s 453us/step - loss: 1.3076 - acc: 0.5453 - val_loss: 2.2719 - val_acc: 0.3450\n",
            "Epoch 24/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 1.2833 - acc: 0.5463 - val_loss: 2.2863 - val_acc: 0.3315\n",
            "Epoch 25/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.2591 - acc: 0.5531 - val_loss: 2.2310 - val_acc: 0.3960\n",
            "Epoch 26/200\n",
            "10424/10424 [==============================] - 5s 442us/step - loss: 1.2390 - acc: 0.5585 - val_loss: 2.5461 - val_acc: 0.3273\n",
            "Epoch 27/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.1945 - acc: 0.5655 - val_loss: 2.4423 - val_acc: 0.3776\n",
            "Epoch 28/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 1.2134 - acc: 0.5641 - val_loss: 2.3713 - val_acc: 0.3945\n",
            "Epoch 29/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 1.1562 - acc: 0.5811 - val_loss: 2.4166 - val_acc: 0.3906\n",
            "Epoch 30/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 1.1109 - acc: 0.5893 - val_loss: 2.4940 - val_acc: 0.3684\n",
            "Epoch 31/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 1.0892 - acc: 0.5915 - val_loss: 2.6183 - val_acc: 0.3526\n",
            "Epoch 32/200\n",
            "10424/10424 [==============================] - 5s 442us/step - loss: 1.0455 - acc: 0.6121 - val_loss: 2.5473 - val_acc: 0.3668\n",
            "Epoch 33/200\n",
            "10424/10424 [==============================] - 5s 452us/step - loss: 1.0001 - acc: 0.6299 - val_loss: 2.6793 - val_acc: 0.3718\n",
            "Epoch 34/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 1.0158 - acc: 0.6226 - val_loss: 2.7476 - val_acc: 0.3653\n",
            "Epoch 35/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.9661 - acc: 0.6450 - val_loss: 2.7045 - val_acc: 0.3772\n",
            "Epoch 36/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.9105 - acc: 0.6601 - val_loss: 2.7945 - val_acc: 0.3243\n",
            "Epoch 37/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.8520 - acc: 0.6845 - val_loss: 2.9159 - val_acc: 0.3807\n",
            "Epoch 38/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.8004 - acc: 0.7023 - val_loss: 2.9805 - val_acc: 0.3557\n",
            "Epoch 39/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.8445 - acc: 0.6839 - val_loss: 3.0157 - val_acc: 0.3473\n",
            "Epoch 40/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.7523 - acc: 0.7213 - val_loss: 3.0590 - val_acc: 0.3500\n",
            "Epoch 41/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.7047 - acc: 0.7370 - val_loss: 3.2307 - val_acc: 0.3500\n",
            "Epoch 42/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.6947 - acc: 0.7419 - val_loss: 3.4745 - val_acc: 0.3170\n",
            "Epoch 43/200\n",
            "10424/10424 [==============================] - 5s 455us/step - loss: 0.6157 - acc: 0.7690 - val_loss: 3.4634 - val_acc: 0.3469\n",
            "Epoch 44/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.6098 - acc: 0.7810 - val_loss: 3.5443 - val_acc: 0.3676\n",
            "Epoch 45/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.9034 - acc: 0.6825 - val_loss: 3.3703 - val_acc: 0.3208\n",
            "Epoch 46/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.6128 - acc: 0.7810 - val_loss: 3.4710 - val_acc: 0.3484\n",
            "Epoch 47/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.5278 - acc: 0.8085 - val_loss: 3.8107 - val_acc: 0.3331\n",
            "Epoch 48/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.4995 - acc: 0.8178 - val_loss: 3.9752 - val_acc: 0.3254\n",
            "Epoch 49/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.4587 - acc: 0.8346 - val_loss: 4.3198 - val_acc: 0.3216\n",
            "Epoch 50/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.4545 - acc: 0.8345 - val_loss: 4.2799 - val_acc: 0.3246\n",
            "Epoch 51/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.4332 - acc: 0.8440 - val_loss: 4.3336 - val_acc: 0.3081\n",
            "Epoch 52/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.4046 - acc: 0.8554 - val_loss: 4.5858 - val_acc: 0.3097\n",
            "Epoch 53/200\n",
            "10424/10424 [==============================] - 5s 452us/step - loss: 0.3586 - acc: 0.8719 - val_loss: 4.7057 - val_acc: 0.3423\n",
            "Epoch 54/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.3505 - acc: 0.8769 - val_loss: 4.8361 - val_acc: 0.3039\n",
            "Epoch 55/200\n",
            "10424/10424 [==============================] - 5s 442us/step - loss: 0.3524 - acc: 0.8730 - val_loss: 4.7958 - val_acc: 0.3124\n",
            "Epoch 56/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.3178 - acc: 0.8891 - val_loss: 5.0516 - val_acc: 0.2951\n",
            "Epoch 57/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.3051 - acc: 0.8922 - val_loss: 4.9935 - val_acc: 0.2989\n",
            "Epoch 58/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.2828 - acc: 0.9002 - val_loss: 4.8793 - val_acc: 0.3266\n",
            "Epoch 59/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.2760 - acc: 0.9009 - val_loss: 5.1181 - val_acc: 0.3001\n",
            "Epoch 60/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.2368 - acc: 0.9181 - val_loss: 5.0861 - val_acc: 0.3173\n",
            "Epoch 61/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.2331 - acc: 0.9189 - val_loss: 5.2762 - val_acc: 0.3035\n",
            "Epoch 62/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.2299 - acc: 0.9205 - val_loss: 5.4243 - val_acc: 0.3054\n",
            "Epoch 63/200\n",
            "10424/10424 [==============================] - 5s 458us/step - loss: 0.2157 - acc: 0.9257 - val_loss: 5.4472 - val_acc: 0.2893\n",
            "Epoch 64/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1943 - acc: 0.9353 - val_loss: 5.5129 - val_acc: 0.3062\n",
            "Epoch 65/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.1916 - acc: 0.9351 - val_loss: 5.6835 - val_acc: 0.2982\n",
            "Epoch 66/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.2015 - acc: 0.9257 - val_loss: 5.5534 - val_acc: 0.3289\n",
            "Epoch 67/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1934 - acc: 0.9320 - val_loss: 5.6009 - val_acc: 0.3243\n",
            "Epoch 68/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1703 - acc: 0.9407 - val_loss: 5.9471 - val_acc: 0.2878\n",
            "Epoch 69/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1953 - acc: 0.9327 - val_loss: 5.6798 - val_acc: 0.3070\n",
            "Epoch 70/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1676 - acc: 0.9434 - val_loss: 5.9488 - val_acc: 0.2832\n",
            "Epoch 71/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.1428 - acc: 0.9529 - val_loss: 5.8642 - val_acc: 0.3070\n",
            "Epoch 72/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.1892 - acc: 0.9378 - val_loss: 5.8648 - val_acc: 0.3047\n",
            "Epoch 73/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1597 - acc: 0.9455 - val_loss: 5.8944 - val_acc: 0.2989\n",
            "Epoch 74/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.1611 - acc: 0.9428 - val_loss: 6.0112 - val_acc: 0.3108\n",
            "Epoch 75/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.1577 - acc: 0.9451 - val_loss: 6.0900 - val_acc: 0.2982\n",
            "Epoch 76/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1708 - acc: 0.9408 - val_loss: 6.2451 - val_acc: 0.2889\n",
            "Epoch 77/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1287 - acc: 0.9557 - val_loss: 6.0561 - val_acc: 0.2736\n",
            "Epoch 78/200\n",
            "10424/10424 [==============================] - 5s 453us/step - loss: 0.1070 - acc: 0.9640 - val_loss: 6.1775 - val_acc: 0.2993\n",
            "Epoch 79/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.1055 - acc: 0.9656 - val_loss: 6.2641 - val_acc: 0.2920\n",
            "Epoch 80/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1026 - acc: 0.9650 - val_loss: 6.3132 - val_acc: 0.3024\n",
            "Epoch 81/200\n",
            "10424/10424 [==============================] - 5s 449us/step - loss: 0.1054 - acc: 0.9625 - val_loss: 6.1595 - val_acc: 0.2982\n",
            "Epoch 82/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.1107 - acc: 0.9609 - val_loss: 6.1217 - val_acc: 0.2909\n",
            "Epoch 83/200\n",
            "10424/10424 [==============================] - 5s 451us/step - loss: 0.1354 - acc: 0.9536 - val_loss: 6.2686 - val_acc: 0.2782\n",
            "Epoch 84/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1214 - acc: 0.9568 - val_loss: 6.3242 - val_acc: 0.2740\n",
            "Epoch 85/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1004 - acc: 0.9658 - val_loss: 6.4042 - val_acc: 0.2978\n",
            "Epoch 86/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0982 - acc: 0.9664 - val_loss: 6.5219 - val_acc: 0.2897\n",
            "Epoch 87/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0963 - acc: 0.9673 - val_loss: 6.5333 - val_acc: 0.2939\n",
            "Epoch 88/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.1097 - acc: 0.9608 - val_loss: 6.4763 - val_acc: 0.2847\n",
            "Epoch 89/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1478 - acc: 0.9505 - val_loss: 6.5916 - val_acc: 0.2955\n",
            "Epoch 90/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1338 - acc: 0.9527 - val_loss: 6.6109 - val_acc: 0.2866\n",
            "Epoch 91/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1046 - acc: 0.9654 - val_loss: 6.6087 - val_acc: 0.2882\n",
            "Epoch 92/200\n",
            "10424/10424 [==============================] - 5s 449us/step - loss: 0.0939 - acc: 0.9677 - val_loss: 6.5344 - val_acc: 0.2820\n",
            "Epoch 93/200\n",
            "10424/10424 [==============================] - 5s 453us/step - loss: 0.0783 - acc: 0.9731 - val_loss: 6.7506 - val_acc: 0.2863\n",
            "Epoch 94/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0774 - acc: 0.9747 - val_loss: 6.6464 - val_acc: 0.2878\n",
            "Epoch 95/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0819 - acc: 0.9716 - val_loss: 6.7923 - val_acc: 0.2836\n",
            "Epoch 96/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0831 - acc: 0.9735 - val_loss: 6.7941 - val_acc: 0.2801\n",
            "Epoch 97/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0735 - acc: 0.9754 - val_loss: 6.7475 - val_acc: 0.2836\n",
            "Epoch 98/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0709 - acc: 0.9745 - val_loss: 6.6630 - val_acc: 0.2809\n",
            "Epoch 99/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0833 - acc: 0.9715 - val_loss: 6.7881 - val_acc: 0.2928\n",
            "Epoch 100/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1137 - acc: 0.9616 - val_loss: 6.9298 - val_acc: 0.2686\n",
            "Epoch 101/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.1759 - acc: 0.9464 - val_loss: 6.6949 - val_acc: 0.2828\n",
            "Epoch 102/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.4518 - acc: 0.8615 - val_loss: 5.5976 - val_acc: 0.2970\n",
            "Epoch 103/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.2430 - acc: 0.9154 - val_loss: 5.7083 - val_acc: 0.2982\n",
            "Epoch 104/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.1041 - acc: 0.9665 - val_loss: 5.8406 - val_acc: 0.2939\n",
            "Epoch 105/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0797 - acc: 0.9744 - val_loss: 5.8870 - val_acc: 0.2893\n",
            "Epoch 106/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0715 - acc: 0.9766 - val_loss: 5.9049 - val_acc: 0.3043\n",
            "Epoch 107/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0690 - acc: 0.9781 - val_loss: 6.1701 - val_acc: 0.2886\n",
            "Epoch 108/200\n",
            "10424/10424 [==============================] - 5s 456us/step - loss: 0.0640 - acc: 0.9787 - val_loss: 6.2692 - val_acc: 0.2874\n",
            "Epoch 109/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0619 - acc: 0.9789 - val_loss: 6.2893 - val_acc: 0.2824\n",
            "Epoch 110/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0611 - acc: 0.9796 - val_loss: 6.3789 - val_acc: 0.2874\n",
            "Epoch 111/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0608 - acc: 0.9792 - val_loss: 6.3934 - val_acc: 0.2909\n",
            "Epoch 112/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0583 - acc: 0.9800 - val_loss: 6.4292 - val_acc: 0.2966\n",
            "Epoch 113/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.0580 - acc: 0.9808 - val_loss: 6.4997 - val_acc: 0.2840\n",
            "Epoch 114/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0554 - acc: 0.9795 - val_loss: 6.4404 - val_acc: 0.2863\n",
            "Epoch 115/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0596 - acc: 0.9796 - val_loss: 6.5855 - val_acc: 0.2897\n",
            "Epoch 116/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0612 - acc: 0.9800 - val_loss: 6.5883 - val_acc: 0.3005\n",
            "Epoch 117/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0600 - acc: 0.9793 - val_loss: 6.5284 - val_acc: 0.2832\n",
            "Epoch 118/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0664 - acc: 0.9781 - val_loss: 6.5927 - val_acc: 0.2924\n",
            "Epoch 119/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0627 - acc: 0.9788 - val_loss: 6.7851 - val_acc: 0.2817\n",
            "Epoch 120/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0592 - acc: 0.9796 - val_loss: 6.7835 - val_acc: 0.2851\n",
            "Epoch 121/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0627 - acc: 0.9796 - val_loss: 6.8355 - val_acc: 0.2882\n",
            "Epoch 122/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0606 - acc: 0.9797 - val_loss: 6.9046 - val_acc: 0.2878\n",
            "Epoch 123/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0621 - acc: 0.9795 - val_loss: 6.7939 - val_acc: 0.2778\n",
            "Epoch 124/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0746 - acc: 0.9749 - val_loss: 6.7144 - val_acc: 0.2843\n",
            "Epoch 125/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.2532 - acc: 0.9233 - val_loss: 5.9185 - val_acc: 0.3350\n",
            "Epoch 126/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.2661 - acc: 0.9119 - val_loss: 6.1159 - val_acc: 0.2943\n",
            "Epoch 127/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1453 - acc: 0.9531 - val_loss: 6.1669 - val_acc: 0.3051\n",
            "Epoch 128/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0912 - acc: 0.9708 - val_loss: 6.3325 - val_acc: 0.2916\n",
            "Epoch 129/200\n",
            "10424/10424 [==============================] - 5s 450us/step - loss: 0.0674 - acc: 0.9796 - val_loss: 6.4346 - val_acc: 0.2951\n",
            "Epoch 130/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0575 - acc: 0.9815 - val_loss: 6.5841 - val_acc: 0.2855\n",
            "Epoch 131/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0541 - acc: 0.9826 - val_loss: 6.6122 - val_acc: 0.2824\n",
            "Epoch 132/200\n",
            "10424/10424 [==============================] - 5s 455us/step - loss: 0.0534 - acc: 0.9819 - val_loss: 6.5655 - val_acc: 0.3012\n",
            "Epoch 133/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0574 - acc: 0.9822 - val_loss: 6.6211 - val_acc: 0.2886\n",
            "Epoch 134/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0532 - acc: 0.9829 - val_loss: 6.6496 - val_acc: 0.2882\n",
            "Epoch 135/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0490 - acc: 0.9835 - val_loss: 6.5801 - val_acc: 0.2836\n",
            "Epoch 136/200\n",
            "10424/10424 [==============================] - 5s 442us/step - loss: 0.0467 - acc: 0.9839 - val_loss: 6.8036 - val_acc: 0.2928\n",
            "Epoch 137/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0585 - acc: 0.9793 - val_loss: 6.6195 - val_acc: 0.2843\n",
            "Epoch 138/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0581 - acc: 0.9800 - val_loss: 6.5494 - val_acc: 0.2985\n",
            "Epoch 139/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0575 - acc: 0.9796 - val_loss: 6.7743 - val_acc: 0.3235\n",
            "Epoch 140/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0670 - acc: 0.9772 - val_loss: 6.7274 - val_acc: 0.2959\n",
            "Epoch 141/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0800 - acc: 0.9730 - val_loss: 6.6030 - val_acc: 0.2985\n",
            "Epoch 142/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0649 - acc: 0.9802 - val_loss: 6.9345 - val_acc: 0.2978\n",
            "Epoch 143/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0593 - acc: 0.9796 - val_loss: 6.7248 - val_acc: 0.3139\n",
            "Epoch 144/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0632 - acc: 0.9798 - val_loss: 6.9921 - val_acc: 0.2920\n",
            "Epoch 145/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0611 - acc: 0.9797 - val_loss: 6.7171 - val_acc: 0.2974\n",
            "Epoch 146/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0627 - acc: 0.9790 - val_loss: 6.5699 - val_acc: 0.3154\n",
            "Epoch 147/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0661 - acc: 0.9767 - val_loss: 6.6338 - val_acc: 0.2962\n",
            "Epoch 148/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0581 - acc: 0.9800 - val_loss: 6.9519 - val_acc: 0.2836\n",
            "Epoch 149/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0541 - acc: 0.9817 - val_loss: 7.1020 - val_acc: 0.3005\n",
            "Epoch 150/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0520 - acc: 0.9825 - val_loss: 6.7741 - val_acc: 0.3020\n",
            "Epoch 151/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0534 - acc: 0.9820 - val_loss: 6.9476 - val_acc: 0.2928\n",
            "Epoch 152/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0551 - acc: 0.9810 - val_loss: 7.0822 - val_acc: 0.2932\n",
            "Epoch 153/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0533 - acc: 0.9822 - val_loss: 6.8792 - val_acc: 0.3074\n",
            "Epoch 154/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0500 - acc: 0.9825 - val_loss: 6.9152 - val_acc: 0.2859\n",
            "Epoch 155/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0500 - acc: 0.9838 - val_loss: 6.9503 - val_acc: 0.2820\n",
            "Epoch 156/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0509 - acc: 0.9815 - val_loss: 6.9782 - val_acc: 0.2863\n",
            "Epoch 157/200\n",
            "10424/10424 [==============================] - 5s 442us/step - loss: 0.0515 - acc: 0.9815 - val_loss: 7.0263 - val_acc: 0.2851\n",
            "Epoch 158/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0484 - acc: 0.9838 - val_loss: 7.0900 - val_acc: 0.2947\n",
            "Epoch 159/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0485 - acc: 0.9828 - val_loss: 6.9907 - val_acc: 0.2866\n",
            "Epoch 160/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0522 - acc: 0.9825 - val_loss: 6.7142 - val_acc: 0.3001\n",
            "Epoch 161/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0625 - acc: 0.9793 - val_loss: 6.9095 - val_acc: 0.2955\n",
            "Epoch 162/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.2306 - acc: 0.9282 - val_loss: 6.0224 - val_acc: 0.2836\n",
            "Epoch 163/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.3107 - acc: 0.8977 - val_loss: 5.7470 - val_acc: 0.3108\n",
            "Epoch 164/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1219 - acc: 0.9571 - val_loss: 5.7907 - val_acc: 0.2936\n",
            "Epoch 165/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0733 - acc: 0.9757 - val_loss: 6.1603 - val_acc: 0.2774\n",
            "Epoch 166/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0622 - acc: 0.9808 - val_loss: 6.2006 - val_acc: 0.3035\n",
            "Epoch 167/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0498 - acc: 0.9832 - val_loss: 6.3847 - val_acc: 0.2859\n",
            "Epoch 168/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0452 - acc: 0.9855 - val_loss: 6.4602 - val_acc: 0.2843\n",
            "Epoch 169/200\n",
            "10424/10424 [==============================] - 5s 455us/step - loss: 0.0433 - acc: 0.9851 - val_loss: 6.4545 - val_acc: 0.2901\n",
            "Epoch 170/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0431 - acc: 0.9853 - val_loss: 6.5568 - val_acc: 0.2824\n",
            "Epoch 171/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0425 - acc: 0.9846 - val_loss: 6.6031 - val_acc: 0.2870\n",
            "Epoch 172/200\n",
            "10424/10424 [==============================] - 5s 447us/step - loss: 0.0447 - acc: 0.9844 - val_loss: 6.5837 - val_acc: 0.2828\n",
            "Epoch 173/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0424 - acc: 0.9842 - val_loss: 6.5883 - val_acc: 0.2870\n",
            "Epoch 174/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0428 - acc: 0.9848 - val_loss: 6.7023 - val_acc: 0.2828\n",
            "Epoch 175/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0481 - acc: 0.9829 - val_loss: 6.6851 - val_acc: 0.2982\n",
            "Epoch 176/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.0478 - acc: 0.9829 - val_loss: 6.7112 - val_acc: 0.2947\n",
            "Epoch 177/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0453 - acc: 0.9838 - val_loss: 6.7604 - val_acc: 0.2897\n",
            "Epoch 178/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0476 - acc: 0.9825 - val_loss: 6.6329 - val_acc: 0.2978\n",
            "Epoch 179/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0430 - acc: 0.9849 - val_loss: 6.6324 - val_acc: 0.2970\n",
            "Epoch 180/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0451 - acc: 0.9837 - val_loss: 6.7326 - val_acc: 0.2855\n",
            "Epoch 181/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0429 - acc: 0.9839 - val_loss: 6.7402 - val_acc: 0.2820\n",
            "Epoch 182/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.0439 - acc: 0.9847 - val_loss: 6.7708 - val_acc: 0.2870\n",
            "Epoch 183/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0430 - acc: 0.9834 - val_loss: 6.8458 - val_acc: 0.2759\n",
            "Epoch 184/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0461 - acc: 0.9840 - val_loss: 6.6630 - val_acc: 0.2893\n",
            "Epoch 185/200\n",
            "10424/10424 [==============================] - 5s 443us/step - loss: 0.0468 - acc: 0.9823 - val_loss: 6.7677 - val_acc: 0.2916\n",
            "Epoch 186/200\n",
            "10424/10424 [==============================] - 5s 448us/step - loss: 0.0482 - acc: 0.9827 - val_loss: 6.5989 - val_acc: 0.2974\n",
            "Epoch 187/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0441 - acc: 0.9847 - val_loss: 6.6677 - val_acc: 0.2993\n",
            "Epoch 188/200\n",
            "10424/10424 [==============================] - 5s 449us/step - loss: 0.0489 - acc: 0.9826 - val_loss: 6.7603 - val_acc: 0.2989\n",
            "Epoch 189/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0528 - acc: 0.9810 - val_loss: 6.7167 - val_acc: 0.2978\n",
            "Epoch 190/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0557 - acc: 0.9826 - val_loss: 6.8020 - val_acc: 0.2939\n",
            "Epoch 191/200\n",
            "10424/10424 [==============================] - 5s 442us/step - loss: 0.0512 - acc: 0.9814 - val_loss: 6.6217 - val_acc: 0.3047\n",
            "Epoch 192/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0528 - acc: 0.9812 - val_loss: 6.5991 - val_acc: 0.3031\n",
            "Epoch 193/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.0542 - acc: 0.9803 - val_loss: 6.7565 - val_acc: 0.3001\n",
            "Epoch 194/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0820 - acc: 0.9724 - val_loss: 6.5489 - val_acc: 0.3227\n",
            "Epoch 195/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1034 - acc: 0.9682 - val_loss: 6.3901 - val_acc: 0.3170\n",
            "Epoch 196/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.1095 - acc: 0.9623 - val_loss: 6.3056 - val_acc: 0.3051\n",
            "Epoch 197/200\n",
            "10424/10424 [==============================] - 5s 445us/step - loss: 0.1079 - acc: 0.9648 - val_loss: 6.3249 - val_acc: 0.3189\n",
            "Epoch 198/200\n",
            "10424/10424 [==============================] - 5s 446us/step - loss: 0.0803 - acc: 0.9741 - val_loss: 6.1413 - val_acc: 0.3051\n",
            "Epoch 199/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0687 - acc: 0.9761 - val_loss: 6.5875 - val_acc: 0.2916\n",
            "Epoch 200/200\n",
            "10424/10424 [==============================] - 5s 444us/step - loss: 0.0611 - acc: 0.9786 - val_loss: 6.5164 - val_acc: 0.3062\n",
            "{0: 0.0, 1: Fraction(1, 12), 2: Fraction(1, 6), 3: 0.25, 4: Fraction(1, 3), 5: Fraction(5, 12), 6: 0.5, 7: Fraction(2, 3), 8: 0.75, 9: 1.0, 10: Fraction(7, 6), 11: 1.25, 12: Fraction(4, 3), 13: 1.5, 14: Fraction(5, 3), 15: 1.75, 16: 2.0, 17: 2.25, 18: 2.5, 19: 3.0, 20: 3.25, 21: 3.5, 22: 4.0, 23: Fraction(25, 6), 24: 4.25, 25: 4.5, 26: 4.75, 27: 5.0, 28: 5.25, 29: 5.75, 30: 6.0, 31: 6.5, 32: 7.0, 33: Fraction(23, 3), 34: 8.0, 35: 8.5, 36: 9.0, 37: 14.5, 38: 16.0, 39: 18.0, 40: 27.5, 41: 32.0, 42: 32.5, 43: 36.0, 44: 36.25, 45: 39.5, 46: 47.5, 47: 48.0, 48: 61.0, 49: 64.25, 50: 68.0, 51: 72.25, 52: 72.5, 53: 80.0, 54: 84.25, 55: Fraction(553, 6), 56: 94.5, 57: 128.25, 58: 161.0, 59: 176.0, 60: 200.5}\n",
            " Predicting.  Duration:  399\n",
            "\n",
            "Running notes training on d:0 s:256\n",
            "Train on 5006 samples, validate on 1252 samples\n",
            "Epoch 1/200\n",
            "5006/5006 [==============================] - 9s 2ms/step - loss: 3.9000 - acc: 0.0657 - val_loss: 4.0611 - val_acc: 0.0367\n",
            "Epoch 2/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 3.7575 - acc: 0.0789 - val_loss: 4.0655 - val_acc: 0.0487\n",
            "Epoch 3/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 3.6437 - acc: 0.0943 - val_loss: 4.2774 - val_acc: 0.0152\n",
            "Epoch 4/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 3.4609 - acc: 0.1213 - val_loss: 4.4604 - val_acc: 0.0399\n",
            "Epoch 5/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 3.2316 - acc: 0.1556 - val_loss: 4.8364 - val_acc: 0.0256\n",
            "Epoch 6/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 3.0000 - acc: 0.1910 - val_loss: 4.9731 - val_acc: 0.0304\n",
            "Epoch 7/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 2.8083 - acc: 0.2313 - val_loss: 5.1403 - val_acc: 0.0351\n",
            "Epoch 8/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 2.9128 - acc: 0.2149 - val_loss: 5.1182 - val_acc: 0.0351\n",
            "Epoch 9/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 2.5107 - acc: 0.2661 - val_loss: 5.3759 - val_acc: 0.0351\n",
            "Epoch 10/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 2.3362 - acc: 0.2913 - val_loss: 5.6585 - val_acc: 0.0280\n",
            "Epoch 11/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 2.1616 - acc: 0.3152 - val_loss: 5.7858 - val_acc: 0.0280\n",
            "Epoch 12/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 2.1214 - acc: 0.3310 - val_loss: 5.8550 - val_acc: 0.0312\n",
            "Epoch 13/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 2.0004 - acc: 0.3388 - val_loss: 5.9292 - val_acc: 0.0391\n",
            "Epoch 14/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.8907 - acc: 0.3596 - val_loss: 6.3354 - val_acc: 0.0351\n",
            "Epoch 15/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.8039 - acc: 0.3710 - val_loss: 6.3883 - val_acc: 0.0399\n",
            "Epoch 16/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.7754 - acc: 0.3775 - val_loss: 6.3991 - val_acc: 0.0312\n",
            "Epoch 17/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.7139 - acc: 0.3933 - val_loss: 6.4903 - val_acc: 0.0240\n",
            "Epoch 18/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.6519 - acc: 0.4007 - val_loss: 6.4516 - val_acc: 0.0367\n",
            "Epoch 19/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.6075 - acc: 0.4159 - val_loss: 6.6989 - val_acc: 0.0455\n",
            "Epoch 20/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.5486 - acc: 0.4209 - val_loss: 6.6184 - val_acc: 0.0224\n",
            "Epoch 21/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.5029 - acc: 0.4383 - val_loss: 6.8220 - val_acc: 0.0319\n",
            "Epoch 22/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.4084 - acc: 0.4614 - val_loss: 6.9778 - val_acc: 0.0431\n",
            "Epoch 23/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.3593 - acc: 0.4716 - val_loss: 6.8940 - val_acc: 0.0615\n",
            "Epoch 24/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.3242 - acc: 0.4820 - val_loss: 7.1395 - val_acc: 0.0423\n",
            "Epoch 25/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.2770 - acc: 0.5044 - val_loss: 7.2351 - val_acc: 0.0319\n",
            "Epoch 26/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.3295 - acc: 0.4884 - val_loss: 7.4195 - val_acc: 0.0463\n",
            "Epoch 27/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.2168 - acc: 0.5208 - val_loss: 7.5529 - val_acc: 0.0567\n",
            "Epoch 28/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.1397 - acc: 0.5475 - val_loss: 7.5980 - val_acc: 0.0479\n",
            "Epoch 29/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.1345 - acc: 0.5537 - val_loss: 7.8028 - val_acc: 0.0327\n",
            "Epoch 30/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.1123 - acc: 0.5607 - val_loss: 7.7824 - val_acc: 0.0447\n",
            "Epoch 31/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.0578 - acc: 0.5627 - val_loss: 7.8560 - val_acc: 0.0527\n",
            "Epoch 32/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.9958 - acc: 0.6023 - val_loss: 7.8667 - val_acc: 0.0439\n",
            "Epoch 33/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 1.0168 - acc: 0.5933 - val_loss: 7.8558 - val_acc: 0.0391\n",
            "Epoch 34/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.9001 - acc: 0.6348 - val_loss: 8.0014 - val_acc: 0.0495\n",
            "Epoch 35/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.8614 - acc: 0.6512 - val_loss: 8.2831 - val_acc: 0.0431\n",
            "Epoch 36/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.9170 - acc: 0.6288 - val_loss: 8.0065 - val_acc: 0.0495\n",
            "Epoch 37/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.8333 - acc: 0.6712 - val_loss: 8.1557 - val_acc: 0.0447\n",
            "Epoch 38/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.7707 - acc: 0.6886 - val_loss: 8.2339 - val_acc: 0.0367\n",
            "Epoch 39/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.7217 - acc: 0.7107 - val_loss: 8.3684 - val_acc: 0.0463\n",
            "Epoch 40/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6954 - acc: 0.7317 - val_loss: 8.7345 - val_acc: 0.0479\n",
            "Epoch 41/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6740 - acc: 0.7375 - val_loss: 8.6598 - val_acc: 0.0495\n",
            "Epoch 42/200\n",
            "5006/5006 [==============================] - 12s 2ms/step - loss: 0.6578 - acc: 0.7399 - val_loss: 8.9361 - val_acc: 0.0551\n",
            "Epoch 43/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6458 - acc: 0.7509 - val_loss: 9.2665 - val_acc: 0.0455\n",
            "Epoch 44/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6216 - acc: 0.7585 - val_loss: 9.0145 - val_acc: 0.0351\n",
            "Epoch 45/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6224 - acc: 0.7669 - val_loss: 8.6498 - val_acc: 0.0415\n",
            "Epoch 46/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.9135 - acc: 0.6468 - val_loss: 8.8690 - val_acc: 0.0367\n",
            "Epoch 47/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6611 - acc: 0.7343 - val_loss: 9.1979 - val_acc: 0.0415\n",
            "Epoch 48/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.5944 - acc: 0.7697 - val_loss: 9.0923 - val_acc: 0.0383\n",
            "Epoch 49/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.5147 - acc: 0.8136 - val_loss: 9.2418 - val_acc: 0.0455\n",
            "Epoch 50/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.4545 - acc: 0.8342 - val_loss: 9.7106 - val_acc: 0.0439\n",
            "Epoch 51/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.4583 - acc: 0.8326 - val_loss: 9.7188 - val_acc: 0.0407\n",
            "Epoch 52/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.4232 - acc: 0.8480 - val_loss: 9.8888 - val_acc: 0.0351\n",
            "Epoch 53/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.4169 - acc: 0.8544 - val_loss: 10.0386 - val_acc: 0.0463\n",
            "Epoch 54/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.3864 - acc: 0.8636 - val_loss: 10.0417 - val_acc: 0.0535\n",
            "Epoch 55/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.4035 - acc: 0.8614 - val_loss: 10.0699 - val_acc: 0.0431\n",
            "Epoch 56/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.3546 - acc: 0.8738 - val_loss: 10.3375 - val_acc: 0.0447\n",
            "Epoch 57/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.3552 - acc: 0.8750 - val_loss: 10.0983 - val_acc: 0.0471\n",
            "Epoch 58/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.3145 - acc: 0.8945 - val_loss: 10.1863 - val_acc: 0.0407\n",
            "Epoch 59/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.4243 - acc: 0.8550 - val_loss: 10.4876 - val_acc: 0.0487\n",
            "Epoch 60/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6320 - acc: 0.7671 - val_loss: 10.2542 - val_acc: 0.0431\n",
            "Epoch 61/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.4371 - acc: 0.8458 - val_loss: 10.4828 - val_acc: 0.0519\n",
            "Epoch 62/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.3366 - acc: 0.8859 - val_loss: 10.1660 - val_acc: 0.0543\n",
            "Epoch 63/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2782 - acc: 0.9085 - val_loss: 10.2695 - val_acc: 0.0543\n",
            "Epoch 64/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2380 - acc: 0.9285 - val_loss: 10.5707 - val_acc: 0.0479\n",
            "Epoch 65/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2259 - acc: 0.9325 - val_loss: 10.6410 - val_acc: 0.0519\n",
            "Epoch 66/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2539 - acc: 0.9209 - val_loss: 10.9656 - val_acc: 0.0511\n",
            "Epoch 67/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2329 - acc: 0.9293 - val_loss: 10.6864 - val_acc: 0.0479\n",
            "Epoch 68/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2481 - acc: 0.9185 - val_loss: 11.1629 - val_acc: 0.0447\n",
            "Epoch 69/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2698 - acc: 0.9047 - val_loss: 11.3435 - val_acc: 0.0455\n",
            "Epoch 70/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1986 - acc: 0.9399 - val_loss: 11.0339 - val_acc: 0.0367\n",
            "Epoch 71/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1752 - acc: 0.9481 - val_loss: 10.9756 - val_acc: 0.0463\n",
            "Epoch 72/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1596 - acc: 0.9541 - val_loss: 10.9017 - val_acc: 0.0479\n",
            "Epoch 73/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1593 - acc: 0.9539 - val_loss: 11.1561 - val_acc: 0.0479\n",
            "Epoch 74/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1380 - acc: 0.9614 - val_loss: 11.4312 - val_acc: 0.0375\n",
            "Epoch 75/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1531 - acc: 0.9579 - val_loss: 11.4594 - val_acc: 0.0511\n",
            "Epoch 76/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1115 - acc: 0.9696 - val_loss: 11.2828 - val_acc: 0.0439\n",
            "Epoch 77/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1425 - acc: 0.9602 - val_loss: 11.4714 - val_acc: 0.0335\n",
            "Epoch 78/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1884 - acc: 0.9445 - val_loss: 11.2854 - val_acc: 0.0463\n",
            "Epoch 79/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1531 - acc: 0.9573 - val_loss: 11.5793 - val_acc: 0.0391\n",
            "Epoch 80/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1857 - acc: 0.9421 - val_loss: 11.4806 - val_acc: 0.0415\n",
            "Epoch 81/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2104 - acc: 0.9311 - val_loss: 11.8046 - val_acc: 0.0415\n",
            "Epoch 82/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1763 - acc: 0.9473 - val_loss: 11.3270 - val_acc: 0.0503\n",
            "Epoch 83/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1381 - acc: 0.9612 - val_loss: 11.5443 - val_acc: 0.0407\n",
            "Epoch 84/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0982 - acc: 0.9750 - val_loss: 11.7219 - val_acc: 0.0487\n",
            "Epoch 85/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0787 - acc: 0.9808 - val_loss: 11.7608 - val_acc: 0.0455\n",
            "Epoch 86/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0706 - acc: 0.9818 - val_loss: 11.8614 - val_acc: 0.0495\n",
            "Epoch 87/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0886 - acc: 0.9778 - val_loss: 12.0073 - val_acc: 0.0463\n",
            "Epoch 88/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0880 - acc: 0.9760 - val_loss: 11.9416 - val_acc: 0.0543\n",
            "Epoch 89/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.9172 - acc: 0.7325 - val_loss: 10.6664 - val_acc: 0.0399\n",
            "Epoch 90/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.7034 - acc: 0.7711 - val_loss: 11.4420 - val_acc: 0.0423\n",
            "Epoch 91/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.3684 - acc: 0.8750 - val_loss: 11.2216 - val_acc: 0.0479\n",
            "Epoch 92/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2091 - acc: 0.9323 - val_loss: 11.4590 - val_acc: 0.0439\n",
            "Epoch 93/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2063 - acc: 0.9369 - val_loss: 11.4282 - val_acc: 0.0495\n",
            "Epoch 94/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1688 - acc: 0.9503 - val_loss: 11.2510 - val_acc: 0.0487\n",
            "Epoch 95/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1337 - acc: 0.9644 - val_loss: 11.4310 - val_acc: 0.0455\n",
            "Epoch 96/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0845 - acc: 0.9808 - val_loss: 11.4693 - val_acc: 0.0527\n",
            "Epoch 97/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0713 - acc: 0.9840 - val_loss: 11.6487 - val_acc: 0.0503\n",
            "Epoch 98/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0601 - acc: 0.9888 - val_loss: 11.5823 - val_acc: 0.0375\n",
            "Epoch 99/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0417 - acc: 0.9926 - val_loss: 11.6105 - val_acc: 0.0447\n",
            "Epoch 100/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0353 - acc: 0.9932 - val_loss: 11.6517 - val_acc: 0.0527\n",
            "Epoch 101/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0328 - acc: 0.9934 - val_loss: 11.7621 - val_acc: 0.0519\n",
            "Epoch 102/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0272 - acc: 0.9962 - val_loss: 11.8234 - val_acc: 0.0471\n",
            "Epoch 103/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0322 - acc: 0.9950 - val_loss: 11.7807 - val_acc: 0.0487\n",
            "Epoch 104/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0340 - acc: 0.9942 - val_loss: 11.9293 - val_acc: 0.0535\n",
            "Epoch 105/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0393 - acc: 0.9922 - val_loss: 11.9308 - val_acc: 0.0495\n",
            "Epoch 106/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0320 - acc: 0.9934 - val_loss: 11.9736 - val_acc: 0.0503\n",
            "Epoch 107/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0296 - acc: 0.9928 - val_loss: 11.9309 - val_acc: 0.0527\n",
            "Epoch 108/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0244 - acc: 0.9958 - val_loss: 12.0224 - val_acc: 0.0535\n",
            "Epoch 109/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0255 - acc: 0.9946 - val_loss: 12.0714 - val_acc: 0.0535\n",
            "Epoch 110/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0269 - acc: 0.9944 - val_loss: 12.1119 - val_acc: 0.0535\n",
            "Epoch 111/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0274 - acc: 0.9940 - val_loss: 12.0058 - val_acc: 0.0535\n",
            "Epoch 112/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0244 - acc: 0.9952 - val_loss: 12.0951 - val_acc: 0.0559\n",
            "Epoch 113/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0200 - acc: 0.9962 - val_loss: 12.1449 - val_acc: 0.0551\n",
            "Epoch 114/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0199 - acc: 0.9956 - val_loss: 12.2272 - val_acc: 0.0535\n",
            "Epoch 115/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0208 - acc: 0.9960 - val_loss: 12.1857 - val_acc: 0.0479\n",
            "Epoch 116/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0258 - acc: 0.9940 - val_loss: 12.0534 - val_acc: 0.0503\n",
            "Epoch 117/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0223 - acc: 0.9954 - val_loss: 12.2911 - val_acc: 0.0527\n",
            "Epoch 118/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0235 - acc: 0.9944 - val_loss: 12.2158 - val_acc: 0.0527\n",
            "Epoch 119/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0345 - acc: 0.9910 - val_loss: 12.3214 - val_acc: 0.0511\n",
            "Epoch 120/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0719 - acc: 0.9834 - val_loss: 12.1850 - val_acc: 0.0463\n",
            "Epoch 121/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0539 - acc: 0.9862 - val_loss: 12.4030 - val_acc: 0.0399\n",
            "Epoch 122/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.3900 - acc: 0.8825 - val_loss: 11.9885 - val_acc: 0.0479\n",
            "Epoch 123/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6684 - acc: 0.8008 - val_loss: 11.8609 - val_acc: 0.0296\n",
            "Epoch 124/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.4402 - acc: 0.8554 - val_loss: 11.8360 - val_acc: 0.0439\n",
            "Epoch 125/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2586 - acc: 0.9185 - val_loss: 11.7499 - val_acc: 0.0447\n",
            "Epoch 126/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1119 - acc: 0.9688 - val_loss: 11.4612 - val_acc: 0.0511\n",
            "Epoch 127/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0651 - acc: 0.9852 - val_loss: 11.7615 - val_acc: 0.0463\n",
            "Epoch 128/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0430 - acc: 0.9920 - val_loss: 11.6538 - val_acc: 0.0463\n",
            "Epoch 129/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0318 - acc: 0.9940 - val_loss: 11.7141 - val_acc: 0.0503\n",
            "Epoch 130/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0242 - acc: 0.9946 - val_loss: 11.7848 - val_acc: 0.0479\n",
            "Epoch 131/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0226 - acc: 0.9950 - val_loss: 11.7926 - val_acc: 0.0503\n",
            "Epoch 132/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0179 - acc: 0.9974 - val_loss: 11.8943 - val_acc: 0.0455\n",
            "Epoch 133/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0157 - acc: 0.9966 - val_loss: 11.8782 - val_acc: 0.0495\n",
            "Epoch 134/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0215 - acc: 0.9962 - val_loss: 11.9456 - val_acc: 0.0503\n",
            "Epoch 135/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0148 - acc: 0.9970 - val_loss: 11.9571 - val_acc: 0.0495\n",
            "Epoch 136/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0132 - acc: 0.9972 - val_loss: 12.0101 - val_acc: 0.0503\n",
            "Epoch 137/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0125 - acc: 0.9972 - val_loss: 12.0080 - val_acc: 0.0487\n",
            "Epoch 138/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0111 - acc: 0.9972 - val_loss: 12.0347 - val_acc: 0.0503\n",
            "Epoch 139/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0112 - acc: 0.9972 - val_loss: 12.0537 - val_acc: 0.0463\n",
            "Epoch 140/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0106 - acc: 0.9972 - val_loss: 12.1157 - val_acc: 0.0503\n",
            "Epoch 141/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0097 - acc: 0.9982 - val_loss: 12.1111 - val_acc: 0.0503\n",
            "Epoch 142/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0105 - acc: 0.9966 - val_loss: 12.1718 - val_acc: 0.0495\n",
            "Epoch 143/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0102 - acc: 0.9970 - val_loss: 12.1816 - val_acc: 0.0495\n",
            "Epoch 144/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0092 - acc: 0.9978 - val_loss: 12.1696 - val_acc: 0.0455\n",
            "Epoch 145/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0094 - acc: 0.9976 - val_loss: 12.2103 - val_acc: 0.0479\n",
            "Epoch 146/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0091 - acc: 0.9978 - val_loss: 12.2083 - val_acc: 0.0511\n",
            "Epoch 147/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0079 - acc: 0.9978 - val_loss: 12.2470 - val_acc: 0.0455\n",
            "Epoch 148/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0082 - acc: 0.9980 - val_loss: 12.2500 - val_acc: 0.0503\n",
            "Epoch 149/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0081 - acc: 0.9976 - val_loss: 12.2557 - val_acc: 0.0503\n",
            "Epoch 150/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0091 - acc: 0.9976 - val_loss: 12.3121 - val_acc: 0.0455\n",
            "Epoch 151/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0104 - acc: 0.9976 - val_loss: 12.3016 - val_acc: 0.0455\n",
            "Epoch 152/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0078 - acc: 0.9980 - val_loss: 12.3167 - val_acc: 0.0495\n",
            "Epoch 153/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0074 - acc: 0.9980 - val_loss: 12.3434 - val_acc: 0.0503\n",
            "Epoch 154/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0082 - acc: 0.9974 - val_loss: 12.3227 - val_acc: 0.0455\n",
            "Epoch 155/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0124 - acc: 0.9968 - val_loss: 12.4197 - val_acc: 0.0455\n",
            "Epoch 156/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0124 - acc: 0.9970 - val_loss: 12.3977 - val_acc: 0.0439\n",
            "Epoch 157/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0181 - acc: 0.9944 - val_loss: 12.4478 - val_acc: 0.0471\n",
            "Epoch 158/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2346 - acc: 0.9343 - val_loss: 12.5660 - val_acc: 0.0431\n",
            "Epoch 159/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.7235 - acc: 0.7883 - val_loss: 12.1816 - val_acc: 0.0288\n",
            "Epoch 160/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.9608 - acc: 0.7301 - val_loss: 11.7494 - val_acc: 0.0288\n",
            "Epoch 161/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.5311 - acc: 0.8266 - val_loss: 11.8240 - val_acc: 0.0423\n",
            "Epoch 162/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2621 - acc: 0.9195 - val_loss: 11.9127 - val_acc: 0.0423\n",
            "Epoch 163/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1368 - acc: 0.9592 - val_loss: 11.8956 - val_acc: 0.0415\n",
            "Epoch 164/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0870 - acc: 0.9790 - val_loss: 11.9114 - val_acc: 0.0359\n",
            "Epoch 165/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0662 - acc: 0.9868 - val_loss: 11.9462 - val_acc: 0.0447\n",
            "Epoch 166/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0349 - acc: 0.9938 - val_loss: 11.9484 - val_acc: 0.0407\n",
            "Epoch 167/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0232 - acc: 0.9960 - val_loss: 12.0096 - val_acc: 0.0455\n",
            "Epoch 168/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0215 - acc: 0.9962 - val_loss: 11.9958 - val_acc: 0.0399\n",
            "Epoch 169/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0187 - acc: 0.9970 - val_loss: 12.0030 - val_acc: 0.0471\n",
            "Epoch 170/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0160 - acc: 0.9980 - val_loss: 12.0614 - val_acc: 0.0471\n",
            "Epoch 171/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0153 - acc: 0.9970 - val_loss: 12.0989 - val_acc: 0.0455\n",
            "Epoch 172/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0203 - acc: 0.9968 - val_loss: 12.1497 - val_acc: 0.0455\n",
            "Epoch 173/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0162 - acc: 0.9974 - val_loss: 12.0969 - val_acc: 0.0415\n",
            "Epoch 174/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0143 - acc: 0.9978 - val_loss: 12.1583 - val_acc: 0.0415\n",
            "Epoch 175/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0126 - acc: 0.9982 - val_loss: 12.1923 - val_acc: 0.0431\n",
            "Epoch 176/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0105 - acc: 0.9978 - val_loss: 12.2099 - val_acc: 0.0415\n",
            "Epoch 177/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0097 - acc: 0.9974 - val_loss: 12.1943 - val_acc: 0.0415\n",
            "Epoch 178/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0100 - acc: 0.9980 - val_loss: 12.2183 - val_acc: 0.0423\n",
            "Epoch 179/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0089 - acc: 0.9980 - val_loss: 12.2372 - val_acc: 0.0423\n",
            "Epoch 180/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0092 - acc: 0.9974 - val_loss: 12.2256 - val_acc: 0.0431\n",
            "Epoch 181/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0101 - acc: 0.9978 - val_loss: 12.3073 - val_acc: 0.0391\n",
            "Epoch 182/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0079 - acc: 0.9980 - val_loss: 12.3010 - val_acc: 0.0415\n",
            "Epoch 183/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0074 - acc: 0.9980 - val_loss: 12.3195 - val_acc: 0.0415\n",
            "Epoch 184/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0068 - acc: 0.9980 - val_loss: 12.3364 - val_acc: 0.0415\n",
            "Epoch 185/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0071 - acc: 0.9982 - val_loss: 12.3629 - val_acc: 0.0423\n",
            "Epoch 186/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0071 - acc: 0.9982 - val_loss: 12.3289 - val_acc: 0.0423\n",
            "Epoch 187/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0087 - acc: 0.9976 - val_loss: 12.3741 - val_acc: 0.0415\n",
            "Epoch 188/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0075 - acc: 0.9978 - val_loss: 12.4232 - val_acc: 0.0423\n",
            "Epoch 189/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0072 - acc: 0.9980 - val_loss: 12.3847 - val_acc: 0.0415\n",
            "Epoch 190/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0077 - acc: 0.9980 - val_loss: 12.4080 - val_acc: 0.0423\n",
            "Epoch 191/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0177 - acc: 0.9962 - val_loss: 12.4046 - val_acc: 0.0375\n",
            "Epoch 192/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0214 - acc: 0.9958 - val_loss: 12.4456 - val_acc: 0.0415\n",
            "Epoch 193/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0259 - acc: 0.9950 - val_loss: 12.3089 - val_acc: 0.0439\n",
            "Epoch 194/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0352 - acc: 0.9922 - val_loss: 12.2856 - val_acc: 0.0519\n",
            "Epoch 195/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0385 - acc: 0.9902 - val_loss: 12.5855 - val_acc: 0.0383\n",
            "Epoch 196/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.0519 - acc: 0.9874 - val_loss: 12.3282 - val_acc: 0.0415\n",
            "Epoch 197/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.1090 - acc: 0.9684 - val_loss: 12.3417 - val_acc: 0.0431\n",
            "Epoch 198/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.2211 - acc: 0.9345 - val_loss: 12.7935 - val_acc: 0.0343\n",
            "Epoch 199/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.6900 - acc: 0.8064 - val_loss: 11.7300 - val_acc: 0.0415\n",
            "Epoch 200/200\n",
            "5006/5006 [==============================] - 7s 1ms/step - loss: 0.3435 - acc: 0.8981 - val_loss: 11.8742 - val_acc: 0.0543\n",
            "{0: 'A1', 1: 'A2', 2: 'A3', 3: 'A4', 4: 'A5', 5: 'B-1', 6: 'B-2', 7: 'B-3', 8: 'B-4', 9: 'B-5', 10: 'B1', 11: 'B2', 12: 'B3', 13: 'B4', 14: 'B5', 15: 'C#1', 16: 'C#2', 17: 'C#3', 18: 'C#4', 19: 'C#5', 20: 'C#6', 21: 'C1', 22: 'C2', 23: 'C3', 24: 'C4', 25: 'C5', 26: 'C6', 27: 'D1', 28: 'D2', 29: 'D3', 30: 'D4', 31: 'D5', 32: 'D6', 33: 'E-0', 34: 'E-2', 35: 'E-3', 36: 'E-4', 37: 'E-5', 38: 'E-6', 39: 'E1', 40: 'E2', 41: 'E3', 42: 'E4', 43: 'E5', 44: 'E6', 45: 'F#1', 46: 'F#2', 47: 'F#3', 48: 'F#4', 49: 'F#5', 50: 'F#6', 51: 'F0', 52: 'F1', 53: 'F2', 54: 'F3', 55: 'F4', 56: 'F5', 57: 'G#1', 58: 'G#2', 59: 'G#3', 60: 'G#4', 61: 'G#5', 62: 'G0', 63: 'G1', 64: 'G2', 65: 'G3', 66: 'G4', 67: 'G5'}\n",
            " Predicting.  Note:  399\n",
            "Wrote midi...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "bj7oTMXCs0hX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Notewise Chord"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y2pOeD5RtGXz"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Set Training Parameters*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "gbfHxM9RtGX6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SET PARAMETERS (ACTION)\n",
        "RESTS = True\n",
        "ROOT_EXTRACTION = True\n",
        "DURATION_BATCH_SIZE = 256\n",
        "NOTE_BATCH_SIZE = 128\n",
        "# SPECIFY PARAMETERS TO TEST AS LIST\n",
        "DROPOUTS = 0\n",
        "MODEL_SIZES = 256\n",
        "EPOCHS = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x_MnJoWhtPZh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "vWUiJVkktPpb"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 Preprocess Data (MIDI Data Into Notes Corpus and Duration Corpus)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "24ddc675-a554-4e0b-a2c8-070822d79a24",
        "id": "kKoVkdHOtPph",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1044
        }
      },
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "notes_corpus = []\n",
        "durations_corpus = []\n",
        "\n",
        "for file in glob.glob(DATA_FOLDER + \"/*.mid\"):\n",
        "    try:\n",
        "      print(\"Extracting MIDI File: \", file)\n",
        "      midi_stream = converter.parse(file)\n",
        "\n",
        "      notes = None\n",
        "\n",
        "      partition = instrument.partitionByInstrument(midi_stream)\n",
        "\n",
        "      if not RESTS:\n",
        "        # No rests\n",
        "        if partition: \n",
        "            notes = partition.parts[0].recurse()\n",
        "        else: \n",
        "            notes = midi_stream.flat.notes\n",
        "      else:\n",
        "        # With rests\n",
        "        if partition: \n",
        "            notes = partition.parts[0].recurse()\n",
        "        else: \n",
        "            notes = midi_stream.notesAndRests\n",
        "          \n",
        "      in_song_notes = []\n",
        "      in_song_durations = []\n",
        "      for element in notes:\n",
        "          in_song_durations.append(element.duration.quarterLength)\n",
        "          if isinstance(element, note.Note):\n",
        "              in_song_notes.append(str(element.pitch))\n",
        "          elif RESTS and isinstance(element, note.Rest):\n",
        "              in_song_notes.append(\"R\")\n",
        "          elif isinstance(element, chord.Chord):\n",
        "              if ROOT_EXTRACTION:\n",
        "                  in_song_notes.append(element.root().nameWithOctave)\n",
        "              else:\n",
        "                  in_song.append('.'.join(str(n) for n in element.normalOrder))\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    notes_corpus.append(in_song_notes)\n",
        "    durations_corpus.append(in_song_durations)\n",
        "            \n",
        "# Write\n",
        "with open(INTERMED_FOLDER + CORPUS + 'notes_corpus', 'wb+') as filepath:\n",
        "    pickle.dump(notes_corpus, filepath)\n",
        "    \n",
        "with open(INTERMED_FOLDER + CORPUS + 'durations_corpus', 'wb+') as filepath:\n",
        "    pickle.dump(durations_corpus, filepath)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Afrojack _ David Guetta - Another Life  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Lonely Together.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Axwell _ Ingrosso - More Than You Know  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Sing Me To Sleep  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Without You.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alesso ft. Matthew Koma - Years (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Tired  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Alone  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Faded (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Dear Boy.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Axwell _ Shapov - Belong (Axwell and Years Remode) (Midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 2 - C Maj.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 3 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Bruno Mars - That_s What I Like (Alan Walker Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Calvin Harris - Blame  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Calvin Harris - My Way  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 5 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 6 - F Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 1 - C Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 4 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 7 - F Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 14 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 8 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 16 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 9 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 15 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 10 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 13 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 12 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 11 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Galantis - Love On Me  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kill Paris ft. Royal - Operate (Illenium Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Headhunterz _ KSHMR - Dharma  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Ritual (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Galantis - No Money  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Summer  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kygo _ Selena Gomez - It Ain_t Me  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Alone (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kygo _ Ellie Goulding - First Time  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Madeon - Icarus  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix _ Bebe Rexha - In The Name Of Love  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix - There For You  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix ft. The Federal Empire - Hold On _ Believe (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Porter Robinson _ Madeon - Shelter (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix _ Dua Lipa - Scared To Be Lonely  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Porter Robinson - Sad Machine  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Nero - The Thrill (Porter Robinson Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Seven Lions x Echos - Cold Skin  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Sebastian Ingrosso _ Alesso feat. Ryan Tedder - Calling (Lose My Mind).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/R3hab _ KSHMR - Strong  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers _ Coldplay - Something Just Like This.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers ft. Halsey - Closer.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers ft. Phoebe Ryan - All We Know.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers Ft. Daya - Don_t Let Me Down.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers - Paris.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Tiesto _ KSHMR feat. Vassy - Secrets  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Zedd _ Hailee Steinfeld Grey - Starving  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers feat. XYLO - Setting Fires.mid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GYSQiAWVtXbj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "tRMoIgZatXqT"
      },
      "cell_type": "markdown",
      "source": [
        "# 5 Preprocess Corpus Into Train Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "51JkV8VEtXqb"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Notes Corpus"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "2300e8c8-3e24-492f-884d-a1df521fec83",
        "id": "etZBEwNxtXqd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# load notes corpus\n",
        "with open(INTERMED_FOLDER + CORPUS + 'notes_corpus', 'rb') as filepath:\n",
        "    notes_corpus = pickle.load(filepath)\n",
        "    \n",
        "# If doing learning for one song at a time only\n",
        "flattened_notes_corpus = []\n",
        "for song_notes in notes_corpus:\n",
        "    flattened_notes_corpus += song_notes\n",
        "\n",
        "vocab_size = len(set(flattened_notes_corpus))\n",
        "print(vocab_size)\n",
        "\n",
        "# Produce input output sequences\n",
        "window_size = 60\n",
        "note_sequence_input = []\n",
        "next_note_output = []\n",
        "\n",
        "notes = sorted(set(flattened_notes_corpus))\n",
        "note2int = dict((note, num) for num, note in enumerate(notes))\n",
        "\n",
        "for i in range(0, len(notes_corpus)):\n",
        "    for j in range(0, len(notes_corpus[i]) - window_size):\n",
        "        current_sequence = [note2int[note] for note in notes_corpus[i][j:window_size+j]]\n",
        "        next_note = note2int[notes_corpus[i][window_size+j]]\n",
        "        note_sequence_input.append(current_sequence)\n",
        "        next_note_output.append(next_note)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "818f5518-1719-4a78-bcc3-af03240f0f51",
        "id": "aLLlPYu1tXqu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# check train and label shapes\n",
        "training_data = np.reshape(note_sequence_input, (len(note_sequence_input), window_size , 1))\n",
        "training_data = training_data / float(vocab_size)\n",
        "print('Train shape: ' + str(training_data.shape))\n",
        "training_label = np_utils.to_categorical(next_note_output)\n",
        "print('Label shape: ' + str(training_label.shape))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (11392, 60, 1)\n",
            "Label shape: (11392, 65)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "H7ri60yFtXq7"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.2 Durations Corpus"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "b620679c-a031-440e-9d14-c317492d4c22",
        "id": "6q3IxrwHtXq8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "cell_type": "code",
      "source": [
        "# load notes corpus\n",
        "with open(INTERMED_FOLDER + CORPUS + 'durations_corpus', 'rb') as filepath:\n",
        "    duration_corpus = pickle.load(filepath)\n",
        "\n",
        "# Learn on one song at a time\n",
        "flattened_duration_corpus = []\n",
        "for song_durations in duration_corpus:\n",
        "    flattened_duration_corpus += song_durations\n",
        "    \n",
        "import collections\n",
        "counter = collections.Counter(flattened_duration_corpus)\n",
        "print(counter)\n",
        "\n",
        "\n",
        "duration_vocab_size = len(set(flattened_duration_corpus))\n",
        "print(duration_vocab_size)\n",
        "\n",
        "# Produce input output sequences\n",
        "duration_window_size = 10\n",
        "duration_sequence_input = []\n",
        "next_duration_output = []\n",
        "\n",
        "durations = sorted(set(flattened_duration_corpus))\n",
        "duration2int = dict((duration, num) for num, duration in enumerate(durations))\n",
        "\n",
        "print(duration2int)\n",
        "# Write\n",
        "with open(INTERMED_FOLDER + \"/edm_duration_counter\", 'wb+') as filepath:\n",
        "    pickle.dump(counter, filepath)\n",
        "\n",
        "for i in range(0, len(duration_corpus)):\n",
        "    for j in range(0, len(duration_corpus[i]) - duration_window_size):\n",
        "        current_duration_sequence = [duration2int[note] for note in duration_corpus[i][j:duration_window_size+j]]\n",
        "        next_duration = duration2int[duration_corpus[i][duration_window_size+j]]\n",
        "        duration_sequence_input.append(current_duration_sequence)\n",
        "        next_duration_output.append(next_duration)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({0.25: 4993, 0.5: 4383, 0.0: 966, Fraction(1, 3): 935, 1.0: 735, Fraction(1, 6): 729, 0.75: 461, 4.0: 165, 2.0: 122, Fraction(2, 3): 93, Fraction(1, 12): 86, Fraction(5, 12): 72, 1.5: 55, 3.0: 41, Fraction(7, 6): 32, 1.75: 30, 1.25: 29, 2.25: 25, 4.5: 22, 3.5: 13, 6.5: 13, 8.0: 11, 4.25: 9, 2.5: 9, Fraction(25, 6): 7, 5.0: 6, 7.0: 5, 8.5: 4, 32.0: 4, 6.0: 4, Fraction(5, 3): 3, 48.0: 2, 68.0: 2, 176.0: 2, 80.0: 2, 16.0: 2, Fraction(23, 3): 1, 47.5: 1, 3.25: 1, 5.75: 1, 200.5: 1, 36.25: 1, 64.25: 1, 84.25: 1, 72.5: 1, 72.25: 1, 14.5: 1, 9.0: 1, 27.5: 1, 61.0: 1, 161.0: 1, 5.25: 1, 4.75: 1, 94.5: 1, 128.25: 1, 18.0: 1, 39.5: 1, Fraction(553, 6): 1, Fraction(4, 3): 1, 32.5: 1, 36.0: 1})\n",
            "61\n",
            "{0.0: 0, Fraction(1, 12): 1, Fraction(1, 6): 2, 0.25: 3, Fraction(1, 3): 4, Fraction(5, 12): 5, 0.5: 6, Fraction(2, 3): 7, 0.75: 8, 1.0: 9, Fraction(7, 6): 10, 1.25: 11, Fraction(4, 3): 12, 1.5: 13, Fraction(5, 3): 14, 1.75: 15, 2.0: 16, 2.25: 17, 2.5: 18, 3.0: 19, 3.25: 20, 3.5: 21, 4.0: 22, Fraction(25, 6): 23, 4.25: 24, 4.5: 25, 4.75: 26, 5.0: 27, 5.25: 28, 5.75: 29, 6.0: 30, 6.5: 31, 7.0: 32, Fraction(23, 3): 33, 8.0: 34, 8.5: 35, 9.0: 36, 14.5: 37, 16.0: 38, 18.0: 39, 27.5: 40, 32.0: 41, 32.5: 42, 36.0: 43, 36.25: 44, 39.5: 45, 47.5: 46, 48.0: 47, 61.0: 48, 64.25: 49, 68.0: 50, 72.25: 51, 72.5: 52, 80.0: 53, 84.25: 54, Fraction(553, 6): 55, 94.5: 56, 128.25: 57, 161.0: 58, 176.0: 59, 200.5: 60}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "af8426d8-59c0-4117-cd29-e4c72ea5b349",
        "id": "cPl-PCGEtXrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "# check train and label shapes\n",
        "duration_training_data = np.reshape(duration_sequence_input, (len(duration_sequence_input), duration_window_size , 1))\n",
        "duration_training_data = duration_training_data / float(duration_vocab_size)\n",
        "print('Train shape: ' + str(duration_training_data.shape))\n",
        "duration_training_label = np_utils.to_categorical(next_duration_output)\n",
        "print('Label shape: ' + str(duration_training_label.shape))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (13677, 10, 1)\n",
            "Label shape: (13677, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "7pUmnGOotnKP"
      },
      "cell_type": "markdown",
      "source": [
        "# 6 Train"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TGbx8eNbtnKS"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1 Helpers to Create Model"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5X6lMFXOtnKV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_model(network_input, n_vocab, model_size, dropout):\n",
        "  model = Sequential()\n",
        "  reg = L1L2(0, 0)\n",
        "  model.add(LSTM(\n",
        "      model_size,\n",
        "      input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "      return_sequences=True,\n",
        "      dropout=dropout, recurrent_dropout=0.3\n",
        "  ))\n",
        "  model.add(CuDNNLSTM(model_size, return_sequences=True, kernel_regularizer=reg))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(CuDNNLSTM(model_size, kernel_regularizer=reg))\n",
        "  model.add(Dense(128))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_callback_list(model_size, dropout, model_type):\n",
        "  filepath = INTERMED_FOLDER + '/%skarpathy-model-weights-%s-%s-%s.hdf5' % (CORPUS, model_type, model_size, dropout)\n",
        "  model_checkpoint = ModelCheckpoint(\n",
        "    filepath,\n",
        "    monitor='loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    mode='min'\n",
        "  )\n",
        "  return [model_checkpoint], filepath\n",
        "\n",
        "# acc history\n",
        "def setup_plot(dropout, size):\n",
        "  plt.title('Model Accuracy vs. Epoc with Dropout=%s Size=%s' % (dropout, size))\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  \n",
        "def plot_history(history, model_type, dropout, size):\n",
        "  plt.plot(history.history['acc'], label=\"%s train accuracy\" % model_type)\n",
        "  plt.plot(history.history['val_acc'], label=\"%s val accuracy\" % model_type)\n",
        "\n",
        "def save_plot(file_path):\n",
        "  plt.legend()\n",
        "  plt.savefig(file_path)\n",
        "  plt.clf()\n",
        "  \n",
        "def predict_duration(model, WEIGHT_PATH):\n",
        "  # Prediction\n",
        "  model.load_weights(WEIGHT_PATH)\n",
        "  starting_sequence = np.random.randint(219, size=duration_window_size)\n",
        "  pattern_sequence = starting_sequence.tolist()\n",
        "  prediction_output = []\n",
        "\n",
        "  duration2note = dict((num, note) for num, note in enumerate(durations))\n",
        "  print (duration2note)\n",
        "  \n",
        "  for i in range(400):\n",
        "      prediction_input = np.reshape(pattern_sequence, (1, len(pattern_sequence), 1))\n",
        "      prediction_input = prediction_input / float(duration_vocab_size)\n",
        "\n",
        "      if i%3 == 0:   \n",
        "          print('\\r', 'Predicting.  Duration: ', i, end='')  \n",
        "      if i%3 == 1:   \n",
        "          print('\\r', 'Predicting.. Duration: ', i, end='')\n",
        "      if i%3 == 2:   \n",
        "          print('\\r', 'Predicting...Duration: ', i, end='')\n",
        "      prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "      prediction_values = np.arange(len(prediction[0]))\n",
        "      prediction_prob = np.asarray(list(prediction[0])) / float(sum(prediction[0]))\n",
        "\n",
        "      # Most probable note prediction\n",
        "#       index = np.argmax(prediction)\n",
        "#       note_instance = duration2note[index]\n",
        "#       prediction_output.append(note_instance)\n",
        "      index = np.random.choice(prediction_values, 1, p=prediction_prob)\n",
        "      note_instance = duration2note[int(index[0])]\n",
        "      prediction_output.append(note_instance)\n",
        "\n",
        "      pattern_sequence.append(index)\n",
        "      pattern_sequence = pattern_sequence[1:len(pattern_sequence)]    \n",
        "\n",
        "  prediction_output = prediction_output[300:len(prediction_output)]\n",
        "\n",
        "  # Write\n",
        "  with open(INTERMED_FOLDER + (\"%sduration_prediction_output\" % CORPUS), 'wb+') as filepath:\n",
        "      pickle.dump(prediction_output, filepath)\n",
        "  \n",
        "  return prediction_output\n",
        "\n",
        "def predict_note(model, WEIGHT_PATH):\n",
        "  model.load_weights(WEIGHT_PATH)\n",
        "  # Prediction\n",
        "  starting_sequence = np.random.randint(219, size=window_size)\n",
        "  pattern_sequence = starting_sequence.tolist()\n",
        "  prediction_output = []\n",
        "\n",
        "  int2note = dict((num, note) for num, note in enumerate(notes))\n",
        "  print (int2note)\n",
        "\n",
        "  for i in range(400):\n",
        "      prediction_input = np.reshape(pattern_sequence, (1, len(pattern_sequence), 1))\n",
        "      prediction_input = prediction_input / float(vocab_size)\n",
        "\n",
        "      if i%3 == 0:   \n",
        "          print('\\r', 'Predicting.  Note: ', i, end='')  \n",
        "      if i%3 == 1:   \n",
        "          print('\\r', 'Predicting.. Note: ', i, end='')\n",
        "      if i%3 == 2:   \n",
        "          print('\\r', 'Predicting...Note: ', i, end='')\n",
        "      prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "#       prediction_values = np.arange(len(prediction[0]))\n",
        "#       prediction_prob = np.asarray(list(prediction[0])) / float(sum(prediction[0]))\n",
        "#       print(prediction_prob)\n",
        "\n",
        "      # Most probable note prediction\n",
        "      index = np.argmax(prediction)\n",
        "      note_instance = int2note[index]\n",
        "      prediction_output.append(note_instance)\n",
        "\n",
        "      # Predict based on prob dist\n",
        "#       index = np.random.choice(prediction_values, 1, p=prediction_prob)\n",
        "  #     print(index[0])\n",
        "  #     print(type(index[0]))\n",
        "#       note_instance = int2note[int(index[0])]\n",
        "#       prediction_output.append(note_instance)\n",
        "\n",
        "      pattern_sequence.append(index)\n",
        "      pattern_sequence = pattern_sequence[1:len(pattern_sequence)]    \n",
        "\n",
        "  prediction_output = prediction_output[300:len(prediction_output)]\n",
        "  # Write\n",
        "  with open(INTERMED_FOLDER + (\"%snotes_prediction_output\" % CORPUS), 'wb+') as filepath:\n",
        "      pickle.dump(prediction_output, filepath)\n",
        "  return prediction_output\n",
        "\n",
        "def output_midi(prediction_output, duration_prediction_output, dropout, model_size):\n",
        "  offset = 0\n",
        "  output_notes = []\n",
        "  for pattern, duration in zip(prediction_output, duration_prediction_output):\n",
        "\n",
        "      if ('.' in pattern) or pattern.isdigit():\n",
        "          chord_array = pattern.split('.')\n",
        "          chord_notes = []\n",
        "          for note_instance in chord_array:\n",
        "              note_object = note.Note(int(note_instance))\n",
        "              note_object.duration.quarterLength = duration\n",
        "              note_object.storedInstrument = instrument.Piano()\n",
        "              chord_notes.append(note_object)\n",
        "          chord_object = chord.Chord(chord_notes)\n",
        "          chord_object.offset = offset\n",
        "          output_notes.append(chord_object)\n",
        "      elif 'R' == pattern:\n",
        "          note_object = note.Rest()\n",
        "          note_object.duration.quarterLength = duration\n",
        "          note_object.offset = offset\n",
        "          output_notes.append(note_object)\n",
        "      else:\n",
        "          note_object = note.Note(pattern)\n",
        "          note_object.duration.quarterLength = duration\n",
        "          note_object.offset = offset\n",
        "          note_object.storedInstrument = instrument.Piano()\n",
        "          output_notes.append(note_object)\n",
        "\n",
        "      offset += 0.5\n",
        "\n",
        "  midi_stream = stream.Stream(output_notes)\n",
        "  midi_stream.write('midi', fp=MIDI_OUTPUT_FOLDER + CORPUS + '%s_%s.mid' % (dropout, model_size))\n",
        "  print('\\nWrote midi...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eupgZUEXtnKa"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2 Preprocessing Optimization"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "GLAefFactnKc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 14121
        },
        "outputId": "6cc9592e-51ac-46f3-88ea-7fb6d846bec5"
      },
      "cell_type": "code",
      "source": [
        "notes_histories = {}\n",
        "duration_histories = {}\n",
        "\n",
        "dropout, model_size = DROPOUTS, MODEL_SIZES\n",
        "setup_plot(dropout, model_size)\n",
        "print('Running duration training on notewise with rests:%s and root extraction:%s' % (RESTS, ROOT_EXTRACTION))\n",
        "duration_callbacks, duration_weight_path = create_callback_list('duration', dropout, model_size)\n",
        "duration_model = create_model(duration_training_data, duration_vocab_size, model_size, dropout)\n",
        "duration_histories[(dropout, model_size)] = duration_model.fit(duration_training_data, duration_training_label, epochs=EPOCHS, batch_size=DURATION_BATCH_SIZE, callbacks=duration_callbacks, validation_split=0.2)\n",
        "plot_history(duration_histories[(dropout, model_size)], 'Durations', dropout, model_size)\n",
        "# output intermed duration\n",
        "duration_prediction = predict_duration(duration_model, duration_weight_path)\n",
        "\n",
        "print('\\n\\nRunning notes training on d:%s s:%s' % (dropout, model_size))\n",
        "notes_callbacks, note_weight_path = create_callback_list('notes', dropout, model_size)\n",
        "notes_model = create_model(training_data, vocab_size, model_size, dropout)\n",
        "notes_histories[(dropout, model_size)] = notes_model.fit(training_data, training_label, epochs=EPOCHS, batch_size=NOTE_BATCH_SIZE, callbacks=notes_callbacks, validation_split=0.2)\n",
        "plot_history(notes_histories[(dropout, model_size)], 'Notes', dropout, model_size)\n",
        "save_plot(GRAPHS_FOLDER + EDM_CORPUS + 'dropout=%s_size=%s' % (dropout, model_size))\n",
        "# output intermed notes\n",
        "note_prediction = predict_note(notes_model, note_weight_path)\n",
        "\n",
        "# output final midi\n",
        "output_midi(note_prediction, duration_prediction, dropout, model_size)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running duration training on notewise with rests:True and root extraction:True\n",
            "Train on 10941 samples, validate on 2736 samples\n",
            "Epoch 1/200\n",
            "10941/10941 [==============================] - 5s 426us/step - loss: 2.2891 - acc: 0.3439 - val_loss: 2.0196 - val_acc: 0.2606\n",
            "Epoch 2/200\n",
            "10941/10941 [==============================] - 2s 193us/step - loss: 1.8384 - acc: 0.3870 - val_loss: 1.9654 - val_acc: 0.2606\n",
            "Epoch 3/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 1.7894 - acc: 0.4261 - val_loss: 1.9548 - val_acc: 0.4079\n",
            "Epoch 4/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 1.6681 - acc: 0.4912 - val_loss: 1.9329 - val_acc: 0.4291\n",
            "Epoch 5/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 1.6382 - acc: 0.4939 - val_loss: 1.9242 - val_acc: 0.4232\n",
            "Epoch 6/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 1.6266 - acc: 0.4991 - val_loss: 1.9470 - val_acc: 0.4276\n",
            "Epoch 7/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 1.6042 - acc: 0.4980 - val_loss: 1.9103 - val_acc: 0.4218\n",
            "Epoch 8/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 1.5851 - acc: 0.4988 - val_loss: 1.8759 - val_acc: 0.4276\n",
            "Epoch 9/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 1.5819 - acc: 0.5001 - val_loss: 1.8917 - val_acc: 0.4178\n",
            "Epoch 10/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 1.5725 - acc: 0.4988 - val_loss: 1.9269 - val_acc: 0.4137\n",
            "Epoch 11/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 1.5485 - acc: 0.5048 - val_loss: 1.9417 - val_acc: 0.4163\n",
            "Epoch 12/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 1.5397 - acc: 0.5073 - val_loss: 1.9183 - val_acc: 0.4265\n",
            "Epoch 13/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 1.5290 - acc: 0.5073 - val_loss: 2.0022 - val_acc: 0.4287\n",
            "Epoch 14/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 1.5099 - acc: 0.5155 - val_loss: 1.9627 - val_acc: 0.4203\n",
            "Epoch 15/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 1.5109 - acc: 0.5170 - val_loss: 1.9187 - val_acc: 0.4518\n",
            "Epoch 16/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 1.4635 - acc: 0.5275 - val_loss: 1.9013 - val_acc: 0.4229\n",
            "Epoch 17/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 1.4856 - acc: 0.5141 - val_loss: 1.9445 - val_acc: 0.4459\n",
            "Epoch 18/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 1.4162 - acc: 0.5366 - val_loss: 1.9535 - val_acc: 0.4419\n",
            "Epoch 19/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 1.4039 - acc: 0.5353 - val_loss: 1.8569 - val_acc: 0.4273\n",
            "Epoch 20/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 1.3111 - acc: 0.5691 - val_loss: 1.9421 - val_acc: 0.4474\n",
            "Epoch 21/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 1.3079 - acc: 0.5678 - val_loss: 1.9302 - val_acc: 0.4547\n",
            "Epoch 22/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 1.2269 - acc: 0.5974 - val_loss: 2.0429 - val_acc: 0.4492\n",
            "Epoch 23/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 1.2055 - acc: 0.5999 - val_loss: 1.9975 - val_acc: 0.4401\n",
            "Epoch 24/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 1.1256 - acc: 0.6352 - val_loss: 2.0614 - val_acc: 0.4488\n",
            "Epoch 25/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 1.0537 - acc: 0.6576 - val_loss: 2.0740 - val_acc: 0.4653\n",
            "Epoch 26/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.9958 - acc: 0.6757 - val_loss: 2.1220 - val_acc: 0.4412\n",
            "Epoch 27/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.9605 - acc: 0.6864 - val_loss: 2.0817 - val_acc: 0.4499\n",
            "Epoch 28/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.8756 - acc: 0.7151 - val_loss: 2.1252 - val_acc: 0.4433\n",
            "Epoch 29/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.8461 - acc: 0.7210 - val_loss: 2.2592 - val_acc: 0.4708\n",
            "Epoch 30/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.8561 - acc: 0.7152 - val_loss: 2.2339 - val_acc: 0.4390\n",
            "Epoch 31/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.7617 - acc: 0.7504 - val_loss: 2.3347 - val_acc: 0.4488\n",
            "Epoch 32/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.7063 - acc: 0.7625 - val_loss: 2.4309 - val_acc: 0.4327\n",
            "Epoch 33/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.6443 - acc: 0.7894 - val_loss: 2.5070 - val_acc: 0.4529\n",
            "Epoch 34/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.6073 - acc: 0.8062 - val_loss: 2.5660 - val_acc: 0.4748\n",
            "Epoch 35/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.5710 - acc: 0.8180 - val_loss: 2.7780 - val_acc: 0.4415\n",
            "Epoch 36/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.5323 - acc: 0.8304 - val_loss: 2.7633 - val_acc: 0.4408\n",
            "Epoch 37/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.5322 - acc: 0.8280 - val_loss: 3.0020 - val_acc: 0.4280\n",
            "Epoch 38/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.4777 - acc: 0.8471 - val_loss: 2.9268 - val_acc: 0.4561\n",
            "Epoch 39/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.4275 - acc: 0.8655 - val_loss: 3.0065 - val_acc: 0.4492\n",
            "Epoch 40/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.4006 - acc: 0.8724 - val_loss: 3.0476 - val_acc: 0.4510\n",
            "Epoch 41/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.3932 - acc: 0.8738 - val_loss: 3.0818 - val_acc: 0.4357\n",
            "Epoch 42/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.3443 - acc: 0.8911 - val_loss: 3.2270 - val_acc: 0.4046\n",
            "Epoch 43/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.3313 - acc: 0.8935 - val_loss: 3.2674 - val_acc: 0.4660\n",
            "Epoch 44/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.3177 - acc: 0.8990 - val_loss: 3.4363 - val_acc: 0.4437\n",
            "Epoch 45/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.4616 - acc: 0.8510 - val_loss: 3.2011 - val_acc: 0.4448\n",
            "Epoch 46/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.3031 - acc: 0.9039 - val_loss: 3.1577 - val_acc: 0.4408\n",
            "Epoch 47/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.2671 - acc: 0.9155 - val_loss: 3.4073 - val_acc: 0.4247\n",
            "Epoch 48/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.2482 - acc: 0.9224 - val_loss: 3.4350 - val_acc: 0.4507\n",
            "Epoch 49/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.2390 - acc: 0.9219 - val_loss: 3.5185 - val_acc: 0.4444\n",
            "Epoch 50/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.2359 - acc: 0.9264 - val_loss: 3.5390 - val_acc: 0.4539\n",
            "Epoch 51/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.2354 - acc: 0.9240 - val_loss: 3.5625 - val_acc: 0.4539\n",
            "Epoch 52/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.2159 - acc: 0.9300 - val_loss: 3.6547 - val_acc: 0.4463\n",
            "Epoch 53/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.2072 - acc: 0.9343 - val_loss: 3.7120 - val_acc: 0.4236\n",
            "Epoch 54/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.2052 - acc: 0.9328 - val_loss: 3.8624 - val_acc: 0.4393\n",
            "Epoch 55/200\n",
            "10941/10941 [==============================] - 2s 188us/step - loss: 0.1995 - acc: 0.9357 - val_loss: 3.8221 - val_acc: 0.4218\n",
            "Epoch 56/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1984 - acc: 0.9372 - val_loss: 3.7505 - val_acc: 0.4225\n",
            "Epoch 57/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1912 - acc: 0.9394 - val_loss: 3.7554 - val_acc: 0.4349\n",
            "Epoch 58/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1875 - acc: 0.9381 - val_loss: 3.9585 - val_acc: 0.4207\n",
            "Epoch 59/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1829 - acc: 0.9399 - val_loss: 3.8135 - val_acc: 0.4459\n",
            "Epoch 60/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1892 - acc: 0.9375 - val_loss: 3.8907 - val_acc: 0.4444\n",
            "Epoch 61/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1887 - acc: 0.9389 - val_loss: 3.8266 - val_acc: 0.4448\n",
            "Epoch 62/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1869 - acc: 0.9405 - val_loss: 3.9688 - val_acc: 0.4280\n",
            "Epoch 63/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1780 - acc: 0.9410 - val_loss: 3.9765 - val_acc: 0.4393\n",
            "Epoch 64/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1709 - acc: 0.9433 - val_loss: 3.9968 - val_acc: 0.4459\n",
            "Epoch 65/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1766 - acc: 0.9425 - val_loss: 4.0690 - val_acc: 0.4170\n",
            "Epoch 66/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1692 - acc: 0.9436 - val_loss: 4.0784 - val_acc: 0.4433\n",
            "Epoch 67/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1720 - acc: 0.9408 - val_loss: 4.0796 - val_acc: 0.4423\n",
            "Epoch 68/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1680 - acc: 0.9436 - val_loss: 4.0177 - val_acc: 0.4444\n",
            "Epoch 69/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1631 - acc: 0.9453 - val_loss: 3.8576 - val_acc: 0.4576\n",
            "Epoch 70/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1589 - acc: 0.9473 - val_loss: 4.0569 - val_acc: 0.4349\n",
            "Epoch 71/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1630 - acc: 0.9438 - val_loss: 4.1160 - val_acc: 0.4331\n",
            "Epoch 72/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1596 - acc: 0.9460 - val_loss: 4.1350 - val_acc: 0.4185\n",
            "Epoch 73/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1585 - acc: 0.9482 - val_loss: 4.0571 - val_acc: 0.4477\n",
            "Epoch 74/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1561 - acc: 0.9472 - val_loss: 4.0722 - val_acc: 0.4349\n",
            "Epoch 75/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1549 - acc: 0.9472 - val_loss: 4.0652 - val_acc: 0.4218\n",
            "Epoch 76/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1525 - acc: 0.9476 - val_loss: 4.2883 - val_acc: 0.4342\n",
            "Epoch 77/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1566 - acc: 0.9463 - val_loss: 4.2822 - val_acc: 0.4401\n",
            "Epoch 78/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1695 - acc: 0.9454 - val_loss: 4.1354 - val_acc: 0.4357\n",
            "Epoch 79/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1614 - acc: 0.9470 - val_loss: 4.0452 - val_acc: 0.4550\n",
            "Epoch 80/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1597 - acc: 0.9460 - val_loss: 4.1791 - val_acc: 0.4236\n",
            "Epoch 81/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1468 - acc: 0.9499 - val_loss: 4.1504 - val_acc: 0.4349\n",
            "Epoch 82/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1523 - acc: 0.9485 - val_loss: 4.1373 - val_acc: 0.4295\n",
            "Epoch 83/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1507 - acc: 0.9463 - val_loss: 4.1893 - val_acc: 0.4251\n",
            "Epoch 84/200\n",
            "10941/10941 [==============================] - 2s 186us/step - loss: 0.1484 - acc: 0.9488 - val_loss: 4.2545 - val_acc: 0.4335\n",
            "Epoch 85/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1526 - acc: 0.9490 - val_loss: 4.1984 - val_acc: 0.4254\n",
            "Epoch 86/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1505 - acc: 0.9495 - val_loss: 4.2099 - val_acc: 0.4536\n",
            "Epoch 87/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.2025 - acc: 0.9346 - val_loss: 4.6641 - val_acc: 0.4130\n",
            "Epoch 88/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.3241 - acc: 0.9041 - val_loss: 4.2045 - val_acc: 0.4192\n",
            "Epoch 89/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1671 - acc: 0.9432 - val_loss: 4.1541 - val_acc: 0.4254\n",
            "Epoch 90/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1513 - acc: 0.9491 - val_loss: 4.2283 - val_acc: 0.4251\n",
            "Epoch 91/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1444 - acc: 0.9505 - val_loss: 4.1876 - val_acc: 0.4353\n",
            "Epoch 92/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1411 - acc: 0.9500 - val_loss: 4.2058 - val_acc: 0.4331\n",
            "Epoch 93/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1382 - acc: 0.9522 - val_loss: 4.1699 - val_acc: 0.4148\n",
            "Epoch 94/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1413 - acc: 0.9506 - val_loss: 4.1482 - val_acc: 0.4225\n",
            "Epoch 95/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1442 - acc: 0.9477 - val_loss: 4.2197 - val_acc: 0.4265\n",
            "Epoch 96/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1479 - acc: 0.9493 - val_loss: 4.1565 - val_acc: 0.4214\n",
            "Epoch 97/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1702 - acc: 0.9418 - val_loss: 4.3191 - val_acc: 0.4203\n",
            "Epoch 98/200\n",
            "10941/10941 [==============================] - 2s 187us/step - loss: 0.1452 - acc: 0.9484 - val_loss: 4.2334 - val_acc: 0.4254\n",
            "Epoch 99/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1449 - acc: 0.9495 - val_loss: 4.1729 - val_acc: 0.4170\n",
            "Epoch 100/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1387 - acc: 0.9506 - val_loss: 4.3326 - val_acc: 0.4284\n",
            "Epoch 101/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1424 - acc: 0.9481 - val_loss: 4.2604 - val_acc: 0.4229\n",
            "Epoch 102/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1373 - acc: 0.9513 - val_loss: 4.2306 - val_acc: 0.4276\n",
            "Epoch 103/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1369 - acc: 0.9520 - val_loss: 4.2667 - val_acc: 0.4390\n",
            "Epoch 104/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1449 - acc: 0.9504 - val_loss: 4.1331 - val_acc: 0.4335\n",
            "Epoch 105/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1400 - acc: 0.9506 - val_loss: 4.1818 - val_acc: 0.4335\n",
            "Epoch 106/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1365 - acc: 0.9498 - val_loss: 4.2101 - val_acc: 0.4203\n",
            "Epoch 107/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1355 - acc: 0.9523 - val_loss: 4.3139 - val_acc: 0.4273\n",
            "Epoch 108/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1458 - acc: 0.9495 - val_loss: 4.2515 - val_acc: 0.4042\n",
            "Epoch 109/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1548 - acc: 0.9460 - val_loss: 4.3006 - val_acc: 0.4141\n",
            "Epoch 110/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1462 - acc: 0.9493 - val_loss: 4.2438 - val_acc: 0.4185\n",
            "Epoch 111/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1392 - acc: 0.9505 - val_loss: 4.3004 - val_acc: 0.4295\n",
            "Epoch 112/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1422 - acc: 0.9508 - val_loss: 4.2396 - val_acc: 0.4196\n",
            "Epoch 113/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1390 - acc: 0.9517 - val_loss: 4.1952 - val_acc: 0.4251\n",
            "Epoch 114/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1439 - acc: 0.9493 - val_loss: 4.2343 - val_acc: 0.4159\n",
            "Epoch 115/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1383 - acc: 0.9507 - val_loss: 4.3414 - val_acc: 0.4313\n",
            "Epoch 116/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1389 - acc: 0.9490 - val_loss: 4.1980 - val_acc: 0.4306\n",
            "Epoch 117/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1387 - acc: 0.9514 - val_loss: 4.1681 - val_acc: 0.4090\n",
            "Epoch 118/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1370 - acc: 0.9504 - val_loss: 4.2746 - val_acc: 0.4364\n",
            "Epoch 119/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1362 - acc: 0.9508 - val_loss: 4.1824 - val_acc: 0.4101\n",
            "Epoch 120/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1345 - acc: 0.9506 - val_loss: 4.2382 - val_acc: 0.4200\n",
            "Epoch 121/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1344 - acc: 0.9518 - val_loss: 4.2030 - val_acc: 0.4287\n",
            "Epoch 122/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1349 - acc: 0.9506 - val_loss: 4.2583 - val_acc: 0.4298\n",
            "Epoch 123/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1348 - acc: 0.9516 - val_loss: 4.2764 - val_acc: 0.4371\n",
            "Epoch 124/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1304 - acc: 0.9532 - val_loss: 4.1954 - val_acc: 0.4167\n",
            "Epoch 125/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1340 - acc: 0.9516 - val_loss: 4.2298 - val_acc: 0.4320\n",
            "Epoch 126/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1363 - acc: 0.9517 - val_loss: 4.2785 - val_acc: 0.4364\n",
            "Epoch 127/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1343 - acc: 0.9515 - val_loss: 4.1345 - val_acc: 0.4306\n",
            "Epoch 128/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1364 - acc: 0.9505 - val_loss: 4.2815 - val_acc: 0.4203\n",
            "Epoch 129/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1331 - acc: 0.9520 - val_loss: 4.2016 - val_acc: 0.4265\n",
            "Epoch 130/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1320 - acc: 0.9497 - val_loss: 4.3003 - val_acc: 0.4159\n",
            "Epoch 131/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1342 - acc: 0.9519 - val_loss: 4.1266 - val_acc: 0.4477\n",
            "Epoch 132/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1520 - acc: 0.9471 - val_loss: 4.2574 - val_acc: 0.4123\n",
            "Epoch 133/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1440 - acc: 0.9504 - val_loss: 4.2918 - val_acc: 0.4393\n",
            "Epoch 134/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1438 - acc: 0.9474 - val_loss: 4.3851 - val_acc: 0.4295\n",
            "Epoch 135/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1427 - acc: 0.9505 - val_loss: 4.2454 - val_acc: 0.4419\n",
            "Epoch 136/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1378 - acc: 0.9503 - val_loss: 4.3353 - val_acc: 0.4364\n",
            "Epoch 137/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1338 - acc: 0.9511 - val_loss: 4.2781 - val_acc: 0.4284\n",
            "Epoch 138/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1343 - acc: 0.9513 - val_loss: 4.3748 - val_acc: 0.4265\n",
            "Epoch 139/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1329 - acc: 0.9517 - val_loss: 4.3257 - val_acc: 0.4232\n",
            "Epoch 140/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1314 - acc: 0.9511 - val_loss: 4.1947 - val_acc: 0.4404\n",
            "Epoch 141/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1346 - acc: 0.9525 - val_loss: 4.2756 - val_acc: 0.4393\n",
            "Epoch 142/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1371 - acc: 0.9506 - val_loss: 4.2967 - val_acc: 0.4379\n",
            "Epoch 143/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1319 - acc: 0.9527 - val_loss: 4.3322 - val_acc: 0.4262\n",
            "Epoch 144/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.5832 - acc: 0.8431 - val_loss: 3.6630 - val_acc: 0.3984\n",
            "Epoch 145/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.2620 - acc: 0.9163 - val_loss: 3.6332 - val_acc: 0.4433\n",
            "Epoch 146/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1442 - acc: 0.9506 - val_loss: 3.7639 - val_acc: 0.4529\n",
            "Epoch 147/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1327 - acc: 0.9522 - val_loss: 3.7743 - val_acc: 0.4397\n",
            "Epoch 148/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1306 - acc: 0.9529 - val_loss: 3.8056 - val_acc: 0.4488\n",
            "Epoch 149/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1285 - acc: 0.9527 - val_loss: 3.8650 - val_acc: 0.4393\n",
            "Epoch 150/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1282 - acc: 0.9527 - val_loss: 3.8602 - val_acc: 0.4357\n",
            "Epoch 151/200\n",
            "10941/10941 [==============================] - 2s 186us/step - loss: 0.1289 - acc: 0.9523 - val_loss: 3.9886 - val_acc: 0.4433\n",
            "Epoch 152/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1272 - acc: 0.9535 - val_loss: 3.9654 - val_acc: 0.4444\n",
            "Epoch 153/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1312 - acc: 0.9513 - val_loss: 3.9377 - val_acc: 0.4444\n",
            "Epoch 154/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1269 - acc: 0.9534 - val_loss: 3.9667 - val_acc: 0.4496\n",
            "Epoch 155/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1267 - acc: 0.9538 - val_loss: 3.9629 - val_acc: 0.4496\n",
            "Epoch 156/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1290 - acc: 0.9525 - val_loss: 3.9908 - val_acc: 0.4423\n",
            "Epoch 157/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1302 - acc: 0.9537 - val_loss: 4.0260 - val_acc: 0.4346\n",
            "Epoch 158/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1386 - acc: 0.9494 - val_loss: 3.9736 - val_acc: 0.4375\n",
            "Epoch 159/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1288 - acc: 0.9539 - val_loss: 4.0406 - val_acc: 0.4455\n",
            "Epoch 160/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1262 - acc: 0.9517 - val_loss: 4.0192 - val_acc: 0.4393\n",
            "Epoch 161/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1257 - acc: 0.9531 - val_loss: 3.9970 - val_acc: 0.4496\n",
            "Epoch 162/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1251 - acc: 0.9529 - val_loss: 4.0240 - val_acc: 0.4430\n",
            "Epoch 163/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1246 - acc: 0.9533 - val_loss: 4.0264 - val_acc: 0.4518\n",
            "Epoch 164/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1253 - acc: 0.9535 - val_loss: 4.0480 - val_acc: 0.4459\n",
            "Epoch 165/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1268 - acc: 0.9518 - val_loss: 4.0622 - val_acc: 0.4412\n",
            "Epoch 166/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1289 - acc: 0.9529 - val_loss: 4.0115 - val_acc: 0.4430\n",
            "Epoch 167/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1284 - acc: 0.9515 - val_loss: 3.9744 - val_acc: 0.4349\n",
            "Epoch 168/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1262 - acc: 0.9535 - val_loss: 4.0543 - val_acc: 0.4353\n",
            "Epoch 169/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1237 - acc: 0.9536 - val_loss: 4.0205 - val_acc: 0.4313\n",
            "Epoch 170/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1250 - acc: 0.9518 - val_loss: 4.0398 - val_acc: 0.4437\n",
            "Epoch 171/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1261 - acc: 0.9526 - val_loss: 4.0124 - val_acc: 0.4430\n",
            "Epoch 172/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1247 - acc: 0.9538 - val_loss: 4.0880 - val_acc: 0.4444\n",
            "Epoch 173/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1305 - acc: 0.9527 - val_loss: 3.9941 - val_acc: 0.4287\n",
            "Epoch 174/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1264 - acc: 0.9548 - val_loss: 4.0404 - val_acc: 0.4521\n",
            "Epoch 175/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1253 - acc: 0.9538 - val_loss: 4.0470 - val_acc: 0.4444\n",
            "Epoch 176/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1226 - acc: 0.9539 - val_loss: 4.0501 - val_acc: 0.4459\n",
            "Epoch 177/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1251 - acc: 0.9523 - val_loss: 3.9421 - val_acc: 0.4466\n",
            "Epoch 178/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1262 - acc: 0.9539 - val_loss: 3.9886 - val_acc: 0.4327\n",
            "Epoch 179/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1265 - acc: 0.9511 - val_loss: 4.0497 - val_acc: 0.4327\n",
            "Epoch 180/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1299 - acc: 0.9522 - val_loss: 4.0305 - val_acc: 0.4466\n",
            "Epoch 181/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1255 - acc: 0.9538 - val_loss: 4.0445 - val_acc: 0.4360\n",
            "Epoch 182/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1250 - acc: 0.9537 - val_loss: 4.0535 - val_acc: 0.4360\n",
            "Epoch 183/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1240 - acc: 0.9544 - val_loss: 4.0195 - val_acc: 0.4346\n",
            "Epoch 184/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1325 - acc: 0.9507 - val_loss: 4.0202 - val_acc: 0.4295\n",
            "Epoch 185/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1299 - acc: 0.9522 - val_loss: 4.0901 - val_acc: 0.4539\n",
            "Epoch 186/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1278 - acc: 0.9526 - val_loss: 4.0026 - val_acc: 0.4357\n",
            "Epoch 187/200\n",
            "10941/10941 [==============================] - 2s 181us/step - loss: 0.1249 - acc: 0.9540 - val_loss: 4.1036 - val_acc: 0.4547\n",
            "Epoch 188/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1237 - acc: 0.9555 - val_loss: 4.0799 - val_acc: 0.4331\n",
            "Epoch 189/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1271 - acc: 0.9535 - val_loss: 4.0687 - val_acc: 0.4346\n",
            "Epoch 190/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1269 - acc: 0.9541 - val_loss: 4.0019 - val_acc: 0.4371\n",
            "Epoch 191/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1281 - acc: 0.9513 - val_loss: 4.0063 - val_acc: 0.4492\n",
            "Epoch 192/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1266 - acc: 0.9519 - val_loss: 4.0637 - val_acc: 0.4317\n",
            "Epoch 193/200\n",
            "10941/10941 [==============================] - 2s 182us/step - loss: 0.1265 - acc: 0.9534 - val_loss: 4.0666 - val_acc: 0.4324\n",
            "Epoch 194/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1246 - acc: 0.9537 - val_loss: 4.0549 - val_acc: 0.4364\n",
            "Epoch 195/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1247 - acc: 0.9542 - val_loss: 4.0494 - val_acc: 0.4353\n",
            "Epoch 196/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1226 - acc: 0.9542 - val_loss: 4.0329 - val_acc: 0.4437\n",
            "Epoch 197/200\n",
            "10941/10941 [==============================] - 2s 185us/step - loss: 0.1256 - acc: 0.9518 - val_loss: 4.0116 - val_acc: 0.4529\n",
            "Epoch 198/200\n",
            "10941/10941 [==============================] - 2s 186us/step - loss: 0.1248 - acc: 0.9529 - val_loss: 4.1273 - val_acc: 0.4459\n",
            "Epoch 199/200\n",
            "10941/10941 [==============================] - 2s 184us/step - loss: 0.1243 - acc: 0.9532 - val_loss: 4.0356 - val_acc: 0.4349\n",
            "Epoch 200/200\n",
            "10941/10941 [==============================] - 2s 183us/step - loss: 0.1252 - acc: 0.9527 - val_loss: 4.0113 - val_acc: 0.4284\n",
            "{0: 0.0, 1: Fraction(1, 12), 2: Fraction(1, 6), 3: 0.25, 4: Fraction(1, 3), 5: Fraction(5, 12), 6: 0.5, 7: Fraction(2, 3), 8: 0.75, 9: 1.0, 10: Fraction(7, 6), 11: 1.25, 12: Fraction(4, 3), 13: 1.5, 14: Fraction(5, 3), 15: 1.75, 16: 2.0, 17: 2.25, 18: 2.5, 19: 3.0, 20: 3.25, 21: 3.5, 22: 4.0, 23: Fraction(25, 6), 24: 4.25, 25: 4.5, 26: 4.75, 27: 5.0, 28: 5.25, 29: 5.75, 30: 6.0, 31: 6.5, 32: 7.0, 33: Fraction(23, 3), 34: 8.0, 35: 8.5, 36: 9.0, 37: 14.5, 38: 16.0, 39: 18.0, 40: 27.5, 41: 32.0, 42: 32.5, 43: 36.0, 44: 36.25, 45: 39.5, 46: 47.5, 47: 48.0, 48: 61.0, 49: 64.25, 50: 68.0, 51: 72.25, 52: 72.5, 53: 80.0, 54: 84.25, 55: Fraction(553, 6), 56: 94.5, 57: 128.25, 58: 161.0, 59: 176.0, 60: 200.5}\n",
            " Predicting.  Duration:  399\n",
            "\n",
            "Running notes training on d:0 s:256\n",
            "Train on 9113 samples, validate on 2279 samples\n",
            "Epoch 1/200\n",
            "9113/9113 [==============================] - 16s 2ms/step - loss: 3.1102 - acc: 0.3799 - val_loss: 3.3242 - val_acc: 0.3414\n",
            "Epoch 2/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 3.0297 - acc: 0.3858 - val_loss: 3.3002 - val_acc: 0.3414\n",
            "Epoch 3/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 3.0232 - acc: 0.3858 - val_loss: 3.3009 - val_acc: 0.3414\n",
            "Epoch 4/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 3.0159 - acc: 0.3858 - val_loss: 3.3066 - val_acc: 0.3414\n",
            "Epoch 5/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.9797 - acc: 0.3855 - val_loss: 3.3876 - val_acc: 0.3414\n",
            "Epoch 6/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.9357 - acc: 0.3856 - val_loss: 3.4716 - val_acc: 0.3414\n",
            "Epoch 7/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 2.8816 - acc: 0.3854 - val_loss: 3.5198 - val_acc: 0.3330\n",
            "Epoch 8/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 2.7761 - acc: 0.3871 - val_loss: 3.6383 - val_acc: 0.3124\n",
            "Epoch 9/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 2.5798 - acc: 0.3991 - val_loss: 3.5274 - val_acc: 0.3076\n",
            "Epoch 10/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 2.3002 - acc: 0.4252 - val_loss: 3.7321 - val_acc: 0.3229\n",
            "Epoch 11/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 1.9956 - acc: 0.4695 - val_loss: 3.9429 - val_acc: 0.2971\n",
            "Epoch 12/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 1.6989 - acc: 0.5163 - val_loss: 3.9745 - val_acc: 0.2751\n",
            "Epoch 13/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 1.5048 - acc: 0.5580 - val_loss: 4.3143 - val_acc: 0.2857\n",
            "Epoch 14/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 1.2276 - acc: 0.6226 - val_loss: 4.5771 - val_acc: 0.2694\n",
            "Epoch 15/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 1.0331 - acc: 0.6734 - val_loss: 4.7789 - val_acc: 0.2554\n",
            "Epoch 16/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.8687 - acc: 0.7278 - val_loss: 5.2310 - val_acc: 0.2462\n",
            "Epoch 17/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.7186 - acc: 0.7698 - val_loss: 5.4272 - val_acc: 0.2431\n",
            "Epoch 18/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.5997 - acc: 0.8137 - val_loss: 5.4752 - val_acc: 0.2448\n",
            "Epoch 19/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.4827 - acc: 0.8527 - val_loss: 5.8418 - val_acc: 0.2247\n",
            "Epoch 20/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.4040 - acc: 0.8772 - val_loss: 6.2107 - val_acc: 0.2058\n",
            "Epoch 21/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.3239 - acc: 0.9057 - val_loss: 6.4219 - val_acc: 0.2247\n",
            "Epoch 22/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.2738 - acc: 0.9247 - val_loss: 6.5408 - val_acc: 0.2317\n",
            "Epoch 23/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.2077 - acc: 0.9411 - val_loss: 6.7378 - val_acc: 0.2238\n",
            "Epoch 24/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.3221 - acc: 0.9018 - val_loss: 6.7854 - val_acc: 0.2185\n",
            "Epoch 25/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1909 - acc: 0.9471 - val_loss: 6.8532 - val_acc: 0.2400\n",
            "Epoch 26/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1377 - acc: 0.9651 - val_loss: 6.9794 - val_acc: 0.2269\n",
            "Epoch 27/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.1165 - acc: 0.9704 - val_loss: 7.2222 - val_acc: 0.2352\n",
            "Epoch 28/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0909 - acc: 0.9784 - val_loss: 7.2572 - val_acc: 0.2286\n",
            "Epoch 29/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0811 - acc: 0.9787 - val_loss: 7.4165 - val_acc: 0.2321\n",
            "Epoch 30/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0739 - acc: 0.9808 - val_loss: 7.4534 - val_acc: 0.2488\n",
            "Epoch 31/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0876 - acc: 0.9774 - val_loss: 7.5761 - val_acc: 0.2269\n",
            "Epoch 32/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0690 - acc: 0.9824 - val_loss: 7.6419 - val_acc: 0.2470\n",
            "Epoch 33/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0677 - acc: 0.9834 - val_loss: 7.7606 - val_acc: 0.2211\n",
            "Epoch 34/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0773 - acc: 0.9796 - val_loss: 7.7009 - val_acc: 0.2365\n",
            "Epoch 35/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0599 - acc: 0.9832 - val_loss: 7.7379 - val_acc: 0.2133\n",
            "Epoch 36/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0596 - acc: 0.9851 - val_loss: 7.8468 - val_acc: 0.2365\n",
            "Epoch 37/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0802 - acc: 0.9767 - val_loss: 8.0280 - val_acc: 0.2260\n",
            "Epoch 38/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.1086 - acc: 0.9723 - val_loss: 7.8303 - val_acc: 0.2089\n",
            "Epoch 39/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.1088 - acc: 0.9700 - val_loss: 8.0004 - val_acc: 0.2339\n",
            "Epoch 40/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0630 - acc: 0.9834 - val_loss: 7.9671 - val_acc: 0.2400\n",
            "Epoch 41/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0537 - acc: 0.9869 - val_loss: 7.9408 - val_acc: 0.2312\n",
            "Epoch 42/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0469 - acc: 0.9888 - val_loss: 8.1010 - val_acc: 0.2405\n",
            "Epoch 43/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0440 - acc: 0.9885 - val_loss: 8.0777 - val_acc: 0.2431\n",
            "Epoch 44/200\n",
            "9113/9113 [==============================] - 16s 2ms/step - loss: 0.0372 - acc: 0.9905 - val_loss: 8.1053 - val_acc: 0.2444\n",
            "Epoch 45/200\n",
            "9113/9113 [==============================] - 19s 2ms/step - loss: 0.0397 - acc: 0.9890 - val_loss: 8.1291 - val_acc: 0.2400\n",
            "Epoch 46/200\n",
            "9113/9113 [==============================] - 12s 1ms/step - loss: 0.0359 - acc: 0.9894 - val_loss: 8.2171 - val_acc: 0.2317\n",
            "Epoch 47/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0368 - acc: 0.9896 - val_loss: 8.1974 - val_acc: 0.2334\n",
            "Epoch 48/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0418 - acc: 0.9890 - val_loss: 8.1075 - val_acc: 0.2330\n",
            "Epoch 49/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0472 - acc: 0.9871 - val_loss: 8.1462 - val_acc: 0.2339\n",
            "Epoch 50/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0489 - acc: 0.9866 - val_loss: 8.2262 - val_acc: 0.2150\n",
            "Epoch 51/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0717 - acc: 0.9807 - val_loss: 8.1662 - val_acc: 0.2225\n",
            "Epoch 52/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.2284 - acc: 0.9320 - val_loss: 8.0344 - val_acc: 0.2159\n",
            "Epoch 53/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1872 - acc: 0.9396 - val_loss: 8.1317 - val_acc: 0.2247\n",
            "Epoch 54/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0998 - acc: 0.9729 - val_loss: 8.1319 - val_acc: 0.2233\n",
            "Epoch 55/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0524 - acc: 0.9864 - val_loss: 8.1368 - val_acc: 0.2286\n",
            "Epoch 56/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0402 - acc: 0.9885 - val_loss: 8.1345 - val_acc: 0.2356\n",
            "Epoch 57/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0321 - acc: 0.9908 - val_loss: 8.1551 - val_acc: 0.2387\n",
            "Epoch 58/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0325 - acc: 0.9905 - val_loss: 8.0914 - val_acc: 0.2400\n",
            "Epoch 59/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0306 - acc: 0.9901 - val_loss: 8.1816 - val_acc: 0.2405\n",
            "Epoch 60/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0288 - acc: 0.9906 - val_loss: 8.1322 - val_acc: 0.2391\n",
            "Epoch 61/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0338 - acc: 0.9905 - val_loss: 8.1635 - val_acc: 0.2505\n",
            "Epoch 62/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0338 - acc: 0.9899 - val_loss: 8.0910 - val_acc: 0.2470\n",
            "Epoch 63/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0300 - acc: 0.9913 - val_loss: 8.1917 - val_acc: 0.2369\n",
            "Epoch 64/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0299 - acc: 0.9910 - val_loss: 8.2262 - val_acc: 0.2102\n",
            "Epoch 65/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0288 - acc: 0.9908 - val_loss: 8.1707 - val_acc: 0.2418\n",
            "Epoch 66/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0300 - acc: 0.9906 - val_loss: 8.1285 - val_acc: 0.2435\n",
            "Epoch 67/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0257 - acc: 0.9911 - val_loss: 8.1999 - val_acc: 0.2576\n",
            "Epoch 68/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0266 - acc: 0.9916 - val_loss: 8.2001 - val_acc: 0.2312\n",
            "Epoch 69/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0269 - acc: 0.9909 - val_loss: 8.1824 - val_acc: 0.2453\n",
            "Epoch 70/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0278 - acc: 0.9910 - val_loss: 8.3081 - val_acc: 0.2356\n",
            "Epoch 71/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0302 - acc: 0.9910 - val_loss: 8.2408 - val_acc: 0.2462\n",
            "Epoch 72/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0268 - acc: 0.9912 - val_loss: 8.2420 - val_acc: 0.2484\n",
            "Epoch 73/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0279 - acc: 0.9911 - val_loss: 8.1910 - val_acc: 0.2413\n",
            "Epoch 74/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0317 - acc: 0.9901 - val_loss: 8.2583 - val_acc: 0.2361\n",
            "Epoch 75/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0361 - acc: 0.9889 - val_loss: 8.1412 - val_acc: 0.2440\n",
            "Epoch 76/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0432 - acc: 0.9867 - val_loss: 8.2180 - val_acc: 0.2317\n",
            "Epoch 77/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0704 - acc: 0.9793 - val_loss: 8.3231 - val_acc: 0.2409\n",
            "Epoch 78/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0927 - acc: 0.9740 - val_loss: 8.1716 - val_acc: 0.2273\n",
            "Epoch 79/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1485 - acc: 0.9526 - val_loss: 8.1336 - val_acc: 0.2277\n",
            "Epoch 80/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1369 - acc: 0.9591 - val_loss: 8.2586 - val_acc: 0.2295\n",
            "Epoch 81/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0951 - acc: 0.9722 - val_loss: 8.0754 - val_acc: 0.2282\n",
            "Epoch 82/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0512 - acc: 0.9835 - val_loss: 8.1095 - val_acc: 0.2448\n",
            "Epoch 83/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0339 - acc: 0.9895 - val_loss: 8.1687 - val_acc: 0.2492\n",
            "Epoch 84/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0265 - acc: 0.9908 - val_loss: 8.1949 - val_acc: 0.2435\n",
            "Epoch 85/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0247 - acc: 0.9910 - val_loss: 8.1680 - val_acc: 0.2448\n",
            "Epoch 86/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0261 - acc: 0.9912 - val_loss: 8.1677 - val_acc: 0.2615\n",
            "Epoch 87/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0236 - acc: 0.9921 - val_loss: 8.1336 - val_acc: 0.2462\n",
            "Epoch 88/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0249 - acc: 0.9912 - val_loss: 8.1375 - val_acc: 0.2457\n",
            "Epoch 89/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0296 - acc: 0.9914 - val_loss: 8.1399 - val_acc: 0.2554\n",
            "Epoch 90/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0250 - acc: 0.9910 - val_loss: 8.2373 - val_acc: 0.2580\n",
            "Epoch 91/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0236 - acc: 0.9913 - val_loss: 8.2798 - val_acc: 0.2488\n",
            "Epoch 92/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0267 - acc: 0.9911 - val_loss: 8.1585 - val_acc: 0.2720\n",
            "Epoch 93/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0251 - acc: 0.9922 - val_loss: 8.2127 - val_acc: 0.2633\n",
            "Epoch 94/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0245 - acc: 0.9921 - val_loss: 8.1209 - val_acc: 0.2475\n",
            "Epoch 95/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0264 - acc: 0.9923 - val_loss: 8.1825 - val_acc: 0.2646\n",
            "Epoch 96/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0242 - acc: 0.9914 - val_loss: 8.1505 - val_acc: 0.2703\n",
            "Epoch 97/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0236 - acc: 0.9911 - val_loss: 8.1634 - val_acc: 0.2470\n",
            "Epoch 98/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0799 - acc: 0.9764 - val_loss: 8.2013 - val_acc: 0.2405\n",
            "Epoch 99/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1184 - acc: 0.9644 - val_loss: 8.3919 - val_acc: 0.2238\n",
            "Epoch 100/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0981 - acc: 0.9719 - val_loss: 8.4068 - val_acc: 0.2453\n",
            "Epoch 101/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0571 - acc: 0.9847 - val_loss: 8.2766 - val_acc: 0.2584\n",
            "Epoch 102/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0341 - acc: 0.9896 - val_loss: 8.2179 - val_acc: 0.2571\n",
            "Epoch 103/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0330 - acc: 0.9902 - val_loss: 8.4012 - val_acc: 0.2602\n",
            "Epoch 104/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0260 - acc: 0.9913 - val_loss: 8.3945 - val_acc: 0.2589\n",
            "Epoch 105/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0258 - acc: 0.9921 - val_loss: 8.4065 - val_acc: 0.2488\n",
            "Epoch 106/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0230 - acc: 0.9918 - val_loss: 8.4165 - val_acc: 0.2681\n",
            "Epoch 107/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0382 - acc: 0.9878 - val_loss: 8.3293 - val_acc: 0.2492\n",
            "Epoch 108/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0260 - acc: 0.9913 - val_loss: 8.2609 - val_acc: 0.2620\n",
            "Epoch 109/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0264 - acc: 0.9900 - val_loss: 8.3510 - val_acc: 0.2584\n",
            "Epoch 110/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0254 - acc: 0.9913 - val_loss: 8.4187 - val_acc: 0.2514\n",
            "Epoch 111/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0300 - acc: 0.9899 - val_loss: 8.3283 - val_acc: 0.2532\n",
            "Epoch 112/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0314 - acc: 0.9897 - val_loss: 8.2814 - val_acc: 0.2427\n",
            "Epoch 113/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0291 - acc: 0.9908 - val_loss: 8.2258 - val_acc: 0.2681\n",
            "Epoch 114/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0254 - acc: 0.9921 - val_loss: 8.2097 - val_acc: 0.2642\n",
            "Epoch 115/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0253 - acc: 0.9912 - val_loss: 8.2430 - val_acc: 0.2821\n",
            "Epoch 116/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0245 - acc: 0.9917 - val_loss: 8.2058 - val_acc: 0.2611\n",
            "Epoch 117/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0244 - acc: 0.9921 - val_loss: 8.1962 - val_acc: 0.2808\n",
            "Epoch 118/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0224 - acc: 0.9918 - val_loss: 8.2219 - val_acc: 0.2549\n",
            "Epoch 119/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0209 - acc: 0.9921 - val_loss: 8.3136 - val_acc: 0.2453\n",
            "Epoch 120/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0378 - acc: 0.9865 - val_loss: 8.2466 - val_acc: 0.2545\n",
            "Epoch 121/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0569 - acc: 0.9839 - val_loss: 8.4715 - val_acc: 0.2247\n",
            "Epoch 122/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0572 - acc: 0.9841 - val_loss: 8.5186 - val_acc: 0.2286\n",
            "Epoch 123/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0425 - acc: 0.9863 - val_loss: 8.4186 - val_acc: 0.2501\n",
            "Epoch 124/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0374 - acc: 0.9873 - val_loss: 8.3377 - val_acc: 0.2466\n",
            "Epoch 125/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0420 - acc: 0.9878 - val_loss: 8.3468 - val_acc: 0.2453\n",
            "Epoch 126/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0296 - acc: 0.9912 - val_loss: 8.4590 - val_acc: 0.2606\n",
            "Epoch 127/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0244 - acc: 0.9916 - val_loss: 8.3033 - val_acc: 0.2470\n",
            "Epoch 128/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0436 - acc: 0.9871 - val_loss: 8.2995 - val_acc: 0.2457\n",
            "Epoch 129/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0908 - acc: 0.9736 - val_loss: 8.4933 - val_acc: 0.2299\n",
            "Epoch 130/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0646 - acc: 0.9830 - val_loss: 8.5270 - val_acc: 0.2378\n",
            "Epoch 131/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0354 - acc: 0.9889 - val_loss: 8.5247 - val_acc: 0.2484\n",
            "Epoch 132/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0278 - acc: 0.9917 - val_loss: 8.4759 - val_acc: 0.2440\n",
            "Epoch 133/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0222 - acc: 0.9918 - val_loss: 8.4555 - val_acc: 0.2699\n",
            "Epoch 134/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0242 - acc: 0.9916 - val_loss: 8.4558 - val_acc: 0.2505\n",
            "Epoch 135/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0248 - acc: 0.9925 - val_loss: 8.5605 - val_acc: 0.2606\n",
            "Epoch 136/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0205 - acc: 0.9918 - val_loss: 8.5446 - val_acc: 0.2422\n",
            "Epoch 137/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0210 - acc: 0.9914 - val_loss: 8.5179 - val_acc: 0.2563\n",
            "Epoch 138/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0203 - acc: 0.9922 - val_loss: 8.5477 - val_acc: 0.2479\n",
            "Epoch 139/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0377 - acc: 0.9876 - val_loss: 8.3982 - val_acc: 0.2282\n",
            "Epoch 140/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0480 - acc: 0.9863 - val_loss: 8.4965 - val_acc: 0.2260\n",
            "Epoch 141/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0572 - acc: 0.9838 - val_loss: 8.6278 - val_acc: 0.2356\n",
            "Epoch 142/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0390 - acc: 0.9878 - val_loss: 8.5704 - val_acc: 0.2462\n",
            "Epoch 143/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0487 - acc: 0.9871 - val_loss: 8.6223 - val_acc: 0.2374\n",
            "Epoch 144/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0446 - acc: 0.9858 - val_loss: 8.3987 - val_acc: 0.2650\n",
            "Epoch 145/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0337 - acc: 0.9887 - val_loss: 8.5241 - val_acc: 0.2606\n",
            "Epoch 146/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0286 - acc: 0.9909 - val_loss: 8.5627 - val_acc: 0.2593\n",
            "Epoch 147/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0216 - acc: 0.9920 - val_loss: 8.5107 - val_acc: 0.2422\n",
            "Epoch 148/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0208 - acc: 0.9923 - val_loss: 8.4074 - val_acc: 0.2510\n",
            "Epoch 149/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0204 - acc: 0.9921 - val_loss: 8.4771 - val_acc: 0.2567\n",
            "Epoch 150/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0204 - acc: 0.9924 - val_loss: 8.5218 - val_acc: 0.2554\n",
            "Epoch 151/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0188 - acc: 0.9917 - val_loss: 8.5161 - val_acc: 0.2580\n",
            "Epoch 152/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0195 - acc: 0.9925 - val_loss: 8.5067 - val_acc: 0.2558\n",
            "Epoch 153/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0195 - acc: 0.9919 - val_loss: 8.5408 - val_acc: 0.2672\n",
            "Epoch 154/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0211 - acc: 0.9920 - val_loss: 8.4351 - val_acc: 0.2536\n",
            "Epoch 155/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0201 - acc: 0.9923 - val_loss: 8.4780 - val_acc: 0.2571\n",
            "Epoch 156/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0187 - acc: 0.9923 - val_loss: 8.5621 - val_acc: 0.2668\n",
            "Epoch 157/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0205 - acc: 0.9910 - val_loss: 8.5124 - val_acc: 0.2589\n",
            "Epoch 158/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0197 - acc: 0.9919 - val_loss: 8.4718 - val_acc: 0.2606\n",
            "Epoch 159/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0186 - acc: 0.9924 - val_loss: 8.4484 - val_acc: 0.2554\n",
            "Epoch 160/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0219 - acc: 0.9914 - val_loss: 8.4383 - val_acc: 0.2532\n",
            "Epoch 161/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0215 - acc: 0.9912 - val_loss: 8.4659 - val_acc: 0.2545\n",
            "Epoch 162/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0222 - acc: 0.9914 - val_loss: 8.3944 - val_acc: 0.2545\n",
            "Epoch 163/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0221 - acc: 0.9921 - val_loss: 8.4402 - val_acc: 0.2598\n",
            "Epoch 164/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0209 - acc: 0.9916 - val_loss: 8.3956 - val_acc: 0.2497\n",
            "Epoch 165/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0187 - acc: 0.9914 - val_loss: 8.4656 - val_acc: 0.2549\n",
            "Epoch 166/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0190 - acc: 0.9922 - val_loss: 8.2988 - val_acc: 0.2738\n",
            "Epoch 167/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0205 - acc: 0.9914 - val_loss: 8.3173 - val_acc: 0.2541\n",
            "Epoch 168/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0211 - acc: 0.9920 - val_loss: 8.3309 - val_acc: 0.2558\n",
            "Epoch 169/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0200 - acc: 0.9924 - val_loss: 8.3792 - val_acc: 0.2558\n",
            "Epoch 170/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0204 - acc: 0.9908 - val_loss: 8.3618 - val_acc: 0.2773\n",
            "Epoch 171/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0197 - acc: 0.9909 - val_loss: 8.3177 - val_acc: 0.2580\n",
            "Epoch 172/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0176 - acc: 0.9926 - val_loss: 8.4003 - val_acc: 0.2694\n",
            "Epoch 173/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0181 - acc: 0.9918 - val_loss: 8.3497 - val_acc: 0.2541\n",
            "Epoch 174/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0186 - acc: 0.9921 - val_loss: 8.3213 - val_acc: 0.2655\n",
            "Epoch 175/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0230 - acc: 0.9917 - val_loss: 8.4327 - val_acc: 0.2620\n",
            "Epoch 176/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0350 - acc: 0.9885 - val_loss: 8.4962 - val_acc: 0.2058\n",
            "Epoch 177/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1497 - acc: 0.9541 - val_loss: 8.4602 - val_acc: 0.1825\n",
            "Epoch 178/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.1788 - acc: 0.9470 - val_loss: 8.5624 - val_acc: 0.1865\n",
            "Epoch 179/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0901 - acc: 0.9740 - val_loss: 8.5436 - val_acc: 0.2308\n",
            "Epoch 180/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0444 - acc: 0.9862 - val_loss: 8.4608 - val_acc: 0.2185\n",
            "Epoch 181/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0363 - acc: 0.9894 - val_loss: 8.5357 - val_acc: 0.2378\n",
            "Epoch 182/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0266 - acc: 0.9916 - val_loss: 8.4969 - val_acc: 0.2369\n",
            "Epoch 183/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0251 - acc: 0.9922 - val_loss: 8.5136 - val_acc: 0.2339\n",
            "Epoch 184/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0251 - acc: 0.9920 - val_loss: 8.4230 - val_acc: 0.2435\n",
            "Epoch 185/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0248 - acc: 0.9916 - val_loss: 8.5412 - val_acc: 0.2339\n",
            "Epoch 186/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0235 - acc: 0.9924 - val_loss: 8.5132 - val_acc: 0.2405\n",
            "Epoch 187/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0249 - acc: 0.9910 - val_loss: 8.5762 - val_acc: 0.2435\n",
            "Epoch 188/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0235 - acc: 0.9919 - val_loss: 8.5072 - val_acc: 0.2238\n",
            "Epoch 189/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0240 - acc: 0.9924 - val_loss: 8.5461 - val_acc: 0.2251\n",
            "Epoch 190/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0241 - acc: 0.9920 - val_loss: 8.5277 - val_acc: 0.2365\n",
            "Epoch 191/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0247 - acc: 0.9914 - val_loss: 8.5252 - val_acc: 0.2405\n",
            "Epoch 192/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0235 - acc: 0.9921 - val_loss: 8.4976 - val_acc: 0.2361\n",
            "Epoch 193/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0240 - acc: 0.9914 - val_loss: 8.5419 - val_acc: 0.2264\n",
            "Epoch 194/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0256 - acc: 0.9911 - val_loss: 8.5239 - val_acc: 0.2339\n",
            "Epoch 195/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0238 - acc: 0.9917 - val_loss: 8.5124 - val_acc: 0.2418\n",
            "Epoch 196/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0241 - acc: 0.9918 - val_loss: 8.5371 - val_acc: 0.2576\n",
            "Epoch 197/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0254 - acc: 0.9913 - val_loss: 8.4357 - val_acc: 0.2334\n",
            "Epoch 198/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0242 - acc: 0.9921 - val_loss: 8.4955 - val_acc: 0.2444\n",
            "Epoch 199/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0249 - acc: 0.9918 - val_loss: 8.4863 - val_acc: 0.2295\n",
            "Epoch 200/200\n",
            "9113/9113 [==============================] - 13s 1ms/step - loss: 0.0246 - acc: 0.9912 - val_loss: 8.4845 - val_acc: 0.2330\n",
            "{0: 'A1', 1: 'A2', 2: 'A3', 3: 'A4', 4: 'A5', 5: 'B-1', 6: 'B-2', 7: 'B-3', 8: 'B-4', 9: 'B-5', 10: 'B1', 11: 'B2', 12: 'B3', 13: 'B4', 14: 'B5', 15: 'C#1', 16: 'C#2', 17: 'C#3', 18: 'C#4', 19: 'C#5', 20: 'C#6', 21: 'C2', 22: 'C3', 23: 'C4', 24: 'C5', 25: 'C6', 26: 'D1', 27: 'D2', 28: 'D3', 29: 'D4', 30: 'D5', 31: 'D6', 32: 'E-2', 33: 'E-3', 34: 'E-4', 35: 'E-5', 36: 'E-6', 37: 'E1', 38: 'E2', 39: 'E3', 40: 'E4', 41: 'E5', 42: 'E6', 43: 'F#1', 44: 'F#2', 45: 'F#3', 46: 'F#4', 47: 'F#5', 48: 'F#6', 49: 'F1', 50: 'F2', 51: 'F3', 52: 'F4', 53: 'F5', 54: 'G#1', 55: 'G#2', 56: 'G#3', 57: 'G#4', 58: 'G#5', 59: 'G1', 60: 'G2', 61: 'G3', 62: 'G4', 63: 'G5', 64: 'R'}\n",
            " Predicting.  Note:  399\n",
            "Wrote midi...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}