{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Attention (EDM)-official - Austin.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "RWyZf8Ojbc4i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# INSTRUCTIONS\n",
        "\n",
        "Workflow: Run each cell in order. Put or extract data into relevant folders as defined in section 2. Make remaining folders in drive.  **Cells denoted with * may require extra action.**\n",
        "\n",
        "\n",
        "```\n",
        "Folder Structure Suggested for Section 2:\n",
        "drive/\n",
        "    train_data/ <- input midis go here\n",
        "    train_output/\n",
        "        graphs/ <- train/val accuracy plots go here\n",
        "        intermed/ <- intermediate weights, preprocessing go here\n",
        "        stats/ (currently not being used)\n",
        "        midi/ <- output midis go here\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "3pEE99zUNRlV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1 Imports"
      ]
    },
    {
      "metadata": {
        "id": "lowbKt1LMnng",
        "colab_type": "code",
        "outputId": "621f685a-6a74-49ec-d17d-e038b9408830",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# Import Data Manip, Debug\n",
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import Music21\n",
        "!pip install music21\n",
        "from music21 import converter, instrument, note, chord, stream\n",
        "\n",
        "# Import Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import TimeDistributed\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import GRU\n",
        "from keras.layers import CuDNNLSTM\n",
        "from keras.layers import CuDNNGRU\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Bidirectional\n",
        "from keras.layers import RepeatVector\n",
        "from keras.layers import Flatten\n",
        "from keras.regularizers import L1L2\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import History\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: music21 in /usr/local/lib/python3.6/dist-packages (5.5.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "DSQDTCyePubS",
        "colab_type": "code",
        "outputId": "06cbb661-ce49-482b-fbdf-d1230b16ad6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "cell_type": "code",
      "source": [
        "# stuff needed for colaboratory to connect with drive\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "E: Package 'python-software-properties' has no installation candidate\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 131323 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.1-0ubuntu3~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.1-0ubuntu3~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NxRAEDfZNkIu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2 Mount and Set Directories*"
      ]
    },
    {
      "metadata": {
        "id": "-1Mmh8PPNeDj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D8-DMyMBNxG2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "DATA_FOLDER = 'drive/Project/train_data/edm_all'\n",
        "GRAPHS_FOLDER = 'drive/Project/train_output/graphs'\n",
        "MIDI_OUTPUT_FOLDER = 'drive/Project/train_output/midi'\n",
        "INTERMED_FOLDER = 'drive/Project/train_output/intermed'\n",
        "STATS_FOLDER = 'drive/Project/train_output/stats'\n",
        "\n",
        "# data specific\n",
        "EDM_CORPUS = '/edm_'\n",
        "\n",
        "# SET THIS TO ONE OF ABOVE (ACTION)\n",
        "CORPUS = EDM_CORPUS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xQCIp9r-sxwp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Notewise Root"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "jdYcbpd6s3A1"
      },
      "cell_type": "markdown",
      "source": [
        "# 3 Set Training Parameters*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "88zW5Z6rs3A6",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# SET PARAMETERS (ACTION)\n",
        "RESTS = False\n",
        "ROOT_EXTRACTION = True\n",
        "DURATION_BATCH_SIZE = 256\n",
        "NOTE_BATCH_SIZE = 128\n",
        "# SPECIFY PARAMETERS TO TEST AS LIST\n",
        "DROPOUTS = 0.5\n",
        "MODEL_SIZES = 512\n",
        "EPOCHS = 200"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "S48gjPm1s_CK"
      },
      "cell_type": "markdown",
      "source": [
        "# 4 Preprocess Data (MIDI Data Into Notes Corpus and Duration Corpus)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "1d9b21c3-0762-4c27-8cbe-2ccfa3ef1e3e",
        "id": "n6ju9CN4s_CO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1023
        }
      },
      "cell_type": "code",
      "source": [
        "# Test\n",
        "\n",
        "notes_corpus = []\n",
        "durations_corpus = []\n",
        "\n",
        "for file in glob.glob(DATA_FOLDER + \"/*.mid\"):\n",
        "    try:\n",
        "      print(\"Extracting MIDI File: \", file)\n",
        "      midi_stream = converter.parse(file)\n",
        "\n",
        "      notes = None\n",
        "\n",
        "      partition = instrument.partitionByInstrument(midi_stream)\n",
        "\n",
        "      if not RESTS:\n",
        "        # No rests\n",
        "        if partition: \n",
        "            notes = partition.parts[0].recurse()\n",
        "        else: \n",
        "            notes = midi_stream.flat.notes\n",
        "      else:\n",
        "        # With rests\n",
        "        if partition: \n",
        "            notes = partition.parts[0].recurse()\n",
        "        else: \n",
        "            notes = midi_stream.notesAndRests\n",
        "          \n",
        "      in_song_notes = []\n",
        "      in_song_durations = []\n",
        "      for element in notes:\n",
        "          in_song_durations.append(element.duration.quarterLength)\n",
        "          if isinstance(element, note.Note):\n",
        "              in_song_notes.append(str(element.pitch))\n",
        "          elif RESTS and isinstance(element, note.Rest):\n",
        "              in_song_notes.append(\"R\")\n",
        "          elif isinstance(element, chord.Chord):\n",
        "              if ROOT_EXTRACTION:\n",
        "                  in_song_notes.append(element.root().nameWithOctave)\n",
        "              else:\n",
        "                  in_song.append('.'.join(str(n) for n in element.normalOrder))\n",
        "    except:\n",
        "      pass\n",
        "    \n",
        "    notes_corpus.append(in_song_notes)\n",
        "    durations_corpus.append(in_song_durations)\n",
        "            \n",
        "# Write\n",
        "with open(INTERMED_FOLDER + CORPUS + 'notes_corpus', 'wb+') as filepath:\n",
        "    pickle.dump(notes_corpus, filepath)\n",
        "    \n",
        "with open(INTERMED_FOLDER + CORPUS + 'durations_corpus', 'wb+') as filepath:\n",
        "    pickle.dump(durations_corpus, filepath)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Afrojack _ David Guetta - Another Life  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Lonely Together.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Axwell _ Ingrosso - More Than You Know  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Sing Me To Sleep  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Without You.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alesso ft. Matthew Koma - Years (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Tired  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Alone  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Alan Walker - Faded (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Avicii - Dear Boy.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Axwell _ Shapov - Belong (Axwell and Years Remode) (Midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 2 - C Maj.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 3 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Bruno Mars - That_s What I Like (Alan Walker Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Calvin Harris - Blame  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Calvin Harris - My Way  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 5 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 6 - F Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 1 - C Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 4 - D Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 7 - F Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 14 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 8 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 16 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 9 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 15 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 10 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 13 - A Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 12 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Cymatics - Lazer MIDI 11 - G Min.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Galantis - Love On Me  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kill Paris ft. Royal - Operate (Illenium Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Headhunterz _ KSHMR - Dharma  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Ritual (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Galantis - No Money  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Summer  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kygo _ Selena Gomez - It Ain_t Me  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Marshmello - Alone (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Kygo _ Ellie Goulding - First Time  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Madeon - Icarus  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix _ Bebe Rexha - In The Name Of Love  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix - There For You  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix ft. The Federal Empire - Hold On _ Believe (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Porter Robinson _ Madeon - Shelter (Original Mix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Martin Garrix _ Dua Lipa - Scared To Be Lonely  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Porter Robinson - Sad Machine  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Nero - The Thrill (Porter Robinson Remix) (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Seven Lions x Echos - Cold Skin  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Sebastian Ingrosso _ Alesso feat. Ryan Tedder - Calling (Lose My Mind).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/R3hab _ KSHMR - Strong  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers _ Coldplay - Something Just Like This.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers ft. Halsey - Closer.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers ft. Phoebe Ryan - All We Know.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers Ft. Daya - Don_t Let Me Down.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers - Paris.mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Tiesto _ KSHMR feat. Vassy - Secrets  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/Zedd _ Hailee Steinfeld Grey - Starving  (midi by Carlo Prato) (www.cprato.com).mid\n",
            "Extracting MIDI File:  drive/Project/train_data/edm_all/The Chainsmokers feat. XYLO - Setting Fires.mid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "zpCVM3rqtVBB"
      },
      "cell_type": "markdown",
      "source": [
        "# 5 Preprocess Corpus Into Train Data"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "atPrtzt6tVBI"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.1 Notes Corpus"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "e8c45d2d-ba88-4268-ec0a-8c0b75fd0170",
        "id": "Tw8fJEk8tVBM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# load notes corpus\n",
        "with open(INTERMED_FOLDER + CORPUS + 'notes_corpus', 'rb') as filepath:\n",
        "    notes_corpus = pickle.load(filepath)\n",
        "    \n",
        "# If doing learning for one song at a time only\n",
        "flattened_notes_corpus = []\n",
        "for song_notes in notes_corpus:\n",
        "    flattened_notes_corpus += song_notes\n",
        "\n",
        "vocab_size = len(set(flattened_notes_corpus))\n",
        "print(vocab_size)\n",
        "\n",
        "# Produce input output sequences\n",
        "window_size = 60\n",
        "note_sequence_input = []\n",
        "next_note_output = []\n",
        "\n",
        "notes = sorted(set(flattened_notes_corpus))\n",
        "note2int = dict((note, num) for num, note in enumerate(notes))\n",
        "\n",
        "for i in range(0, len(notes_corpus)):\n",
        "    for j in range(0, len(notes_corpus[i]) - window_size):\n",
        "        current_sequence = [note2int[note] for note in notes_corpus[i][j:window_size+j]]\n",
        "        next_note = note2int[notes_corpus[i][window_size+j]]\n",
        "        note_sequence_input.append(current_sequence)\n",
        "        next_note_output.append(next_note)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "eb97e881-4116-4dd4-ec67-9d96ba168acc",
        "id": "8PrFbyLytVBa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# check train and label shapes\n",
        "training_data = np.reshape(note_sequence_input, (len(note_sequence_input), window_size , 1))\n",
        "training_data = training_data / float(vocab_size)\n",
        "print('Train shape: ' + str(training_data.shape))\n",
        "training_label = np_utils.to_categorical(next_note_output)\n",
        "print('Label shape: ' + str(training_label.shape))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (6258, 60, 1)\n",
            "Label shape: (6258, 68)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "gJuuh07TtVBi"
      },
      "cell_type": "markdown",
      "source": [
        "## 5.2 Durations Corpus"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "3340c1e4-e030-42f7-c561-d35ab4e112b6",
        "id": "xn3bs0FztVBk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "cell_type": "code",
      "source": [
        "# load notes corpus\n",
        "with open(INTERMED_FOLDER + CORPUS + 'durations_corpus', 'rb') as filepath:\n",
        "    duration_corpus = pickle.load(filepath)\n",
        "\n",
        "# Learn on one song at a time\n",
        "flattened_duration_corpus = []\n",
        "for song_durations in duration_corpus:\n",
        "    flattened_duration_corpus += song_durations\n",
        "    \n",
        "# import collections\n",
        "# counter = collections.Counter(flattened_duration_corpus)\n",
        "# print(counter)\n",
        "\n",
        "\n",
        "duration_vocab_size = len(set(flattened_duration_corpus))\n",
        "print(duration_vocab_size)\n",
        "\n",
        "# Produce input output sequences\n",
        "duration_window_size = 30\n",
        "duration_sequence_input = []\n",
        "next_duration_output = []\n",
        "\n",
        "durations = sorted(set(flattened_duration_corpus))\n",
        "duration2int = dict((duration, num) for num, duration in enumerate(durations))\n",
        "\n",
        "print(duration2int)\n",
        "# # Write\n",
        "# with open(INTERMED_FOLDER + \"/edm_duration_counter\", 'wb+') as filepath:\n",
        "#     pickle.dump(counter, filepath)\n",
        "\n",
        "for i in range(0, len(duration_corpus)):\n",
        "    for j in range(0, len(duration_corpus[i]) - duration_window_size):\n",
        "        current_duration_sequence = [duration2int[note] for note in duration_corpus[i][j:duration_window_size+j]]\n",
        "        next_duration = duration2int[duration_corpus[i][duration_window_size+j]]\n",
        "        duration_sequence_input.append(current_duration_sequence)\n",
        "        next_duration_output.append(next_duration)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61\n",
            "{0.0: 0, Fraction(1, 12): 1, Fraction(1, 6): 2, 0.25: 3, Fraction(1, 3): 4, Fraction(5, 12): 5, 0.5: 6, Fraction(2, 3): 7, 0.75: 8, 1.0: 9, Fraction(7, 6): 10, 1.25: 11, Fraction(4, 3): 12, 1.5: 13, Fraction(5, 3): 14, 1.75: 15, 2.0: 16, 2.25: 17, 2.5: 18, 3.0: 19, 3.25: 20, 3.5: 21, 4.0: 22, Fraction(25, 6): 23, 4.25: 24, 4.5: 25, 4.75: 26, 5.0: 27, 5.25: 28, 5.75: 29, 6.0: 30, 6.5: 31, 7.0: 32, Fraction(23, 3): 33, 8.0: 34, 8.5: 35, 9.0: 36, 14.5: 37, 16.0: 38, 18.0: 39, 27.5: 40, 32.0: 41, 32.5: 42, 36.0: 43, 36.25: 44, 39.5: 45, 47.5: 46, 48.0: 47, 61.0: 48, 64.25: 49, 68.0: 50, 72.25: 51, 72.5: 52, 80.0: 53, 84.25: 54, Fraction(553, 6): 55, 94.5: 56, 128.25: 57, 161.0: 58, 176.0: 59, 200.5: 60}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "outputId": "409cad71-f978-49bd-d0d9-13736231e8f3",
        "id": "DYIJqWNZtVBr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "# check train and label shapes\n",
        "duration_training_data = np.reshape(duration_sequence_input, (len(duration_sequence_input), duration_window_size , 1))\n",
        "duration_training_data = duration_training_data / float(duration_vocab_size)\n",
        "print('Train shape: ' + str(duration_training_data.shape))\n",
        "duration_training_label = np_utils.to_categorical(next_duration_output)\n",
        "print('Label shape: ' + str(duration_training_label.shape))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train shape: (13030, 30, 1)\n",
            "Label shape: (13030, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "CJ7qsp3ItjYx"
      },
      "cell_type": "markdown",
      "source": [
        "# 6 Train"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "HXICQ93ntjY5"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.1 Helpers to Create Model"
      ]
    },
    {
      "metadata": {
        "id": "uQmR9opzqC2E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "#from keras import initializations\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer=None, b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            model.add(LSTM(64, return_sequences=True))\n",
        "            model.add(Attention())\n",
        "        \"\"\"\n",
        "        self.supports_masking = True\n",
        "        #self.init = initializations.get('glorot_uniform')\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "\n",
        "        self.W = self.add_weight((input_shape[-1],),\n",
        "                                 initializer=self.init,\n",
        "                                 name='{}_W'.format(self.name),\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        self.features_dim = input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight((input_shape[1],),\n",
        "                                     initializer='zero',\n",
        "                                     name='{}_b'.format(self.name),\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "    def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "        # eij = K.dot(x, self.W) TF backend doesn't support it\n",
        "\n",
        "        # features_dim = self.W.shape[0]\n",
        "        # step_dim = x._keras_shape[1]\n",
        "\n",
        "        features_dim = self.features_dim\n",
        "        step_dim = self.step_dim\n",
        "\n",
        "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "\n",
        "        if self.bias:\n",
        "            eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "            a *= K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        a = K.expand_dims(a)\n",
        "        weighted_input = x * a\n",
        "    #print weigthted_input.shape\n",
        "        return K.sum(weighted_input, axis=1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        #return input_shape[0], input_shape[-1]\n",
        "        return input_shape[0],  self.features_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "lS3ji0DgtjY8",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv1D\n",
        "from keras.layers import MaxPooling1D\n",
        "from keras.layers import BatchNormalization\n",
        "\n",
        "def create_model(network_input, n_vocab, model_size, dropout, window_size):\n",
        "  model = Sequential()\n",
        "  reg = L1L2(0, 0)\n",
        "  \n",
        "  # CNN\n",
        "  model.add(Conv1D(512, 5))\n",
        "  model.add(MaxPooling1D())\n",
        "  model.add(BatchNormalization())\n",
        "  model.add(Activation('relu'))\n",
        "  \n",
        "  #LSTM\n",
        "  model.add(LSTM(\n",
        "      model_size,\n",
        "      input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "      return_sequences=True,\n",
        "      dropout=dropout, recurrent_dropout=0.3\n",
        "  ))\n",
        "  model.add(CuDNNLSTM(model_size, return_sequences=True, kernel_regularizer=reg))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(CuDNNLSTM(model_size, return_sequences=True, kernel_regularizer=reg))\n",
        "  model.add(Attention(window_size))\n",
        "  model.add(Dense(128))\n",
        "  model.add(Dropout(dropout))\n",
        "  model.add(Dense(n_vocab))\n",
        "  model.add(Activation('softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "  return model\n",
        "\n",
        "def create_callback_list(model_size, dropout, model_type):\n",
        "  filepath = INTERMED_FOLDER + '/%skarpathy-model-weights-%s-%s-%s.hdf5' % (CORPUS, model_type, model_size, dropout)\n",
        "  model_checkpoint = ModelCheckpoint(\n",
        "    filepath,\n",
        "    monitor='loss',\n",
        "    verbose=0,\n",
        "    save_best_only=True,\n",
        "    mode='min'\n",
        "  )\n",
        "  return [model_checkpoint], filepath\n",
        "\n",
        "# acc history\n",
        "def setup_plot(dropout, size):\n",
        "  plt.title('Model Accuracy vs. Epoc with Dropout=%s Size=%s' % (dropout, size))\n",
        "  plt.ylabel('accuracy')\n",
        "  plt.xlabel('epoch')\n",
        "  \n",
        "def plot_history(history, model_type, dropout, size):\n",
        "  plt.plot(history.history['acc'], label=\"%s train accuracy\" % model_type)\n",
        "  plt.plot(history.history['val_acc'], label=\"%s val accuracy\" % model_type)\n",
        "\n",
        "def save_plot(file_path):\n",
        "  plt.legend()\n",
        "  plt.savefig(file_path)\n",
        "  plt.clf()\n",
        "  \n",
        "def predict_duration(model, WEIGHT_PATH):\n",
        "  # Prediction\n",
        "  model.load_weights(WEIGHT_PATH)\n",
        "  starting_sequence = np.random.randint(219, size=duration_window_size)\n",
        "  pattern_sequence = starting_sequence.tolist()\n",
        "  prediction_output = []\n",
        "\n",
        "  duration2note = dict((num, note) for num, note in enumerate(durations))\n",
        "  print (duration2note)\n",
        "  \n",
        "  for i in range(400):\n",
        "      prediction_input = np.reshape(pattern_sequence, (1, len(pattern_sequence), 1))\n",
        "      prediction_input = prediction_input / float(duration_vocab_size)\n",
        "\n",
        "      if i%3 == 0:   \n",
        "          print('\\r', 'Predicting.  Duration: ', i, end='')  \n",
        "      if i%3 == 1:   \n",
        "          print('\\r', 'Predicting.. Duration: ', i, end='')\n",
        "      if i%3 == 2:   \n",
        "          print('\\r', 'Predicting...Duration: ', i, end='')\n",
        "      prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "      prediction_values = np.arange(len(prediction[0]))\n",
        "      prediction_prob = np.asarray(list(prediction[0])) / float(sum(prediction[0]))\n",
        "\n",
        "      # Most probable note prediction\n",
        "#       index = np.argmax(prediction)\n",
        "#       note_instance = duration2note[index]\n",
        "#       prediction_output.append(note_instance)\n",
        "      index = np.random.choice(prediction_values, 1, p=prediction_prob)\n",
        "      note_instance = duration2note[int(index[0])]\n",
        "      prediction_output.append(note_instance)\n",
        "\n",
        "      pattern_sequence.append(index)\n",
        "      pattern_sequence = pattern_sequence[1:len(pattern_sequence)]    \n",
        "\n",
        "  prediction_output = prediction_output[300:len(prediction_output)]\n",
        "\n",
        "  # Write\n",
        "  with open(INTERMED_FOLDER + (\"%sduration_prediction_output\" % CORPUS), 'wb+') as filepath:\n",
        "      pickle.dump(prediction_output, filepath)\n",
        "  \n",
        "  return prediction_output\n",
        "\n",
        "def predict_note(model, WEIGHT_PATH):\n",
        "  model.load_weights(WEIGHT_PATH)\n",
        "  # Prediction\n",
        "  starting_sequence = np.random.randint(219, size=window_size)\n",
        "  pattern_sequence = starting_sequence.tolist()\n",
        "  prediction_output = []\n",
        "\n",
        "  int2note = dict((num, note) for num, note in enumerate(notes))\n",
        "  print (int2note)\n",
        "\n",
        "  for i in range(400):\n",
        "      prediction_input = np.reshape(pattern_sequence, (1, len(pattern_sequence), 1))\n",
        "      prediction_input = prediction_input / float(vocab_size)\n",
        "\n",
        "      if i%3 == 0:   \n",
        "          print('\\r', 'Predicting.  Note: ', i, end='')  \n",
        "      if i%3 == 1:   \n",
        "          print('\\r', 'Predicting.. Note: ', i, end='')\n",
        "      if i%3 == 2:   \n",
        "          print('\\r', 'Predicting...Note: ', i, end='')\n",
        "      prediction = model.predict(prediction_input, verbose=0)\n",
        "\n",
        "#       prediction_values = np.arange(len(prediction[0]))\n",
        "#       prediction_prob = np.asarray(list(prediction[0])) / float(sum(prediction[0]))\n",
        "#       print(prediction_prob)\n",
        "\n",
        "      # Most probable note prediction\n",
        "      index = np.argmax(prediction)\n",
        "      note_instance = int2note[index]\n",
        "      prediction_output.append(note_instance)\n",
        "\n",
        "      # Predict based on prob dist\n",
        "#       index = np.random.choice(prediction_values, 1, p=prediction_prob)\n",
        "  #     print(index[0])\n",
        "  #     print(type(index[0]))\n",
        "#       note_instance = int2note[int(index[0])]\n",
        "#       prediction_output.append(note_instance)\n",
        "\n",
        "      pattern_sequence.append(index)\n",
        "      pattern_sequence = pattern_sequence[1:len(pattern_sequence)]    \n",
        "\n",
        "  prediction_output = prediction_output[300:len(prediction_output)]\n",
        "  # Write\n",
        "  with open(INTERMED_FOLDER + (\"%snotes_prediction_output\" % CORPUS), 'wb+') as filepath:\n",
        "      pickle.dump(prediction_output, filepath)\n",
        "  return prediction_output\n",
        "\n",
        "def output_midi(prediction_output, duration_prediction_output, dropout, model_size):\n",
        "  offset = 0\n",
        "  output_notes = []\n",
        "  for pattern, duration in zip(prediction_output, duration_prediction_output):\n",
        "\n",
        "      if ('.' in pattern) or pattern.isdigit():\n",
        "          chord_array = pattern.split('.')\n",
        "          chord_notes = []\n",
        "          for note_instance in chord_array:\n",
        "              note_object = note.Note(int(note_instance))\n",
        "              note_object.duration.quarterLength = duration\n",
        "              note_object.storedInstrument = instrument.Piano()\n",
        "              chord_notes.append(note_object)\n",
        "          chord_object = chord.Chord(chord_notes)\n",
        "          chord_object.offset = offset\n",
        "          output_notes.append(chord_object)\n",
        "      elif 'R' == pattern:\n",
        "          note_object = note.Rest()\n",
        "          note_object.duration.quarterLength = duration\n",
        "          note_object.offset = offset\n",
        "          output_notes.append(note_object)\n",
        "      else:\n",
        "          note_object = note.Note(pattern)\n",
        "          note_object.duration.quarterLength = duration\n",
        "          note_object.offset = offset\n",
        "          note_object.storedInstrument = instrument.Piano()\n",
        "          output_notes.append(note_object)\n",
        "\n",
        "      offset += 0.5\n",
        "\n",
        "  midi_stream = stream.Stream(output_notes)\n",
        "  midi_stream.write('midi', fp=MIDI_OUTPUT_FOLDER + CORPUS + '%s_%s.mid' % (dropout, model_size))\n",
        "  print('\\nWrote midi...')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "sqVfHjjZtjZF"
      },
      "cell_type": "markdown",
      "source": [
        "## 6.2 Preprocessing Optimization"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4q9dE9VrtjZH",
        "outputId": "c01771c5-b501-4c0f-9aff-9405103dd598",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13824
        }
      },
      "cell_type": "code",
      "source": [
        "notes_histories = {}\n",
        "duration_histories = {}\n",
        "\n",
        "dropout, model_size = DROPOUTS, MODEL_SIZES\n",
        "setup_plot(dropout, model_size)\n",
        "print('Running duration training on notewise with rests:%s and root extraction:%s' % (RESTS, ROOT_EXTRACTION))\n",
        "duration_callbacks, duration_weight_path = create_callback_list('duration', dropout, model_size)\n",
        "duration_model = create_model(duration_training_data, duration_training_label.shape[1], model_size, dropout, 13)\n",
        "duration_histories[(dropout, model_size)] = duration_model.fit(duration_training_data, duration_training_label, epochs=EPOCHS, batch_size=DURATION_BATCH_SIZE, callbacks=duration_callbacks, validation_split=0.2)\n",
        "plot_history(duration_histories[(dropout, model_size)], 'Durations', dropout, model_size)\n",
        "# output intermed duration\n",
        "duration_prediction = predict_duration(duration_model, duration_weight_path)\n",
        "\n",
        "print('\\n\\nRunning notes training on d:%s s:%s' % (dropout, model_size))\n",
        "notes_callbacks, note_weight_path = create_callback_list('notes', dropout, model_size)\n",
        "notes_model = create_model(training_data, training_label.shape[1], model_size, dropout, 28)\n",
        "notes_histories[(dropout, model_size)] = notes_model.fit(training_data, training_label, epochs=EPOCHS, batch_size=NOTE_BATCH_SIZE, callbacks=notes_callbacks, validation_split=0.2)\n",
        "plot_history(notes_histories[(dropout, model_size)], 'Notes', dropout, model_size)\n",
        "save_plot(GRAPHS_FOLDER + CORPUS + 'dropout=%s_size=%s.jpg' % (dropout, model_size))\n",
        "# output intermed notes\n",
        "note_prediction = predict_note(notes_model, note_weight_path)\n",
        "\n",
        "\n",
        "# output final midi\n",
        "output_midi(note_prediction, duration_prediction, dropout, model_size)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running duration training on notewise with rests:False and root extraction:True\n",
            "Train on 10424 samples, validate on 2606 samples\n",
            "Epoch 1/200\n",
            "10424/10424 [==============================] - 11s 1ms/step - loss: 2.0345 - acc: 0.4462 - val_loss: 1.8755 - val_acc: 0.4340\n",
            "Epoch 2/200\n",
            "10424/10424 [==============================] - 6s 592us/step - loss: 1.6154 - acc: 0.4907 - val_loss: 1.8908 - val_acc: 0.4536\n",
            "Epoch 3/200\n",
            "10424/10424 [==============================] - 6s 585us/step - loss: 1.5182 - acc: 0.4923 - val_loss: 1.8276 - val_acc: 0.4843\n",
            "Epoch 4/200\n",
            "10424/10424 [==============================] - 6s 586us/step - loss: 1.4423 - acc: 0.5022 - val_loss: 1.9088 - val_acc: 0.4605\n",
            "Epoch 5/200\n",
            "10424/10424 [==============================] - 6s 586us/step - loss: 1.3748 - acc: 0.5215 - val_loss: 1.8547 - val_acc: 0.4470\n",
            "Epoch 6/200\n",
            "10424/10424 [==============================] - 6s 579us/step - loss: 1.3066 - acc: 0.5374 - val_loss: 1.9652 - val_acc: 0.4635\n",
            "Epoch 7/200\n",
            "10424/10424 [==============================] - 6s 580us/step - loss: 1.2741 - acc: 0.5463 - val_loss: 2.0013 - val_acc: 0.4666\n",
            "Epoch 8/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 1.2463 - acc: 0.5502 - val_loss: 1.9926 - val_acc: 0.4574\n",
            "Epoch 9/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 1.2081 - acc: 0.5571 - val_loss: 2.0833 - val_acc: 0.4540\n",
            "Epoch 10/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 1.1787 - acc: 0.5652 - val_loss: 2.0458 - val_acc: 0.4486\n",
            "Epoch 11/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 1.1426 - acc: 0.5769 - val_loss: 2.0315 - val_acc: 0.4658\n",
            "Epoch 12/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 1.1344 - acc: 0.5771 - val_loss: 2.1279 - val_acc: 0.4693\n",
            "Epoch 13/200\n",
            "10424/10424 [==============================] - 6s 584us/step - loss: 1.1136 - acc: 0.5806 - val_loss: 2.1265 - val_acc: 0.4532\n",
            "Epoch 14/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 1.0788 - acc: 0.5962 - val_loss: 2.2276 - val_acc: 0.4528\n",
            "Epoch 15/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 1.0527 - acc: 0.6051 - val_loss: 2.2195 - val_acc: 0.4355\n",
            "Epoch 16/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 1.0122 - acc: 0.6163 - val_loss: 2.1800 - val_acc: 0.4190\n",
            "Epoch 17/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.9965 - acc: 0.6260 - val_loss: 2.3691 - val_acc: 0.4152\n",
            "Epoch 18/200\n",
            "10424/10424 [==============================] - 6s 585us/step - loss: 0.9551 - acc: 0.6431 - val_loss: 2.3702 - val_acc: 0.4632\n",
            "Epoch 19/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.9121 - acc: 0.6593 - val_loss: 2.5031 - val_acc: 0.4436\n",
            "Epoch 20/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.8978 - acc: 0.6639 - val_loss: 2.4800 - val_acc: 0.4455\n",
            "Epoch 21/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.8519 - acc: 0.6825 - val_loss: 2.5395 - val_acc: 0.4597\n",
            "Epoch 22/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.8211 - acc: 0.7019 - val_loss: 2.7941 - val_acc: 0.3803\n",
            "Epoch 23/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.7916 - acc: 0.7074 - val_loss: 2.7553 - val_acc: 0.4447\n",
            "Epoch 24/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.7562 - acc: 0.7255 - val_loss: 2.7926 - val_acc: 0.3872\n",
            "Epoch 25/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.7306 - acc: 0.7338 - val_loss: 2.8616 - val_acc: 0.2828\n",
            "Epoch 26/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.6823 - acc: 0.7523 - val_loss: 3.1022 - val_acc: 0.3615\n",
            "Epoch 27/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.6425 - acc: 0.7674 - val_loss: 3.0078 - val_acc: 0.3668\n",
            "Epoch 28/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.6042 - acc: 0.7789 - val_loss: 3.0933 - val_acc: 0.3292\n",
            "Epoch 29/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.5821 - acc: 0.7910 - val_loss: 3.0659 - val_acc: 0.3899\n",
            "Epoch 30/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.5394 - acc: 0.8072 - val_loss: 3.1872 - val_acc: 0.3853\n",
            "Epoch 31/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.5150 - acc: 0.8159 - val_loss: 3.1006 - val_acc: 0.4071\n",
            "Epoch 32/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.4768 - acc: 0.8277 - val_loss: 3.4784 - val_acc: 0.3975\n",
            "Epoch 33/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.4326 - acc: 0.8455 - val_loss: 3.6992 - val_acc: 0.4010\n",
            "Epoch 34/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.4165 - acc: 0.8550 - val_loss: 3.5401 - val_acc: 0.3645\n",
            "Epoch 35/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.3994 - acc: 0.8572 - val_loss: 3.5560 - val_acc: 0.4102\n",
            "Epoch 36/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.3719 - acc: 0.8676 - val_loss: 3.7723 - val_acc: 0.3745\n",
            "Epoch 37/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.3409 - acc: 0.8825 - val_loss: 3.9742 - val_acc: 0.4110\n",
            "Epoch 38/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.3364 - acc: 0.8841 - val_loss: 3.9260 - val_acc: 0.3853\n",
            "Epoch 39/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.3175 - acc: 0.8906 - val_loss: 4.0844 - val_acc: 0.3561\n",
            "Epoch 40/200\n",
            "10424/10424 [==============================] - 6s 580us/step - loss: 0.2962 - acc: 0.8995 - val_loss: 4.2151 - val_acc: 0.3768\n",
            "Epoch 41/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.2689 - acc: 0.9086 - val_loss: 4.5842 - val_acc: 0.3484\n",
            "Epoch 42/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.2489 - acc: 0.9152 - val_loss: 4.3091 - val_acc: 0.3665\n",
            "Epoch 43/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.2295 - acc: 0.9213 - val_loss: 4.6975 - val_acc: 0.3500\n",
            "Epoch 44/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.2520 - acc: 0.9158 - val_loss: 4.4345 - val_acc: 0.3968\n",
            "Epoch 45/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.2311 - acc: 0.9227 - val_loss: 4.8109 - val_acc: 0.3722\n",
            "Epoch 46/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.2007 - acc: 0.9335 - val_loss: 4.6102 - val_acc: 0.3653\n",
            "Epoch 47/200\n",
            "10424/10424 [==============================] - 6s 579us/step - loss: 0.1879 - acc: 0.9343 - val_loss: 4.3689 - val_acc: 0.3952\n",
            "Epoch 48/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.1849 - acc: 0.9353 - val_loss: 4.5629 - val_acc: 0.3864\n",
            "Epoch 49/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.1658 - acc: 0.9434 - val_loss: 4.5981 - val_acc: 0.4156\n",
            "Epoch 50/200\n",
            "10424/10424 [==============================] - 6s 583us/step - loss: 0.1596 - acc: 0.9464 - val_loss: 4.7725 - val_acc: 0.3929\n",
            "Epoch 51/200\n",
            "10424/10424 [==============================] - 6s 584us/step - loss: 0.1587 - acc: 0.9452 - val_loss: 4.6802 - val_acc: 0.3622\n",
            "Epoch 52/200\n",
            "10424/10424 [==============================] - 6s 583us/step - loss: 0.1547 - acc: 0.9484 - val_loss: 4.7788 - val_acc: 0.4225\n",
            "Epoch 53/200\n",
            "10424/10424 [==============================] - 6s 583us/step - loss: 0.1467 - acc: 0.9523 - val_loss: 4.9093 - val_acc: 0.3833\n",
            "Epoch 54/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.1429 - acc: 0.9516 - val_loss: 5.1246 - val_acc: 0.3795\n",
            "Epoch 55/200\n",
            "10424/10424 [==============================] - 6s 580us/step - loss: 0.1451 - acc: 0.9521 - val_loss: 4.5869 - val_acc: 0.3849\n",
            "Epoch 56/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.1387 - acc: 0.9515 - val_loss: 4.8139 - val_acc: 0.4029\n",
            "Epoch 57/200\n",
            "10424/10424 [==============================] - 6s 582us/step - loss: 0.1409 - acc: 0.9506 - val_loss: 4.5983 - val_acc: 0.4167\n",
            "Epoch 58/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.1288 - acc: 0.9540 - val_loss: 4.8447 - val_acc: 0.3864\n",
            "Epoch 59/200\n",
            "10424/10424 [==============================] - 6s 582us/step - loss: 0.1393 - acc: 0.9517 - val_loss: 5.0064 - val_acc: 0.4094\n",
            "Epoch 60/200\n",
            "10424/10424 [==============================] - 6s 586us/step - loss: 0.1377 - acc: 0.9543 - val_loss: 5.0780 - val_acc: 0.3661\n",
            "Epoch 61/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.1311 - acc: 0.9543 - val_loss: 5.1290 - val_acc: 0.3680\n",
            "Epoch 62/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.1380 - acc: 0.9527 - val_loss: 5.1181 - val_acc: 0.3369\n",
            "Epoch 63/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.1282 - acc: 0.9554 - val_loss: 5.1189 - val_acc: 0.3615\n",
            "Epoch 64/200\n",
            "10424/10424 [==============================] - 6s 579us/step - loss: 0.1160 - acc: 0.9602 - val_loss: 5.3542 - val_acc: 0.3757\n",
            "Epoch 65/200\n",
            "10424/10424 [==============================] - 6s 569us/step - loss: 0.1097 - acc: 0.9628 - val_loss: 5.2506 - val_acc: 0.4052\n",
            "Epoch 66/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.1095 - acc: 0.9653 - val_loss: 5.2060 - val_acc: 0.3776\n",
            "Epoch 67/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.1108 - acc: 0.9634 - val_loss: 5.1840 - val_acc: 0.3726\n",
            "Epoch 68/200\n",
            "10424/10424 [==============================] - 6s 580us/step - loss: 0.1047 - acc: 0.9633 - val_loss: 5.2943 - val_acc: 0.3457\n",
            "Epoch 69/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.1067 - acc: 0.9642 - val_loss: 5.2160 - val_acc: 0.3668\n",
            "Epoch 70/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.1008 - acc: 0.9648 - val_loss: 5.2935 - val_acc: 0.3638\n",
            "Epoch 71/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.0996 - acc: 0.9654 - val_loss: 5.1946 - val_acc: 0.3910\n",
            "Epoch 72/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.0961 - acc: 0.9662 - val_loss: 5.3064 - val_acc: 0.3718\n",
            "Epoch 73/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0987 - acc: 0.9663 - val_loss: 5.5057 - val_acc: 0.3580\n",
            "Epoch 74/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.1033 - acc: 0.9639 - val_loss: 5.4319 - val_acc: 0.3615\n",
            "Epoch 75/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.1048 - acc: 0.9640 - val_loss: 5.5371 - val_acc: 0.3454\n",
            "Epoch 76/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0919 - acc: 0.9695 - val_loss: 5.6346 - val_acc: 0.3381\n",
            "Epoch 77/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0923 - acc: 0.9682 - val_loss: 5.7439 - val_acc: 0.3296\n",
            "Epoch 78/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0853 - acc: 0.9695 - val_loss: 5.4237 - val_acc: 0.3599\n",
            "Epoch 79/200\n",
            "10424/10424 [==============================] - 6s 585us/step - loss: 0.0890 - acc: 0.9689 - val_loss: 5.4350 - val_acc: 0.3553\n",
            "Epoch 80/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0887 - acc: 0.9684 - val_loss: 5.4851 - val_acc: 0.3711\n",
            "Epoch 81/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0894 - acc: 0.9696 - val_loss: 5.6688 - val_acc: 0.3776\n",
            "Epoch 82/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0944 - acc: 0.9680 - val_loss: 5.6787 - val_acc: 0.3388\n",
            "Epoch 83/200\n",
            "10424/10424 [==============================] - 6s 578us/step - loss: 0.0902 - acc: 0.9683 - val_loss: 5.6735 - val_acc: 0.3707\n",
            "Epoch 84/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0826 - acc: 0.9711 - val_loss: 5.6714 - val_acc: 0.3588\n",
            "Epoch 85/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0787 - acc: 0.9725 - val_loss: 5.6344 - val_acc: 0.3642\n",
            "Epoch 86/200\n",
            "10424/10424 [==============================] - 6s 582us/step - loss: 0.0874 - acc: 0.9719 - val_loss: 5.6557 - val_acc: 0.3734\n",
            "Epoch 87/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0905 - acc: 0.9688 - val_loss: 5.6159 - val_acc: 0.3296\n",
            "Epoch 88/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0955 - acc: 0.9682 - val_loss: 5.5011 - val_acc: 0.3500\n",
            "Epoch 89/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0913 - acc: 0.9682 - val_loss: 5.4866 - val_acc: 0.3396\n",
            "Epoch 90/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0913 - acc: 0.9692 - val_loss: 5.3626 - val_acc: 0.3561\n",
            "Epoch 91/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0834 - acc: 0.9713 - val_loss: 5.7107 - val_acc: 0.3285\n",
            "Epoch 92/200\n",
            "10424/10424 [==============================] - 6s 578us/step - loss: 0.0812 - acc: 0.9722 - val_loss: 5.5693 - val_acc: 0.3434\n",
            "Epoch 93/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0823 - acc: 0.9711 - val_loss: 5.5922 - val_acc: 0.3461\n",
            "Epoch 94/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0832 - acc: 0.9726 - val_loss: 5.6660 - val_acc: 0.3837\n",
            "Epoch 95/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0830 - acc: 0.9732 - val_loss: 5.7825 - val_acc: 0.3688\n",
            "Epoch 96/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0700 - acc: 0.9752 - val_loss: 5.8107 - val_acc: 0.3599\n",
            "Epoch 97/200\n",
            "10424/10424 [==============================] - 6s 585us/step - loss: 0.0785 - acc: 0.9726 - val_loss: 5.7713 - val_acc: 0.3772\n",
            "Epoch 98/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.0937 - acc: 0.9685 - val_loss: 5.7129 - val_acc: 0.3580\n",
            "Epoch 99/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0752 - acc: 0.9733 - val_loss: 5.9403 - val_acc: 0.3596\n",
            "Epoch 100/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0742 - acc: 0.9738 - val_loss: 6.0264 - val_acc: 0.3331\n",
            "Epoch 101/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0756 - acc: 0.9745 - val_loss: 5.6037 - val_acc: 0.3807\n",
            "Epoch 102/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0747 - acc: 0.9727 - val_loss: 5.6515 - val_acc: 0.3569\n",
            "Epoch 103/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.0791 - acc: 0.9733 - val_loss: 5.7340 - val_acc: 0.3285\n",
            "Epoch 104/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0810 - acc: 0.9729 - val_loss: 5.4949 - val_acc: 0.3841\n",
            "Epoch 105/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0762 - acc: 0.9734 - val_loss: 5.7315 - val_acc: 0.3576\n",
            "Epoch 106/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0707 - acc: 0.9764 - val_loss: 5.8203 - val_acc: 0.3634\n",
            "Epoch 107/200\n",
            "10424/10424 [==============================] - 6s 578us/step - loss: 0.0733 - acc: 0.9769 - val_loss: 5.8689 - val_acc: 0.3722\n",
            "Epoch 108/200\n",
            "10424/10424 [==============================] - 6s 593us/step - loss: 0.0774 - acc: 0.9729 - val_loss: 5.8760 - val_acc: 0.3676\n",
            "Epoch 109/200\n",
            "10424/10424 [==============================] - 6s 580us/step - loss: 0.0706 - acc: 0.9753 - val_loss: 5.6489 - val_acc: 0.3868\n",
            "Epoch 110/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.0708 - acc: 0.9752 - val_loss: 5.5825 - val_acc: 0.3903\n",
            "Epoch 111/200\n",
            "10424/10424 [==============================] - 6s 579us/step - loss: 0.0703 - acc: 0.9772 - val_loss: 5.6712 - val_acc: 0.3876\n",
            "Epoch 112/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.0713 - acc: 0.9756 - val_loss: 5.6844 - val_acc: 0.3853\n",
            "Epoch 113/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.0807 - acc: 0.9727 - val_loss: 5.6976 - val_acc: 0.3791\n",
            "Epoch 114/200\n",
            "10424/10424 [==============================] - 6s 578us/step - loss: 0.0686 - acc: 0.9744 - val_loss: 5.7641 - val_acc: 0.3960\n",
            "Epoch 115/200\n",
            "10424/10424 [==============================] - 6s 587us/step - loss: 0.0721 - acc: 0.9737 - val_loss: 5.6243 - val_acc: 0.3726\n",
            "Epoch 116/200\n",
            "10424/10424 [==============================] - 6s 583us/step - loss: 0.0680 - acc: 0.9762 - val_loss: 5.8862 - val_acc: 0.3899\n",
            "Epoch 117/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.0731 - acc: 0.9754 - val_loss: 5.9236 - val_acc: 0.3607\n",
            "Epoch 118/200\n",
            "10424/10424 [==============================] - 6s 579us/step - loss: 0.0718 - acc: 0.9750 - val_loss: 5.7383 - val_acc: 0.3795\n",
            "Epoch 119/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.0715 - acc: 0.9757 - val_loss: 5.6418 - val_acc: 0.3791\n",
            "Epoch 120/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0637 - acc: 0.9784 - val_loss: 5.8707 - val_acc: 0.3761\n",
            "Epoch 121/200\n",
            "10424/10424 [==============================] - 6s 567us/step - loss: 0.0732 - acc: 0.9768 - val_loss: 5.8923 - val_acc: 0.3718\n",
            "Epoch 122/200\n",
            "10424/10424 [==============================] - 6s 584us/step - loss: 0.0685 - acc: 0.9763 - val_loss: 5.7658 - val_acc: 0.3573\n",
            "Epoch 123/200\n",
            "10424/10424 [==============================] - 6s 569us/step - loss: 0.0628 - acc: 0.9781 - val_loss: 6.0804 - val_acc: 0.3538\n",
            "Epoch 124/200\n",
            "10424/10424 [==============================] - 6s 579us/step - loss: 0.0658 - acc: 0.9765 - val_loss: 5.7709 - val_acc: 0.3661\n",
            "Epoch 125/200\n",
            "10424/10424 [==============================] - 6s 577us/step - loss: 0.0669 - acc: 0.9756 - val_loss: 5.8624 - val_acc: 0.3738\n",
            "Epoch 126/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0618 - acc: 0.9795 - val_loss: 5.8508 - val_acc: 0.3860\n",
            "Epoch 127/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0628 - acc: 0.9787 - val_loss: 5.7794 - val_acc: 0.3753\n",
            "Epoch 128/200\n",
            "10424/10424 [==============================] - 6s 569us/step - loss: 0.0627 - acc: 0.9775 - val_loss: 5.7129 - val_acc: 0.4094\n",
            "Epoch 129/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0560 - acc: 0.9800 - val_loss: 5.9720 - val_acc: 0.3807\n",
            "Epoch 130/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0559 - acc: 0.9800 - val_loss: 5.8107 - val_acc: 0.3979\n",
            "Epoch 131/200\n",
            "10424/10424 [==============================] - 6s 579us/step - loss: 0.0566 - acc: 0.9795 - val_loss: 5.7521 - val_acc: 0.3787\n",
            "Epoch 132/200\n",
            "10424/10424 [==============================] - 6s 569us/step - loss: 0.0568 - acc: 0.9799 - val_loss: 5.9299 - val_acc: 0.3734\n",
            "Epoch 133/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0590 - acc: 0.9799 - val_loss: 5.9462 - val_acc: 0.3903\n",
            "Epoch 134/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0628 - acc: 0.9784 - val_loss: 5.7033 - val_acc: 0.3926\n",
            "Epoch 135/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0613 - acc: 0.9781 - val_loss: 5.7122 - val_acc: 0.3734\n",
            "Epoch 136/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0569 - acc: 0.9795 - val_loss: 5.9494 - val_acc: 0.3814\n",
            "Epoch 137/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0615 - acc: 0.9786 - val_loss: 5.9383 - val_acc: 0.3741\n",
            "Epoch 138/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0669 - acc: 0.9775 - val_loss: 5.8328 - val_acc: 0.3607\n",
            "Epoch 139/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0642 - acc: 0.9788 - val_loss: 6.2764 - val_acc: 0.3772\n",
            "Epoch 140/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0560 - acc: 0.9808 - val_loss: 6.1856 - val_acc: 0.3296\n",
            "Epoch 141/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0563 - acc: 0.9798 - val_loss: 6.1786 - val_acc: 0.3465\n",
            "Epoch 142/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0515 - acc: 0.9820 - val_loss: 6.3924 - val_acc: 0.3657\n",
            "Epoch 143/200\n",
            "10424/10424 [==============================] - 6s 579us/step - loss: 0.0597 - acc: 0.9796 - val_loss: 6.3000 - val_acc: 0.3538\n",
            "Epoch 144/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0575 - acc: 0.9794 - val_loss: 6.2044 - val_acc: 0.3619\n",
            "Epoch 145/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0543 - acc: 0.9818 - val_loss: 6.2398 - val_acc: 0.3542\n",
            "Epoch 146/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0588 - acc: 0.9779 - val_loss: 6.3551 - val_acc: 0.3546\n",
            "Epoch 147/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0635 - acc: 0.9799 - val_loss: 6.2280 - val_acc: 0.3791\n",
            "Epoch 148/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0689 - acc: 0.9777 - val_loss: 6.0154 - val_acc: 0.3711\n",
            "Epoch 149/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0661 - acc: 0.9774 - val_loss: 6.3057 - val_acc: 0.3442\n",
            "Epoch 150/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0726 - acc: 0.9752 - val_loss: 5.9129 - val_acc: 0.3503\n",
            "Epoch 151/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0653 - acc: 0.9773 - val_loss: 6.0887 - val_acc: 0.3926\n",
            "Epoch 152/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0683 - acc: 0.9775 - val_loss: 5.7616 - val_acc: 0.3638\n",
            "Epoch 153/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0606 - acc: 0.9786 - val_loss: 5.9280 - val_acc: 0.3764\n",
            "Epoch 154/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0597 - acc: 0.9810 - val_loss: 6.0257 - val_acc: 0.3699\n",
            "Epoch 155/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0555 - acc: 0.9817 - val_loss: 5.7942 - val_acc: 0.3592\n",
            "Epoch 156/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0568 - acc: 0.9806 - val_loss: 5.6917 - val_acc: 0.3749\n",
            "Epoch 157/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0643 - acc: 0.9789 - val_loss: 5.5516 - val_acc: 0.3741\n",
            "Epoch 158/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0612 - acc: 0.9777 - val_loss: 5.9881 - val_acc: 0.3642\n",
            "Epoch 159/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0560 - acc: 0.9815 - val_loss: 5.7722 - val_acc: 0.3872\n",
            "Epoch 160/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0562 - acc: 0.9793 - val_loss: 6.0907 - val_acc: 0.3503\n",
            "Epoch 161/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0639 - acc: 0.9784 - val_loss: 5.7248 - val_acc: 0.3661\n",
            "Epoch 162/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0579 - acc: 0.9805 - val_loss: 5.9680 - val_acc: 0.3565\n",
            "Epoch 163/200\n",
            "10424/10424 [==============================] - 6s 575us/step - loss: 0.0636 - acc: 0.9800 - val_loss: 5.9877 - val_acc: 0.3500\n",
            "Epoch 164/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0552 - acc: 0.9802 - val_loss: 6.0874 - val_acc: 0.3615\n",
            "Epoch 165/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0570 - acc: 0.9806 - val_loss: 5.9736 - val_acc: 0.3741\n",
            "Epoch 166/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0573 - acc: 0.9818 - val_loss: 5.9019 - val_acc: 0.3734\n",
            "Epoch 167/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0602 - acc: 0.9799 - val_loss: 5.8260 - val_acc: 0.3638\n",
            "Epoch 168/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0504 - acc: 0.9817 - val_loss: 5.8521 - val_acc: 0.3945\n",
            "Epoch 169/200\n",
            "10424/10424 [==============================] - 6s 584us/step - loss: 0.0519 - acc: 0.9806 - val_loss: 5.8499 - val_acc: 0.3833\n",
            "Epoch 170/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0523 - acc: 0.9817 - val_loss: 6.0510 - val_acc: 0.3964\n",
            "Epoch 171/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0550 - acc: 0.9807 - val_loss: 6.2435 - val_acc: 0.3734\n",
            "Epoch 172/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0615 - acc: 0.9786 - val_loss: 5.8763 - val_acc: 0.3699\n",
            "Epoch 173/200\n",
            "10424/10424 [==============================] - 6s 567us/step - loss: 0.0546 - acc: 0.9801 - val_loss: 5.7958 - val_acc: 0.3833\n",
            "Epoch 174/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0530 - acc: 0.9810 - val_loss: 5.8259 - val_acc: 0.3730\n",
            "Epoch 175/200\n",
            "10424/10424 [==============================] - 6s 568us/step - loss: 0.0563 - acc: 0.9799 - val_loss: 5.7765 - val_acc: 0.3780\n",
            "Epoch 176/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0566 - acc: 0.9810 - val_loss: 5.9027 - val_acc: 0.3887\n",
            "Epoch 177/200\n",
            "10424/10424 [==============================] - 6s 585us/step - loss: 0.0584 - acc: 0.9799 - val_loss: 5.7627 - val_acc: 0.3707\n",
            "Epoch 178/200\n",
            "10424/10424 [==============================] - 6s 573us/step - loss: 0.0576 - acc: 0.9795 - val_loss: 5.8043 - val_acc: 0.3661\n",
            "Epoch 179/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0516 - acc: 0.9819 - val_loss: 5.9833 - val_acc: 0.3730\n",
            "Epoch 180/200\n",
            "10424/10424 [==============================] - 6s 580us/step - loss: 0.0503 - acc: 0.9830 - val_loss: 5.7638 - val_acc: 0.3695\n",
            "Epoch 181/200\n",
            "10424/10424 [==============================] - 6s 588us/step - loss: 0.0485 - acc: 0.9824 - val_loss: 5.9502 - val_acc: 0.3699\n",
            "Epoch 182/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.0529 - acc: 0.9812 - val_loss: 5.9133 - val_acc: 0.3642\n",
            "Epoch 183/200\n",
            "10424/10424 [==============================] - 6s 578us/step - loss: 0.0524 - acc: 0.9820 - val_loss: 6.3180 - val_acc: 0.3365\n",
            "Epoch 184/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.0495 - acc: 0.9830 - val_loss: 6.1268 - val_acc: 0.3868\n",
            "Epoch 185/200\n",
            "10424/10424 [==============================] - 6s 578us/step - loss: 0.0517 - acc: 0.9830 - val_loss: 6.0741 - val_acc: 0.3822\n",
            "Epoch 186/200\n",
            "10424/10424 [==============================] - 6s 582us/step - loss: 0.0615 - acc: 0.9805 - val_loss: 6.2339 - val_acc: 0.3538\n",
            "Epoch 187/200\n",
            "10424/10424 [==============================] - 6s 585us/step - loss: 0.0532 - acc: 0.9813 - val_loss: 5.8958 - val_acc: 0.3776\n",
            "Epoch 188/200\n",
            "10424/10424 [==============================] - 6s 585us/step - loss: 0.0591 - acc: 0.9810 - val_loss: 6.0153 - val_acc: 0.3945\n",
            "Epoch 189/200\n",
            "10424/10424 [==============================] - 6s 581us/step - loss: 0.0534 - acc: 0.9815 - val_loss: 5.9417 - val_acc: 0.3757\n",
            "Epoch 190/200\n",
            "10424/10424 [==============================] - 6s 574us/step - loss: 0.0563 - acc: 0.9817 - val_loss: 6.0322 - val_acc: 0.3695\n",
            "Epoch 191/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.0531 - acc: 0.9818 - val_loss: 6.1275 - val_acc: 0.3860\n",
            "Epoch 192/200\n",
            "10424/10424 [==============================] - 6s 568us/step - loss: 0.0495 - acc: 0.9831 - val_loss: 5.9489 - val_acc: 0.3956\n",
            "Epoch 193/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0489 - acc: 0.9833 - val_loss: 6.1167 - val_acc: 0.3688\n",
            "Epoch 194/200\n",
            "10424/10424 [==============================] - 6s 587us/step - loss: 0.0503 - acc: 0.9824 - val_loss: 5.9778 - val_acc: 0.3741\n",
            "Epoch 195/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0544 - acc: 0.9808 - val_loss: 5.7849 - val_acc: 0.3995\n",
            "Epoch 196/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0576 - acc: 0.9802 - val_loss: 5.8865 - val_acc: 0.3615\n",
            "Epoch 197/200\n",
            "10424/10424 [==============================] - 6s 572us/step - loss: 0.0530 - acc: 0.9823 - val_loss: 6.1699 - val_acc: 0.4033\n",
            "Epoch 198/200\n",
            "10424/10424 [==============================] - 6s 570us/step - loss: 0.0498 - acc: 0.9825 - val_loss: 6.0124 - val_acc: 0.3741\n",
            "Epoch 199/200\n",
            "10424/10424 [==============================] - 6s 571us/step - loss: 0.0474 - acc: 0.9836 - val_loss: 6.3066 - val_acc: 0.3903\n",
            "Epoch 200/200\n",
            "10424/10424 [==============================] - 6s 576us/step - loss: 0.0538 - acc: 0.9817 - val_loss: 6.0009 - val_acc: 0.3423\n",
            "{0: 0.0, 1: Fraction(1, 12), 2: Fraction(1, 6), 3: 0.25, 4: Fraction(1, 3), 5: Fraction(5, 12), 6: 0.5, 7: Fraction(2, 3), 8: 0.75, 9: 1.0, 10: Fraction(7, 6), 11: 1.25, 12: Fraction(4, 3), 13: 1.5, 14: Fraction(5, 3), 15: 1.75, 16: 2.0, 17: 2.25, 18: 2.5, 19: 3.0, 20: 3.25, 21: 3.5, 22: 4.0, 23: Fraction(25, 6), 24: 4.25, 25: 4.5, 26: 4.75, 27: 5.0, 28: 5.25, 29: 5.75, 30: 6.0, 31: 6.5, 32: 7.0, 33: Fraction(23, 3), 34: 8.0, 35: 8.5, 36: 9.0, 37: 14.5, 38: 16.0, 39: 18.0, 40: 27.5, 41: 32.0, 42: 32.5, 43: 36.0, 44: 36.25, 45: 39.5, 46: 47.5, 47: 48.0, 48: 61.0, 49: 64.25, 50: 68.0, 51: 72.25, 52: 72.5, 53: 80.0, 54: 84.25, 55: Fraction(553, 6), 56: 94.5, 57: 128.25, 58: 161.0, 59: 176.0, 60: 200.5}\n",
            " Predicting.  Duration:  399\n",
            "\n",
            "Running notes training on d:0.5 s:512\n",
            "Train on 5006 samples, validate on 1252 samples\n",
            "Epoch 1/200\n",
            "5006/5006 [==============================] - 12s 2ms/step - loss: 3.8566 - acc: 0.0823 - val_loss: 4.1776 - val_acc: 0.0503\n",
            "Epoch 2/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 3.4904 - acc: 0.1328 - val_loss: 4.0783 - val_acc: 0.0567\n",
            "Epoch 3/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 3.2006 - acc: 0.1652 - val_loss: 4.1727 - val_acc: 0.0543\n",
            "Epoch 4/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.9493 - acc: 0.2115 - val_loss: 4.6041 - val_acc: 0.0407\n",
            "Epoch 5/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.7667 - acc: 0.2259 - val_loss: 4.4056 - val_acc: 0.0639\n",
            "Epoch 6/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.5839 - acc: 0.2583 - val_loss: 4.7669 - val_acc: 0.0575\n",
            "Epoch 7/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.4486 - acc: 0.2817 - val_loss: 5.0256 - val_acc: 0.0639\n",
            "Epoch 8/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.3743 - acc: 0.2865 - val_loss: 5.2560 - val_acc: 0.0679\n",
            "Epoch 9/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.2936 - acc: 0.3000 - val_loss: 5.3029 - val_acc: 0.0679\n",
            "Epoch 10/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.2057 - acc: 0.3110 - val_loss: 5.0697 - val_acc: 0.0439\n",
            "Epoch 11/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.1336 - acc: 0.3242 - val_loss: 5.4876 - val_acc: 0.0623\n",
            "Epoch 12/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 2.0565 - acc: 0.3380 - val_loss: 5.4962 - val_acc: 0.0663\n",
            "Epoch 13/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.9926 - acc: 0.3516 - val_loss: 5.3250 - val_acc: 0.0631\n",
            "Epoch 14/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.9530 - acc: 0.3664 - val_loss: 5.7732 - val_acc: 0.0679\n",
            "Epoch 15/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.8629 - acc: 0.3885 - val_loss: 5.8291 - val_acc: 0.0527\n",
            "Epoch 16/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.7788 - acc: 0.4167 - val_loss: 6.4272 - val_acc: 0.0559\n",
            "Epoch 17/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.7418 - acc: 0.4367 - val_loss: 5.5980 - val_acc: 0.0471\n",
            "Epoch 18/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.6295 - acc: 0.4596 - val_loss: 6.0364 - val_acc: 0.0679\n",
            "Epoch 19/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.5632 - acc: 0.4886 - val_loss: 5.8761 - val_acc: 0.0663\n",
            "Epoch 20/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.5239 - acc: 0.4984 - val_loss: 6.1348 - val_acc: 0.0743\n",
            "Epoch 21/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.4041 - acc: 0.5316 - val_loss: 6.1625 - val_acc: 0.0759\n",
            "Epoch 22/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.3774 - acc: 0.5362 - val_loss: 6.1705 - val_acc: 0.0679\n",
            "Epoch 23/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.3245 - acc: 0.5517 - val_loss: 6.3475 - val_acc: 0.0751\n",
            "Epoch 24/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.2907 - acc: 0.5677 - val_loss: 5.8497 - val_acc: 0.0647\n",
            "Epoch 25/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.2338 - acc: 0.5841 - val_loss: 6.3054 - val_acc: 0.0831\n",
            "Epoch 26/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.1829 - acc: 0.6013 - val_loss: 6.5188 - val_acc: 0.0767\n",
            "Epoch 27/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.0930 - acc: 0.6352 - val_loss: 6.1721 - val_acc: 0.0767\n",
            "Epoch 28/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 1.0632 - acc: 0.6360 - val_loss: 6.3571 - val_acc: 0.0727\n",
            "Epoch 29/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.9916 - acc: 0.6600 - val_loss: 7.0780 - val_acc: 0.0695\n",
            "Epoch 30/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.9779 - acc: 0.6658 - val_loss: 6.5057 - val_acc: 0.0727\n",
            "Epoch 31/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.9113 - acc: 0.6806 - val_loss: 7.1201 - val_acc: 0.0887\n",
            "Epoch 32/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.8945 - acc: 0.6934 - val_loss: 7.0088 - val_acc: 0.0735\n",
            "Epoch 33/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.8200 - acc: 0.7195 - val_loss: 7.2826 - val_acc: 0.0727\n",
            "Epoch 34/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.7613 - acc: 0.7359 - val_loss: 7.2136 - val_acc: 0.0735\n",
            "Epoch 35/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.7449 - acc: 0.7447 - val_loss: 7.5242 - val_acc: 0.0775\n",
            "Epoch 36/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.7907 - acc: 0.7279 - val_loss: 7.1815 - val_acc: 0.0855\n",
            "Epoch 37/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.7487 - acc: 0.7461 - val_loss: 7.4742 - val_acc: 0.0807\n",
            "Epoch 38/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.6598 - acc: 0.7717 - val_loss: 7.2269 - val_acc: 0.0687\n",
            "Epoch 39/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.6792 - acc: 0.7545 - val_loss: 7.5356 - val_acc: 0.0735\n",
            "Epoch 40/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.5923 - acc: 0.7994 - val_loss: 7.6732 - val_acc: 0.0815\n",
            "Epoch 41/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.5509 - acc: 0.8046 - val_loss: 7.7229 - val_acc: 0.0751\n",
            "Epoch 42/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.5070 - acc: 0.8218 - val_loss: 7.8757 - val_acc: 0.0743\n",
            "Epoch 43/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.5359 - acc: 0.8152 - val_loss: 7.8972 - val_acc: 0.0823\n",
            "Epoch 44/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.4610 - acc: 0.8340 - val_loss: 7.6850 - val_acc: 0.0743\n",
            "Epoch 45/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.4538 - acc: 0.8426 - val_loss: 8.3579 - val_acc: 0.0759\n",
            "Epoch 46/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.4879 - acc: 0.8346 - val_loss: 8.0242 - val_acc: 0.0823\n",
            "Epoch 47/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.4318 - acc: 0.8508 - val_loss: 8.1486 - val_acc: 0.0767\n",
            "Epoch 48/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.4152 - acc: 0.8582 - val_loss: 8.3031 - val_acc: 0.0751\n",
            "Epoch 49/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.4378 - acc: 0.8500 - val_loss: 8.7128 - val_acc: 0.0735\n",
            "Epoch 50/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.4140 - acc: 0.8578 - val_loss: 8.1022 - val_acc: 0.0791\n",
            "Epoch 51/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.3377 - acc: 0.8805 - val_loss: 8.4314 - val_acc: 0.0799\n",
            "Epoch 52/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.3793 - acc: 0.8716 - val_loss: 8.9263 - val_acc: 0.0799\n",
            "Epoch 53/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.3316 - acc: 0.8847 - val_loss: 8.7638 - val_acc: 0.0679\n",
            "Epoch 54/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.3587 - acc: 0.8809 - val_loss: 8.6617 - val_acc: 0.0775\n",
            "Epoch 55/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.3150 - acc: 0.8921 - val_loss: 8.9276 - val_acc: 0.0775\n",
            "Epoch 56/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.3019 - acc: 0.8973 - val_loss: 8.9864 - val_acc: 0.0823\n",
            "Epoch 57/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.2626 - acc: 0.9155 - val_loss: 9.1763 - val_acc: 0.0759\n",
            "Epoch 58/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.4191 - acc: 0.8614 - val_loss: 8.5400 - val_acc: 0.0799\n",
            "Epoch 59/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.3316 - acc: 0.8899 - val_loss: 8.6817 - val_acc: 0.0863\n",
            "Epoch 60/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.2634 - acc: 0.9107 - val_loss: 8.9603 - val_acc: 0.0727\n",
            "Epoch 61/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.2304 - acc: 0.9293 - val_loss: 9.2205 - val_acc: 0.0831\n",
            "Epoch 62/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1952 - acc: 0.9369 - val_loss: 9.6271 - val_acc: 0.0815\n",
            "Epoch 63/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1912 - acc: 0.9345 - val_loss: 9.4063 - val_acc: 0.0767\n",
            "Epoch 64/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.2225 - acc: 0.9303 - val_loss: 9.5691 - val_acc: 0.0783\n",
            "Epoch 65/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1992 - acc: 0.9341 - val_loss: 9.7452 - val_acc: 0.0799\n",
            "Epoch 66/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.2126 - acc: 0.9277 - val_loss: 9.4168 - val_acc: 0.0759\n",
            "Epoch 67/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1848 - acc: 0.9411 - val_loss: 9.7819 - val_acc: 0.0807\n",
            "Epoch 68/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1716 - acc: 0.9421 - val_loss: 9.8014 - val_acc: 0.0807\n",
            "Epoch 69/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1598 - acc: 0.9479 - val_loss: 9.7184 - val_acc: 0.0855\n",
            "Epoch 70/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1536 - acc: 0.9471 - val_loss: 9.3698 - val_acc: 0.0775\n",
            "Epoch 71/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.2601 - acc: 0.9187 - val_loss: 9.2691 - val_acc: 0.0863\n",
            "Epoch 72/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1863 - acc: 0.9377 - val_loss: 9.8344 - val_acc: 0.0783\n",
            "Epoch 73/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.2018 - acc: 0.9353 - val_loss: 9.5276 - val_acc: 0.0783\n",
            "Epoch 74/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1720 - acc: 0.9445 - val_loss: 9.4853 - val_acc: 0.0791\n",
            "Epoch 75/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1677 - acc: 0.9433 - val_loss: 9.6297 - val_acc: 0.0775\n",
            "Epoch 76/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1550 - acc: 0.9485 - val_loss: 9.7954 - val_acc: 0.0767\n",
            "Epoch 77/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1542 - acc: 0.9507 - val_loss: 9.6596 - val_acc: 0.0871\n",
            "Epoch 78/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1425 - acc: 0.9525 - val_loss: 10.1460 - val_acc: 0.0735\n",
            "Epoch 79/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1273 - acc: 0.9624 - val_loss: 9.9675 - val_acc: 0.0767\n",
            "Epoch 80/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1100 - acc: 0.9662 - val_loss: 9.9252 - val_acc: 0.0839\n",
            "Epoch 81/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1558 - acc: 0.9495 - val_loss: 9.9255 - val_acc: 0.0759\n",
            "Epoch 82/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1884 - acc: 0.9423 - val_loss: 9.8120 - val_acc: 0.0775\n",
            "Epoch 83/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1906 - acc: 0.9421 - val_loss: 9.8375 - val_acc: 0.0815\n",
            "Epoch 84/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1871 - acc: 0.9389 - val_loss: 9.9378 - val_acc: 0.0791\n",
            "Epoch 85/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1646 - acc: 0.9475 - val_loss: 9.7680 - val_acc: 0.0775\n",
            "Epoch 86/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1803 - acc: 0.9405 - val_loss: 10.1413 - val_acc: 0.0807\n",
            "Epoch 87/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1542 - acc: 0.9559 - val_loss: 9.6140 - val_acc: 0.0703\n",
            "Epoch 88/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1441 - acc: 0.9569 - val_loss: 9.4290 - val_acc: 0.0719\n",
            "Epoch 89/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1604 - acc: 0.9541 - val_loss: 9.6225 - val_acc: 0.0703\n",
            "Epoch 90/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1317 - acc: 0.9569 - val_loss: 10.0042 - val_acc: 0.0711\n",
            "Epoch 91/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1104 - acc: 0.9636 - val_loss: 9.7983 - val_acc: 0.0855\n",
            "Epoch 92/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1028 - acc: 0.9694 - val_loss: 10.0722 - val_acc: 0.0759\n",
            "Epoch 93/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0943 - acc: 0.9694 - val_loss: 10.2717 - val_acc: 0.0711\n",
            "Epoch 94/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1602 - acc: 0.9565 - val_loss: 10.1169 - val_acc: 0.0703\n",
            "Epoch 95/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1610 - acc: 0.9567 - val_loss: 10.3524 - val_acc: 0.0751\n",
            "Epoch 96/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1414 - acc: 0.9545 - val_loss: 10.2518 - val_acc: 0.0767\n",
            "Epoch 97/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1602 - acc: 0.9529 - val_loss: 10.2357 - val_acc: 0.0775\n",
            "Epoch 98/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1773 - acc: 0.9495 - val_loss: 10.0544 - val_acc: 0.0711\n",
            "Epoch 99/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1368 - acc: 0.9561 - val_loss: 10.2216 - val_acc: 0.0751\n",
            "Epoch 100/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1021 - acc: 0.9658 - val_loss: 10.1857 - val_acc: 0.0783\n",
            "Epoch 101/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0824 - acc: 0.9744 - val_loss: 10.5075 - val_acc: 0.0703\n",
            "Epoch 102/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0794 - acc: 0.9750 - val_loss: 10.6164 - val_acc: 0.0695\n",
            "Epoch 103/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1034 - acc: 0.9680 - val_loss: 10.4749 - val_acc: 0.0735\n",
            "Epoch 104/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0864 - acc: 0.9726 - val_loss: 10.5634 - val_acc: 0.0703\n",
            "Epoch 105/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0778 - acc: 0.9746 - val_loss: 10.6818 - val_acc: 0.0743\n",
            "Epoch 106/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0844 - acc: 0.9726 - val_loss: 10.7516 - val_acc: 0.0735\n",
            "Epoch 107/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0660 - acc: 0.9786 - val_loss: 10.5600 - val_acc: 0.0775\n",
            "Epoch 108/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0640 - acc: 0.9800 - val_loss: 10.5764 - val_acc: 0.0703\n",
            "Epoch 109/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1359 - acc: 0.9616 - val_loss: 10.8159 - val_acc: 0.0679\n",
            "Epoch 110/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1274 - acc: 0.9616 - val_loss: 10.4333 - val_acc: 0.0695\n",
            "Epoch 111/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1295 - acc: 0.9598 - val_loss: 10.1201 - val_acc: 0.0775\n",
            "Epoch 112/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0909 - acc: 0.9726 - val_loss: 10.5791 - val_acc: 0.0695\n",
            "Epoch 113/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0989 - acc: 0.9744 - val_loss: 10.7193 - val_acc: 0.0743\n",
            "Epoch 114/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1328 - acc: 0.9670 - val_loss: 10.9176 - val_acc: 0.0735\n",
            "Epoch 115/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0879 - acc: 0.9742 - val_loss: 10.9132 - val_acc: 0.0727\n",
            "Epoch 116/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0781 - acc: 0.9740 - val_loss: 10.9049 - val_acc: 0.0687\n",
            "Epoch 117/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0655 - acc: 0.9782 - val_loss: 10.9752 - val_acc: 0.0687\n",
            "Epoch 118/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0812 - acc: 0.9760 - val_loss: 11.0621 - val_acc: 0.0831\n",
            "Epoch 119/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0635 - acc: 0.9788 - val_loss: 11.2120 - val_acc: 0.0639\n",
            "Epoch 120/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0765 - acc: 0.9756 - val_loss: 10.9204 - val_acc: 0.0807\n",
            "Epoch 121/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1359 - acc: 0.9616 - val_loss: 10.9483 - val_acc: 0.0735\n",
            "Epoch 122/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1001 - acc: 0.9658 - val_loss: 10.7648 - val_acc: 0.0767\n",
            "Epoch 123/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1998 - acc: 0.9501 - val_loss: 9.8110 - val_acc: 0.0799\n",
            "Epoch 124/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1272 - acc: 0.9577 - val_loss: 10.0141 - val_acc: 0.0695\n",
            "Epoch 125/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1010 - acc: 0.9674 - val_loss: 10.4363 - val_acc: 0.0831\n",
            "Epoch 126/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0773 - acc: 0.9766 - val_loss: 10.0179 - val_acc: 0.0855\n",
            "Epoch 127/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0584 - acc: 0.9810 - val_loss: 10.3766 - val_acc: 0.0831\n",
            "Epoch 128/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0565 - acc: 0.9796 - val_loss: 10.3492 - val_acc: 0.0823\n",
            "Epoch 129/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0501 - acc: 0.9842 - val_loss: 10.3915 - val_acc: 0.0799\n",
            "Epoch 130/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1010 - acc: 0.9696 - val_loss: 10.5754 - val_acc: 0.0823\n",
            "Epoch 131/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0834 - acc: 0.9750 - val_loss: 10.5903 - val_acc: 0.0759\n",
            "Epoch 132/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1016 - acc: 0.9674 - val_loss: 10.6757 - val_acc: 0.0847\n",
            "Epoch 133/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1212 - acc: 0.9670 - val_loss: 10.8301 - val_acc: 0.0735\n",
            "Epoch 134/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0845 - acc: 0.9724 - val_loss: 10.7034 - val_acc: 0.0791\n",
            "Epoch 135/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1055 - acc: 0.9702 - val_loss: 10.6599 - val_acc: 0.0783\n",
            "Epoch 136/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0674 - acc: 0.9802 - val_loss: 11.0712 - val_acc: 0.0791\n",
            "Epoch 137/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0711 - acc: 0.9778 - val_loss: 10.8881 - val_acc: 0.0879\n",
            "Epoch 138/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0811 - acc: 0.9772 - val_loss: 10.9329 - val_acc: 0.0791\n",
            "Epoch 139/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0623 - acc: 0.9820 - val_loss: 11.0131 - val_acc: 0.0807\n",
            "Epoch 140/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0630 - acc: 0.9822 - val_loss: 10.9038 - val_acc: 0.0807\n",
            "Epoch 141/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1273 - acc: 0.9642 - val_loss: 11.0180 - val_acc: 0.0783\n",
            "Epoch 142/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1063 - acc: 0.9694 - val_loss: 11.1103 - val_acc: 0.0687\n",
            "Epoch 143/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1186 - acc: 0.9652 - val_loss: 11.1316 - val_acc: 0.0847\n",
            "Epoch 144/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1004 - acc: 0.9700 - val_loss: 10.9332 - val_acc: 0.0775\n",
            "Epoch 145/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1006 - acc: 0.9726 - val_loss: 11.1243 - val_acc: 0.0767\n",
            "Epoch 146/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0720 - acc: 0.9750 - val_loss: 11.1061 - val_acc: 0.0751\n",
            "Epoch 147/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0613 - acc: 0.9824 - val_loss: 11.0630 - val_acc: 0.0735\n",
            "Epoch 148/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0661 - acc: 0.9790 - val_loss: 10.8896 - val_acc: 0.0799\n",
            "Epoch 149/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0626 - acc: 0.9812 - val_loss: 10.7132 - val_acc: 0.0751\n",
            "Epoch 150/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0817 - acc: 0.9778 - val_loss: 11.1539 - val_acc: 0.0719\n",
            "Epoch 151/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0783 - acc: 0.9788 - val_loss: 10.8744 - val_acc: 0.0703\n",
            "Epoch 152/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0862 - acc: 0.9782 - val_loss: 10.8301 - val_acc: 0.0807\n",
            "Epoch 153/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0642 - acc: 0.9794 - val_loss: 10.7020 - val_acc: 0.0887\n",
            "Epoch 154/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0690 - acc: 0.9790 - val_loss: 10.7298 - val_acc: 0.0847\n",
            "Epoch 155/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0499 - acc: 0.9824 - val_loss: 10.8726 - val_acc: 0.0767\n",
            "Epoch 156/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0495 - acc: 0.9830 - val_loss: 11.1909 - val_acc: 0.0767\n",
            "Epoch 157/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0464 - acc: 0.9872 - val_loss: 10.9205 - val_acc: 0.0807\n",
            "Epoch 158/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0450 - acc: 0.9852 - val_loss: 11.1822 - val_acc: 0.0791\n",
            "Epoch 159/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0538 - acc: 0.9840 - val_loss: 11.1242 - val_acc: 0.0831\n",
            "Epoch 160/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0511 - acc: 0.9850 - val_loss: 11.1057 - val_acc: 0.0863\n",
            "Epoch 161/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0734 - acc: 0.9802 - val_loss: 11.1057 - val_acc: 0.0791\n",
            "Epoch 162/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0452 - acc: 0.9864 - val_loss: 10.9139 - val_acc: 0.0887\n",
            "Epoch 163/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0420 - acc: 0.9864 - val_loss: 10.8306 - val_acc: 0.0815\n",
            "Epoch 164/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1354 - acc: 0.9648 - val_loss: 11.0367 - val_acc: 0.0863\n",
            "Epoch 165/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0767 - acc: 0.9742 - val_loss: 10.8786 - val_acc: 0.0839\n",
            "Epoch 166/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0840 - acc: 0.9762 - val_loss: 11.0510 - val_acc: 0.0871\n",
            "Epoch 167/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0717 - acc: 0.9756 - val_loss: 11.0735 - val_acc: 0.0847\n",
            "Epoch 168/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0656 - acc: 0.9786 - val_loss: 10.9938 - val_acc: 0.0815\n",
            "Epoch 169/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.2506 - acc: 0.9355 - val_loss: 10.0601 - val_acc: 0.0631\n",
            "Epoch 170/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1373 - acc: 0.9557 - val_loss: 11.0604 - val_acc: 0.0647\n",
            "Epoch 171/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1479 - acc: 0.9567 - val_loss: 10.8096 - val_acc: 0.0703\n",
            "Epoch 172/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1072 - acc: 0.9688 - val_loss: 10.7226 - val_acc: 0.0767\n",
            "Epoch 173/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0908 - acc: 0.9744 - val_loss: 10.6729 - val_acc: 0.0687\n",
            "Epoch 174/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0625 - acc: 0.9792 - val_loss: 10.9126 - val_acc: 0.0719\n",
            "Epoch 175/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0563 - acc: 0.9834 - val_loss: 10.9976 - val_acc: 0.0727\n",
            "Epoch 176/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0569 - acc: 0.9804 - val_loss: 10.9207 - val_acc: 0.0767\n",
            "Epoch 177/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.1015 - acc: 0.9696 - val_loss: 11.2501 - val_acc: 0.0815\n",
            "Epoch 178/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0609 - acc: 0.9804 - val_loss: 11.2508 - val_acc: 0.0775\n",
            "Epoch 179/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0639 - acc: 0.9818 - val_loss: 11.1911 - val_acc: 0.0775\n",
            "Epoch 180/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0727 - acc: 0.9778 - val_loss: 10.8723 - val_acc: 0.0807\n",
            "Epoch 181/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0538 - acc: 0.9828 - val_loss: 11.1465 - val_acc: 0.0735\n",
            "Epoch 182/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0786 - acc: 0.9780 - val_loss: 11.2380 - val_acc: 0.0711\n",
            "Epoch 183/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0636 - acc: 0.9812 - val_loss: 11.1739 - val_acc: 0.0767\n",
            "Epoch 184/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0457 - acc: 0.9850 - val_loss: 10.9521 - val_acc: 0.0735\n",
            "Epoch 185/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0510 - acc: 0.9848 - val_loss: 11.1389 - val_acc: 0.0775\n",
            "Epoch 186/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0407 - acc: 0.9864 - val_loss: 11.1887 - val_acc: 0.0695\n",
            "Epoch 187/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0355 - acc: 0.9882 - val_loss: 11.1415 - val_acc: 0.0751\n",
            "Epoch 188/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0411 - acc: 0.9854 - val_loss: 11.1669 - val_acc: 0.0663\n",
            "Epoch 189/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0356 - acc: 0.9884 - val_loss: 11.0223 - val_acc: 0.0743\n",
            "Epoch 190/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0444 - acc: 0.9846 - val_loss: 11.1183 - val_acc: 0.0783\n",
            "Epoch 191/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0437 - acc: 0.9862 - val_loss: 11.1251 - val_acc: 0.0743\n",
            "Epoch 192/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0421 - acc: 0.9882 - val_loss: 11.1305 - val_acc: 0.0751\n",
            "Epoch 193/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0388 - acc: 0.9874 - val_loss: 11.1332 - val_acc: 0.0743\n",
            "Epoch 194/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0407 - acc: 0.9880 - val_loss: 11.2402 - val_acc: 0.0743\n",
            "Epoch 195/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0426 - acc: 0.9866 - val_loss: 11.3365 - val_acc: 0.0743\n",
            "Epoch 196/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0661 - acc: 0.9814 - val_loss: 11.1750 - val_acc: 0.0815\n",
            "Epoch 197/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0521 - acc: 0.9842 - val_loss: 11.2118 - val_acc: 0.0799\n",
            "Epoch 198/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0523 - acc: 0.9830 - val_loss: 11.3919 - val_acc: 0.0719\n",
            "Epoch 199/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0434 - acc: 0.9874 - val_loss: 11.2873 - val_acc: 0.0807\n",
            "Epoch 200/200\n",
            "5006/5006 [==============================] - 8s 2ms/step - loss: 0.0562 - acc: 0.9826 - val_loss: 11.3789 - val_acc: 0.0839\n",
            "{0: 'A1', 1: 'A2', 2: 'A3', 3: 'A4', 4: 'A5', 5: 'B-1', 6: 'B-2', 7: 'B-3', 8: 'B-4', 9: 'B-5', 10: 'B1', 11: 'B2', 12: 'B3', 13: 'B4', 14: 'B5', 15: 'C#1', 16: 'C#2', 17: 'C#3', 18: 'C#4', 19: 'C#5', 20: 'C#6', 21: 'C1', 22: 'C2', 23: 'C3', 24: 'C4', 25: 'C5', 26: 'C6', 27: 'D1', 28: 'D2', 29: 'D3', 30: 'D4', 31: 'D5', 32: 'D6', 33: 'E-0', 34: 'E-2', 35: 'E-3', 36: 'E-4', 37: 'E-5', 38: 'E-6', 39: 'E1', 40: 'E2', 41: 'E3', 42: 'E4', 43: 'E5', 44: 'E6', 45: 'F#1', 46: 'F#2', 47: 'F#3', 48: 'F#4', 49: 'F#5', 50: 'F#6', 51: 'F0', 52: 'F1', 53: 'F2', 54: 'F3', 55: 'F4', 56: 'F5', 57: 'G#1', 58: 'G#2', 59: 'G#3', 60: 'G#4', 61: 'G#5', 62: 'G0', 63: 'G1', 64: 'G2', 65: 'G3', 66: 'G4', 67: 'G5'}\n",
            " Predicting.  Note:  399\n",
            "Wrote midi...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x396 with 0 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "RNxvAmhCQ4pW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "2f261a9f-6423-4674-cd19-bcb970afecbf"
      },
      "cell_type": "code",
      "source": [
        "# output intermed duration\n",
        "duration_prediction = predict_duration(duration_model, duration_weight_path)\n",
        "\n",
        "# output intermed notes\n",
        "note_prediction = predict_note(notes_model, note_weight_path)\n",
        "\n",
        "# output final midi\n",
        "output_midi(note_prediction, duration_prediction, dropout, model_size)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 0.0, 1: Fraction(1, 12), 2: Fraction(1, 6), 3: 0.25, 4: Fraction(1, 3), 5: Fraction(5, 12), 6: 0.5, 7: Fraction(2, 3), 8: 0.75, 9: 1.0, 10: Fraction(7, 6), 11: 1.25, 12: Fraction(4, 3), 13: 1.5, 14: Fraction(5, 3), 15: 1.75, 16: 2.0, 17: 2.25, 18: 2.5, 19: 3.0, 20: 3.25, 21: 3.5, 22: 4.0, 23: Fraction(25, 6), 24: 4.25, 25: 4.5, 26: 4.75, 27: 5.0, 28: 5.25, 29: 5.75, 30: 6.0, 31: 6.5, 32: 7.0, 33: Fraction(23, 3), 34: 8.0, 35: 8.5, 36: 9.0, 37: 14.5, 38: 16.0, 39: 18.0, 40: 27.5, 41: 32.0, 42: 32.5, 43: 36.0, 44: 36.25, 45: 39.5, 46: 47.5, 47: 48.0, 48: 61.0, 49: 64.25, 50: 68.0, 51: 72.25, 52: 72.5, 53: 80.0, 54: 84.25, 55: Fraction(553, 6), 56: 94.5, 57: 128.25, 58: 161.0, 59: 176.0, 60: 200.5}\n",
            " Predicting.  Duration:  399{0: 'A1', 1: 'A2', 2: 'A3', 3: 'A4', 4: 'A5', 5: 'B-1', 6: 'B-2', 7: 'B-3', 8: 'B-4', 9: 'B-5', 10: 'B1', 11: 'B2', 12: 'B3', 13: 'B4', 14: 'B5', 15: 'C#1', 16: 'C#2', 17: 'C#3', 18: 'C#4', 19: 'C#5', 20: 'C#6', 21: 'C1', 22: 'C2', 23: 'C3', 24: 'C4', 25: 'C5', 26: 'C6', 27: 'D1', 28: 'D2', 29: 'D3', 30: 'D4', 31: 'D5', 32: 'D6', 33: 'E-0', 34: 'E-2', 35: 'E-3', 36: 'E-4', 37: 'E-5', 38: 'E-6', 39: 'E1', 40: 'E2', 41: 'E3', 42: 'E4', 43: 'E5', 44: 'E6', 45: 'F#1', 46: 'F#2', 47: 'F#3', 48: 'F#4', 49: 'F#5', 50: 'F#6', 51: 'F0', 52: 'F1', 53: 'F2', 54: 'F3', 55: 'F4', 56: 'F5', 57: 'G#1', 58: 'G#2', 59: 'G#3', 60: 'G#4', 61: 'G#5', 62: 'G0', 63: 'G1', 64: 'G2', 65: 'G3', 66: 'G4', 67: 'G5'}\n",
            " Predicting.  Note:  399\n",
            "Wrote midi...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cMyBN7aHNai6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}